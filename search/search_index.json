{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Accueil","title":"Accueil"},{"location":"#accueil","text":"","title":"Accueil"},{"location":"api/configuration/commitlint/","text":"Commitlint Commitlint v\u00e9rifie si les messages de commit respectent le format de commit conventionnel . Pr\u00e9requis Husky Installation et utilisation Pour installer Commitlint, il suffit simplement de rentrer la commande npm : npm install --save-dev @commitlint/config-conventional @commitlint/cli Et configurer commitlint pour utiliser la config-conventional module.exports = {extends: ['@commitlint/config-conventional']} Enfin, ajouter la commande Husky : npx husky add .husky/commit-msg 'npx --no-install commitlint --edit'","title":"Commitlint"},{"location":"api/configuration/commitlint/#commitlint","text":"Commitlint v\u00e9rifie si les messages de commit respectent le format de commit conventionnel .","title":"Commitlint"},{"location":"api/configuration/commitlint/#prerequis","text":"Husky","title":"Pr\u00e9requis"},{"location":"api/configuration/commitlint/#installation-et-utilisation","text":"Pour installer Commitlint, il suffit simplement de rentrer la commande npm : npm install --save-dev @commitlint/config-conventional @commitlint/cli Et configurer commitlint pour utiliser la config-conventional module.exports = {extends: ['@commitlint/config-conventional']} Enfin, ajouter la commande Husky : npx husky add .husky/commit-msg 'npx --no-install commitlint --edit'","title":"Installation et utilisation"},{"location":"api/configuration/husky/","text":"Husky Husky est un outil qui permet de cr\u00e9er facilement des hooks git. Un hook git est une action r\u00e9alis\u00e9e avant ou apr\u00e8s une commande git . Au lieu de cr\u00e9er manuellement des hooks git dans le r\u00e9pertoire .git, Husky permet d'ajouter simplement les hooks dans le package.json ou par commande. Pourquoi Husky ? Plus de 25 000 \u00e9toiles sur github Utilis\u00e9 par plus de 700 000 repositories (notamment webpack, babel, Next.js..) Plus de 4 millions de t\u00e9l\u00e9chargements par semaine Avant toute chose Vous n'avez pas besoin de suivre le tutoriel qui suit lorsque vous r\u00e9cup\u00e9rez l'API. Tout sera d\u00e9j\u00e0 install\u00e9 et configur\u00e9, il suffira simplement de lancer la commande : npm ci Le tutoriel qui suit n'est \u00e9crit que dans un but informatif pour en apprendre plus. Installation et utilisation Pour installer Husky, il suffit simplement de rentrer la commande npm : npm install husky -D Ensuite, il faut ajouter un nouveau script npm nomm\u00e9 \"prepare\" et l'ex\u00e9cuter une premi\u00e8re fois : npm set-script prepare \"husky install\" npm run prepare Il ne reste plus qu'\u00e0 ajouter des hooks selon les besoins avec la commande : npx husky add .husky/GIT_HOOK \"COMMAND\" Par exemple, si je souhaite lancer la commande \"npm test\" avant chaque commit, j'utiliserai la commande suivante : npx husky add .husky/pre-commit \"npm test\" Voici la liste des hooks git disponibles : - pre-commit - prepare-commit-msg - commit-msg - post-commit - post-checkout - pre-rebase Ressources : Repository Husky Package NPM Git hooks","title":"Husky"},{"location":"api/configuration/husky/#husky","text":"Husky est un outil qui permet de cr\u00e9er facilement des hooks git. Un hook git est une action r\u00e9alis\u00e9e avant ou apr\u00e8s une commande git . Au lieu de cr\u00e9er manuellement des hooks git dans le r\u00e9pertoire .git, Husky permet d'ajouter simplement les hooks dans le package.json ou par commande. Pourquoi Husky ? Plus de 25 000 \u00e9toiles sur github Utilis\u00e9 par plus de 700 000 repositories (notamment webpack, babel, Next.js..) Plus de 4 millions de t\u00e9l\u00e9chargements par semaine","title":"Husky"},{"location":"api/configuration/husky/#avant-toute-chose","text":"Vous n'avez pas besoin de suivre le tutoriel qui suit lorsque vous r\u00e9cup\u00e9rez l'API. Tout sera d\u00e9j\u00e0 install\u00e9 et configur\u00e9, il suffira simplement de lancer la commande : npm ci Le tutoriel qui suit n'est \u00e9crit que dans un but informatif pour en apprendre plus.","title":"Avant toute chose"},{"location":"api/configuration/husky/#installation-et-utilisation","text":"Pour installer Husky, il suffit simplement de rentrer la commande npm : npm install husky -D Ensuite, il faut ajouter un nouveau script npm nomm\u00e9 \"prepare\" et l'ex\u00e9cuter une premi\u00e8re fois : npm set-script prepare \"husky install\" npm run prepare Il ne reste plus qu'\u00e0 ajouter des hooks selon les besoins avec la commande : npx husky add .husky/GIT_HOOK \"COMMAND\" Par exemple, si je souhaite lancer la commande \"npm test\" avant chaque commit, j'utiliserai la commande suivante : npx husky add .husky/pre-commit \"npm test\" Voici la liste des hooks git disponibles : - pre-commit - prepare-commit-msg - commit-msg - post-commit - post-checkout - pre-rebase","title":"Installation et utilisation"},{"location":"api/configuration/husky/#ressources","text":"Repository Husky Package NPM Git hooks","title":"Ressources :"},{"location":"api/configuration/lint_staged/","text":"Lint staged Lint staged est un outil con\u00e7u pour n\u2019examiner que le travail stag\u00e9 (git). En revanche, il nous incombe de lui dire quels fichiers traiter et avec quels outils. Installation et configuration Pour installer Lint staged, il suffit simplement de rentrer la commande npm : npm install lint-staged -D Il faut ensuite cr\u00e9\u00e9 \u00e0 la racine du projet un fichier lint-staged.config.js pour le configurer. module.exports = { '**/*.php': [ 'php ./vendor/bin/php-cs-fixer fix --config .php-cs-fixer.php --allow-risky=yes' ], }; Dans notre exemple, nous configurons que lint-staged doit seulement examiner les fichiers php avec la commande php-cs-fixer . Lint-staged est appel\u00e9 lors du hook pre-commit \u00e0 l'aide de Husky . Repository Lint-Staged Package NPM","title":"Lint Staged"},{"location":"api/configuration/lint_staged/#lint-staged","text":"Lint staged est un outil con\u00e7u pour n\u2019examiner que le travail stag\u00e9 (git). En revanche, il nous incombe de lui dire quels fichiers traiter et avec quels outils.","title":"Lint staged"},{"location":"api/configuration/lint_staged/#installation-et-configuration","text":"Pour installer Lint staged, il suffit simplement de rentrer la commande npm : npm install lint-staged -D Il faut ensuite cr\u00e9\u00e9 \u00e0 la racine du projet un fichier lint-staged.config.js pour le configurer. module.exports = { '**/*.php': [ 'php ./vendor/bin/php-cs-fixer fix --config .php-cs-fixer.php --allow-risky=yes' ], }; Dans notre exemple, nous configurons que lint-staged doit seulement examiner les fichiers php avec la commande php-cs-fixer . Lint-staged est appel\u00e9 lors du hook pre-commit \u00e0 l'aide de Husky . Repository Lint-Staged Package NPM","title":"Installation et configuration"},{"location":"api/configuration/php_cs_fixer/","text":"Php cs fixer Php cs fixer est un outil qui permet de corriger le code pour qu'il respecte des normes de codage PHP telles que d\u00e9finies dans les PSRs , etc. ou d'autres normes communautaires comme celles de Symfony. Il est \u00e9galement possible de d\u00e9finir son propre style par le biais de la configuration. Installation et utilisation Pour installer Php cs fixer, il suffit simplement de rentrer la commande composer : composer require friendsofphp/php-cs-fixer --dev Il est d\u00e9sormais possible de l'utiliser avec la commande : ./vendor/bin/php-cs-fixer fix /path/to/fix Cependant, pour des raisons de praticit\u00e9, il est recommand\u00e9 de cr\u00e9er deux nouveaux scripts dans composer.json : \"scripts\": { \"sniff\": [ \"./vendor/bin/php-cs-fixer fix -vvv --dry-run --show-progress=dots\" ], \"lint\": [ \"./vendor/bin/php-cs-fixer fix -vvv --show-progress=dots --allow-risky=yes\" ] } La premi\u00e8re, composer sniff permet d'avoir un aper\u00e7u des bouts de codes qui ne respectent pas les normes sans pour autant le corriger. La seconde, composer lint , permet de corriger les manquements aux normes. Il est possible de n'utiliser que la seconde commande, cependant il peut \u00eatre utile d'avoir un aper\u00e7u avant correctif. Configuration Un fichier .php-cs-fixer.php peut \u00eatre cr\u00e9\u00e9 \u00e0 la racine du projet pour d\u00e9finir sa propre configuration. Ceci a \u00e9t\u00e9 fait pour le projet Pronochain avec la configuration suivante : <?php use PhpCsFixer\\Config; use PhpCsFixer\\Finder; $rules = [ '@PhpCsFixer' => true, ]; $finder = Finder::create() ->in(__DIR__) ->exclude(['bootstrap', 'storage', 'vendor']) ->name('*.php') ->notName('*.blade.php') ->ignoreDotFiles(true) ->ignoreVCS(true) ; return (new Config()) ->setFinder($finder) ->setRules($rules) ->setRiskyAllowed(true) ->setUsingCache(true) ; Dans un premier temps, on d\u00e9finit les ensembles de r\u00e8gles qui vont \u00eatre utilis\u00e9s. L'ensemble @PhpCsFixer est utilis\u00e9 car celui-ci impl\u00e9mente l'ensemble @Symfony (qui respecte tous les PSRs) ainsi que d'autres r\u00e8gles . Il est possible de ne pas utiliser d'ensemble de r\u00e8gles et de d\u00e9finir une \u00e0 une le comportement de chaque r\u00e8gle. Ensuite, on configure le fixer en indiquant le r\u00e9pertoire dans lequel il va agir (r\u00e9pertoire courant), les dossiers que l'on ne prend pas en compte, les fichiers pris ou non en compte et on ignore les fichiers dot (.env par exemple). Enfin, on applique notre configuration. Ressources : Repository PhpCsFixer","title":"PhpCsFixer"},{"location":"api/configuration/php_cs_fixer/#php-cs-fixer","text":"Php cs fixer est un outil qui permet de corriger le code pour qu'il respecte des normes de codage PHP telles que d\u00e9finies dans les PSRs , etc. ou d'autres normes communautaires comme celles de Symfony. Il est \u00e9galement possible de d\u00e9finir son propre style par le biais de la configuration.","title":"Php cs fixer"},{"location":"api/configuration/php_cs_fixer/#installation-et-utilisation","text":"Pour installer Php cs fixer, il suffit simplement de rentrer la commande composer : composer require friendsofphp/php-cs-fixer --dev Il est d\u00e9sormais possible de l'utiliser avec la commande : ./vendor/bin/php-cs-fixer fix /path/to/fix Cependant, pour des raisons de praticit\u00e9, il est recommand\u00e9 de cr\u00e9er deux nouveaux scripts dans composer.json : \"scripts\": { \"sniff\": [ \"./vendor/bin/php-cs-fixer fix -vvv --dry-run --show-progress=dots\" ], \"lint\": [ \"./vendor/bin/php-cs-fixer fix -vvv --show-progress=dots --allow-risky=yes\" ] } La premi\u00e8re, composer sniff permet d'avoir un aper\u00e7u des bouts de codes qui ne respectent pas les normes sans pour autant le corriger. La seconde, composer lint , permet de corriger les manquements aux normes. Il est possible de n'utiliser que la seconde commande, cependant il peut \u00eatre utile d'avoir un aper\u00e7u avant correctif.","title":"Installation et utilisation"},{"location":"api/configuration/php_cs_fixer/#configuration","text":"Un fichier .php-cs-fixer.php peut \u00eatre cr\u00e9\u00e9 \u00e0 la racine du projet pour d\u00e9finir sa propre configuration. Ceci a \u00e9t\u00e9 fait pour le projet Pronochain avec la configuration suivante : <?php use PhpCsFixer\\Config; use PhpCsFixer\\Finder; $rules = [ '@PhpCsFixer' => true, ]; $finder = Finder::create() ->in(__DIR__) ->exclude(['bootstrap', 'storage', 'vendor']) ->name('*.php') ->notName('*.blade.php') ->ignoreDotFiles(true) ->ignoreVCS(true) ; return (new Config()) ->setFinder($finder) ->setRules($rules) ->setRiskyAllowed(true) ->setUsingCache(true) ; Dans un premier temps, on d\u00e9finit les ensembles de r\u00e8gles qui vont \u00eatre utilis\u00e9s. L'ensemble @PhpCsFixer est utilis\u00e9 car celui-ci impl\u00e9mente l'ensemble @Symfony (qui respecte tous les PSRs) ainsi que d'autres r\u00e8gles . Il est possible de ne pas utiliser d'ensemble de r\u00e8gles et de d\u00e9finir une \u00e0 une le comportement de chaque r\u00e8gle. Ensuite, on configure le fixer en indiquant le r\u00e9pertoire dans lequel il va agir (r\u00e9pertoire courant), les dossiers que l'on ne prend pas en compte, les fichiers pris ou non en compte et on ignore les fichiers dot (.env par exemple). Enfin, on applique notre configuration.","title":"Configuration"},{"location":"api/configuration/php_cs_fixer/#ressources","text":"Repository PhpCsFixer","title":"Ressources :"},{"location":"api/configuration/standard_version/","text":"Standard Version Standard version est un outil pour le versioning utilisant semver et la g\u00e9n\u00e9ration de CHANGELOG aliment\u00e9 par Conventional Commits . Installation et utilisation Pour installer Standard Version, il suffit simplement de rentrer la commande npm : npm install standard-version -D Il faut ensuite ajouter un script npm dans le package.json : { \"scripts\": { \"release\": \"standard-version\" } } Vous pouvez d\u00e9sormais utiliser la commande npm release. Configuration Un fichier .versionrc.json peut \u00eatre cr\u00e9\u00e9 \u00e0 la racine du projet pour d\u00e9finir sa propre configuration. Ceci a \u00e9t\u00e9 fait pour le projet Pronochain avec la configuration suivante : { \"types\": [ {\"type\": \"feat\", \"section\": \"Fonctionnalit\u00e9s\"}, {\"type\": \"fix\", \"section\": \"Correctifs\"}, {\"type\": \"chore\", \"hidden\": true}, {\"type\": \"docs\", \"hidden\": true}, {\"type\": \"style\", \"hidden\": true}, {\"type\": \"refactor\", \"hidden\": true}, {\"type\": \"perf\", \"hidden\": true}, {\"type\": \"test\", \"hidden\": true} ], \"commitUrlFormat\": \"https://dev.azure.com/SixLegendary/Pronochain/_git/pronochain-back/commit/{{hash}}\", \"compareUrlFormat\": \"https://dev.azure.com/SixLegendary/Pronochain/_git/pronochain-back/branchCompare?baseVersion=GT{{previousTag}}&targetVersion=GT{{currentTag}}\" } Nous avons red\u00e9fini les noms des sections pour que le CHANGELOG g\u00e9n\u00e9r\u00e9 soit en fran\u00e7ais, puis nous avons d\u00e9fini les URL de commit et de comparatif pour avoir des versions cliquables dans le CHANGELOG. Repository Standard Version Package NPM","title":"Standard Version"},{"location":"api/configuration/standard_version/#standard-version","text":"Standard version est un outil pour le versioning utilisant semver et la g\u00e9n\u00e9ration de CHANGELOG aliment\u00e9 par Conventional Commits .","title":"Standard Version"},{"location":"api/configuration/standard_version/#installation-et-utilisation","text":"Pour installer Standard Version, il suffit simplement de rentrer la commande npm : npm install standard-version -D Il faut ensuite ajouter un script npm dans le package.json : { \"scripts\": { \"release\": \"standard-version\" } } Vous pouvez d\u00e9sormais utiliser la commande npm release.","title":"Installation et utilisation"},{"location":"api/configuration/standard_version/#configuration","text":"Un fichier .versionrc.json peut \u00eatre cr\u00e9\u00e9 \u00e0 la racine du projet pour d\u00e9finir sa propre configuration. Ceci a \u00e9t\u00e9 fait pour le projet Pronochain avec la configuration suivante : { \"types\": [ {\"type\": \"feat\", \"section\": \"Fonctionnalit\u00e9s\"}, {\"type\": \"fix\", \"section\": \"Correctifs\"}, {\"type\": \"chore\", \"hidden\": true}, {\"type\": \"docs\", \"hidden\": true}, {\"type\": \"style\", \"hidden\": true}, {\"type\": \"refactor\", \"hidden\": true}, {\"type\": \"perf\", \"hidden\": true}, {\"type\": \"test\", \"hidden\": true} ], \"commitUrlFormat\": \"https://dev.azure.com/SixLegendary/Pronochain/_git/pronochain-back/commit/{{hash}}\", \"compareUrlFormat\": \"https://dev.azure.com/SixLegendary/Pronochain/_git/pronochain-back/branchCompare?baseVersion=GT{{previousTag}}&targetVersion=GT{{currentTag}}\" } Nous avons red\u00e9fini les noms des sections pour que le CHANGELOG g\u00e9n\u00e9r\u00e9 soit en fran\u00e7ais, puis nous avons d\u00e9fini les URL de commit et de comparatif pour avoir des versions cliquables dans le CHANGELOG. Repository Standard Version Package NPM","title":"Configuration"},{"location":"application_mobile/","text":"Accueil Application mobile L'application mobile sera disponible sur Android et IOS . Cette application permettra de faire toutes les functionnalit\u00e9s sp\u00e9cifi\u00e9 dans le cahier des charges. Getting Started 1 . Installation de Docker Engine 2 . Clonage de la solution 3 . Installation et d\u00e9marrage du conteneur docker Getting Started Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows , Linux ou tous autre systemes d'exploitation supportant docker. Pour les systemes d'exploitation ne suportant pas docker il vous faudra suivre des tutoriels en ligne. 1 . Installation de Docker Engine Rendez-vous sur le site officiel de docker pour t\u00e9l\u00e9charger la derniere version : https://docs.docker.com/engine/install/ Effectu\u00e9 ensuite l'installation en suivant les diff\u00e9rents \u00e9crans. 2 . Clonage de la solution Une fois docker install\u00e9 et pret \u00e0 l'emploie, nous allons r\u00e9cup\u00e9rer notre solution en ligne \u00e0 l'aide de la commande : git clone \"lien_du_repository\" Une fois notre solution en local nous allons nous rendre dans le dossier racine du projet. On peu voir un fichier Dockerfile c'est grace \u00e0 lui que nous allons g\u00e9n\u00e9rer notre environement de d\u00e9veloppement. 3 . Installation et d\u00e9marrage du conteneur docker Un conteneur docker a \u00e9t\u00e9 configur\u00e9 pour vous faciliter le d\u00e9veloppement, mais aussi le d\u00e9ploiement. Une fois le conteneur docker lanc\u00e9 en mode d\u00e9veloppement. Ouvez la solution dans Visual Studio Code et t\u00e9l\u00e9charger l'extension Docker . Cliquez ensuite sur l'icone en bas de votre ecran Visual Studio Code : Puis selectionnez Open Folder in Container. S\u00e9lectionnez le r\u00e9pertoire racine qui contient le Dockerfile Pour les fois suivante il vous faudrat selectionn\u00e9 Reopen in container afin de ne pas recr\u00e9er un container. Vous voila pret !","title":"Initialisation"},{"location":"application_mobile/#accueil","text":"","title":"Accueil"},{"location":"application_mobile/#application-mobile","text":"L'application mobile sera disponible sur Android et IOS . Cette application permettra de faire toutes les functionnalit\u00e9s sp\u00e9cifi\u00e9 dans le cahier des charges. Getting Started 1 . Installation de Docker Engine 2 . Clonage de la solution 3 . Installation et d\u00e9marrage du conteneur docker","title":"Application mobile"},{"location":"application_mobile/#getting-started","text":"Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows , Linux ou tous autre systemes d'exploitation supportant docker. Pour les systemes d'exploitation ne suportant pas docker il vous faudra suivre des tutoriels en ligne.","title":"Getting Started"},{"location":"application_mobile/#1-installation-de-docker-engine","text":"Rendez-vous sur le site officiel de docker pour t\u00e9l\u00e9charger la derniere version : https://docs.docker.com/engine/install/ Effectu\u00e9 ensuite l'installation en suivant les diff\u00e9rents \u00e9crans.","title":"1. Installation de Docker Engine"},{"location":"application_mobile/#2-clonage-de-la-solution","text":"Une fois docker install\u00e9 et pret \u00e0 l'emploie, nous allons r\u00e9cup\u00e9rer notre solution en ligne \u00e0 l'aide de la commande : git clone \"lien_du_repository\" Une fois notre solution en local nous allons nous rendre dans le dossier racine du projet. On peu voir un fichier Dockerfile c'est grace \u00e0 lui que nous allons g\u00e9n\u00e9rer notre environement de d\u00e9veloppement.","title":"2. Clonage de la solution"},{"location":"application_mobile/#3-installation-et-demarrage-du-conteneur-docker","text":"Un conteneur docker a \u00e9t\u00e9 configur\u00e9 pour vous faciliter le d\u00e9veloppement, mais aussi le d\u00e9ploiement. Une fois le conteneur docker lanc\u00e9 en mode d\u00e9veloppement. Ouvez la solution dans Visual Studio Code et t\u00e9l\u00e9charger l'extension Docker . Cliquez ensuite sur l'icone en bas de votre ecran Visual Studio Code : Puis selectionnez Open Folder in Container. S\u00e9lectionnez le r\u00e9pertoire racine qui contient le Dockerfile Pour les fois suivante il vous faudrat selectionn\u00e9 Reopen in container afin de ne pas recr\u00e9er un container. Vous voila pret !","title":"3. Installation et d\u00e9marrage du conteneur docker"},{"location":"application_mobile/aide/","text":"Sommaire En avant 1 . Architecture pour Pronochain 2 . Commandes pratiques Getting Started Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows , Linux ou tous autre systemes d'exploitation supportant docker. Pour les systemes d'exploitation ne suportant pas docker il vous faudra suivre des tutoriels en ligne. 1 . Architecture pour Pronochain Model : Le mod\u00e8le contient les donn\u00e9es. G\u00e9n\u00e9ralement, ces donn\u00e9es proviennent d\u2019une base de donn\u00e9es ou d\u2019un service externe comme un API. Ce dossier comprendra un sous dossier services pour nos connection \u00e0 divers apis et services. View : La vue correspond \u00e0 ce qui est affich\u00e9. Un ecran de notre application sur mobile dans notre cas. Viewmodel : Ce composant fait le lien entre le mod\u00e8le et la vue. Il s\u2019occupe de g\u00e9rer les liaisons de donn\u00e9es. En bleu ou l\u2019on va coder nos tests unitaires . Et en vert la page principal ( point d\u2019entr\u00e9e de l\u2019application ). Services : Le dossier service ne sera pas dans le projet. Il n'est pas n\u00e9cessaire. 2 . Commandes pratiques flutter create APP_NAME : Cr\u00e9er une application. flutter doctor : V\u00e9rifie que tous les composants n\u00e9cessaires au fonctionnement de flutter sont install\u00e9 et fonctionnels . flutter pub get : R\u00e9cup\u00e9rer les packages \u00e0 installer depuis le pubspec.yaml . flutter pub add le_nom_du_package : Installer un package . flutter test : Lance les tests de l'application flutter . flutter pub update : Mets \u00e0 jou r les packages flutter**. flutter build : Build le projet.","title":"Aides"},{"location":"application_mobile/aide/#sommaire","text":"En avant 1 . Architecture pour Pronochain 2 . Commandes pratiques","title":"Sommaire"},{"location":"application_mobile/aide/#getting-started","text":"Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows , Linux ou tous autre systemes d'exploitation supportant docker. Pour les systemes d'exploitation ne suportant pas docker il vous faudra suivre des tutoriels en ligne.","title":"Getting Started"},{"location":"application_mobile/aide/#1-architecture-pour-pronochain","text":"Model : Le mod\u00e8le contient les donn\u00e9es. G\u00e9n\u00e9ralement, ces donn\u00e9es proviennent d\u2019une base de donn\u00e9es ou d\u2019un service externe comme un API. Ce dossier comprendra un sous dossier services pour nos connection \u00e0 divers apis et services. View : La vue correspond \u00e0 ce qui est affich\u00e9. Un ecran de notre application sur mobile dans notre cas. Viewmodel : Ce composant fait le lien entre le mod\u00e8le et la vue. Il s\u2019occupe de g\u00e9rer les liaisons de donn\u00e9es. En bleu ou l\u2019on va coder nos tests unitaires . Et en vert la page principal ( point d\u2019entr\u00e9e de l\u2019application ). Services : Le dossier service ne sera pas dans le projet. Il n'est pas n\u00e9cessaire.","title":"1. Architecture pour Pronochain"},{"location":"application_mobile/aide/#2-commandes-pratiques","text":"flutter create APP_NAME : Cr\u00e9er une application. flutter doctor : V\u00e9rifie que tous les composants n\u00e9cessaires au fonctionnement de flutter sont install\u00e9 et fonctionnels . flutter pub get : R\u00e9cup\u00e9rer les packages \u00e0 installer depuis le pubspec.yaml . flutter pub add le_nom_du_package : Installer un package . flutter test : Lance les tests de l'application flutter . flutter pub update : Mets \u00e0 jou r les packages flutter**. flutter build : Build le projet.","title":"2. Commandes pratiques"},{"location":"application_mobile/methode/","text":"Accueil !! ATTENTION !! Ne surtout pas suivre pour ce projet. Cette m\u00e9thode est pr\u00e9sent\u00e9 pour vous si vous souhaitez red\u00e9velopper du flutter dans un autre projet. Nous passerons par un docker qui vous permetras de n'avoir rien \u00e0 installer sur votre pc. Cela nous \u00e9vit\u00e9ra tous probleme de conflit entre les versions et permetras aussi un d\u00e9ploiement plus rapide. Application mobile Getting Started 1 . Installation de GIT Bash 2 . T\u00e9l\u00e9chargement du SDK Flutter 3 . Cr\u00e9ation de la variable d'environnement 4 . Installation du SDK d\u2019Android via Android Studio 5 . Acceptation des licences et ajout des plugins Getting Started Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows uniquement. En ce qui concerne Linux , il vous faudra suivre des tutoriels en ligne. 1 . Installation de Git Bash Vous pouvez taper git dans votre cmd pour v\u00e9rifier la bonne installation. 2 . T\u00e9l\u00e9chargement du SDK Flutter . Extraire le fichier zip et placez le contenu dans l'emplacement d'installation souhait\u00e9 pour le SDK. Attention ! N'installez pas Flutter dans un r\u00e9pertoire tel que C:\\Program Files qui n\u00e9cessite des privil\u00e8ges \u00e9lev\u00e9s. Vous pouvez aussi le t\u00e9l\u00e9charger via la commande dans votre cmd: git clone https://github.com/flutter/flutter.git -b stable . Le \u201c-b stable\u201d dans la commande peut se placer avant ou apr\u00e8s. 3 . Cr\u00e9ation de la variable d'environnement Tapez flutter dans l\u2019invite de commande. Si c\u2019est la premi\u00e8re fois que vous installez Flutter, vous aurez un message vous expliquant que flutter n'est pas recoonu en tant que commande interne... Flutter n\u2019est pas reconnu car il faut installer la variable d\u2019environnement. Pour ce faire, tapez \u00ab environnement \u00bb dans la barre de recherche du menu D\u00e9marrer. Cliquez sur \u00ab Modifier les variables d\u2019environnement syst\u00e8me \u00bb. Cliquez sur le bouton variables d\u2019environnement en bas \u00e0 droite. V\u00e9rifiez la pr\u00e9cense de la variable Path . Ensuite double-cliquez dessus pour ajoutez le lien vers le dossier \u00ab bin \u00bb de Flutter (vous trouverez ce dossier ou vous avez installer Flutter ). Fermer et rouvrir l\u2019invite de commande puis taper \u201cflutter\u201d. si vous n'avez plus l'erreur arretez le cmd. Nous n'avons pas besoin de cette commande pour le moment. 4 . Installation du SDK d\u2019Android via Android Studio Nous allons donc taper flutter doctor pour voir ce qu'il faut installer pour le bon fonctionnement de flutter. Si vous obtennez cette erreur : il faut telecharger android studio via ce lien . Pendant l\u2019installation, cochez bien Android Virtual Device pour obtenir un \u00e9mulateur. Une fois l\u2019installation termin\u00e9, lancez Android Studio. Selectionner \u00ab Custom \u00bb et non \u00ab Standard \u00bb dans la fen\u00eatre \u00ab Install Type \u00bb. Une fois terminer il faut accepter les licences. 5 . Acceptation des licences et ajout des plugins Pour accepter les licenses il vous faut taper la commande : flutter doctor --android-licenses Il faut ensuite valider plusieurs fois en tapant y pour valider. En relancant la commande flutter doctor il ne devrait n'y a voir que deux probl\u00e8mes li\u00e9 \u00e0 Android Studio. Il faut installer les deux plugins Flutter et Dart pour Android Studio. Attention ! N'installez les deux plugins que si vous comptez coder sous Android Studio. Lancez Android Studio. Une fois sur la page de bienvenue il faut allez dans Configure puis dans Plugins . Ensuite dans la barre de recherche des plugins, entrez Flutter puis Dart . Puis on red\u00e9marre Android Studio. T\u00e9l\u00e9charger Visual Studio Code si besoin puis lancez l\u2019application et t\u00e9l\u00e9chargez les extensions Flutter et Dart . En t\u00e9l\u00e9chargeant l\u2019extension Flutter, celle de Dart s\u2019installe automatiquement. Relancez une derniere fois la commande flutter doctor , normalement vous ne devriez plus avoir aucune erreur.","title":"Autres"},{"location":"application_mobile/methode/#accueil","text":"!! ATTENTION !! Ne surtout pas suivre pour ce projet. Cette m\u00e9thode est pr\u00e9sent\u00e9 pour vous si vous souhaitez red\u00e9velopper du flutter dans un autre projet. Nous passerons par un docker qui vous permetras de n'avoir rien \u00e0 installer sur votre pc. Cela nous \u00e9vit\u00e9ra tous probleme de conflit entre les versions et permetras aussi un d\u00e9ploiement plus rapide.","title":"Accueil"},{"location":"application_mobile/methode/#application-mobile","text":"Getting Started 1 . Installation de GIT Bash 2 . T\u00e9l\u00e9chargement du SDK Flutter 3 . Cr\u00e9ation de la variable d'environnement 4 . Installation du SDK d\u2019Android via Android Studio 5 . Acceptation des licences et ajout des plugins","title":"Application mobile"},{"location":"application_mobile/methode/#getting-started","text":"Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows uniquement. En ce qui concerne Linux , il vous faudra suivre des tutoriels en ligne.","title":"Getting Started"},{"location":"application_mobile/methode/#1-installation-de-git-bash","text":"Vous pouvez taper git dans votre cmd pour v\u00e9rifier la bonne installation.","title":"1. Installation de Git Bash"},{"location":"application_mobile/methode/#2-telechargement-du-sdk-flutter","text":"Extraire le fichier zip et placez le contenu dans l'emplacement d'installation souhait\u00e9 pour le SDK. Attention ! N'installez pas Flutter dans un r\u00e9pertoire tel que C:\\Program Files qui n\u00e9cessite des privil\u00e8ges \u00e9lev\u00e9s. Vous pouvez aussi le t\u00e9l\u00e9charger via la commande dans votre cmd: git clone https://github.com/flutter/flutter.git -b stable . Le \u201c-b stable\u201d dans la commande peut se placer avant ou apr\u00e8s.","title":"2. T\u00e9l\u00e9chargement du SDK Flutter."},{"location":"application_mobile/methode/#3-creation-de-la-variable-denvironnement","text":"Tapez flutter dans l\u2019invite de commande. Si c\u2019est la premi\u00e8re fois que vous installez Flutter, vous aurez un message vous expliquant que flutter n'est pas recoonu en tant que commande interne... Flutter n\u2019est pas reconnu car il faut installer la variable d\u2019environnement. Pour ce faire, tapez \u00ab environnement \u00bb dans la barre de recherche du menu D\u00e9marrer. Cliquez sur \u00ab Modifier les variables d\u2019environnement syst\u00e8me \u00bb. Cliquez sur le bouton variables d\u2019environnement en bas \u00e0 droite. V\u00e9rifiez la pr\u00e9cense de la variable Path . Ensuite double-cliquez dessus pour ajoutez le lien vers le dossier \u00ab bin \u00bb de Flutter (vous trouverez ce dossier ou vous avez installer Flutter ). Fermer et rouvrir l\u2019invite de commande puis taper \u201cflutter\u201d. si vous n'avez plus l'erreur arretez le cmd. Nous n'avons pas besoin de cette commande pour le moment.","title":"3. Cr\u00e9ation de la variable d'environnement"},{"location":"application_mobile/methode/#4-installation-du-sdk-dandroid-via-android-studio","text":"Nous allons donc taper flutter doctor pour voir ce qu'il faut installer pour le bon fonctionnement de flutter. Si vous obtennez cette erreur : il faut telecharger android studio via ce lien . Pendant l\u2019installation, cochez bien Android Virtual Device pour obtenir un \u00e9mulateur. Une fois l\u2019installation termin\u00e9, lancez Android Studio. Selectionner \u00ab Custom \u00bb et non \u00ab Standard \u00bb dans la fen\u00eatre \u00ab Install Type \u00bb. Une fois terminer il faut accepter les licences.","title":"4. Installation du SDK d\u2019Android via Android Studio"},{"location":"application_mobile/methode/#5-acceptation-des-licences-et-ajout-des-plugins","text":"Pour accepter les licenses il vous faut taper la commande : flutter doctor --android-licenses Il faut ensuite valider plusieurs fois en tapant y pour valider. En relancant la commande flutter doctor il ne devrait n'y a voir que deux probl\u00e8mes li\u00e9 \u00e0 Android Studio. Il faut installer les deux plugins Flutter et Dart pour Android Studio. Attention ! N'installez les deux plugins que si vous comptez coder sous Android Studio. Lancez Android Studio. Une fois sur la page de bienvenue il faut allez dans Configure puis dans Plugins . Ensuite dans la barre de recherche des plugins, entrez Flutter puis Dart . Puis on red\u00e9marre Android Studio. T\u00e9l\u00e9charger Visual Studio Code si besoin puis lancez l\u2019application et t\u00e9l\u00e9chargez les extensions Flutter et Dart . En t\u00e9l\u00e9chargeant l\u2019extension Flutter, celle de Dart s\u2019installe automatiquement. Relancez une derniere fois la commande flutter doctor , normalement vous ne devriez plus avoir aucune erreur.","title":"5. Acceptation des licences et ajout des plugins"},{"location":"bdd/bdd/","text":"Base de donn\u00e9es Installation Pour l'installation sur window vous pouvez installer pgAdmin ici . Pour linux vous devrez utiliser l'image docker . Une fois pgAdmin install\u00e9 cr\u00e9ez une bdd avec les options de base et utilisez le scipt que vous pouvez r\u00e9cuperer ici pour cr\u00e9er les tables. Structure Vous pouveez consulter le sch\u00e9ma de la bdd sur cette page R\u00e8gles m\u00e9tier Cette partie d\u00e9taille les r\u00e8gles m\u00e9tier non explicites sur le sch\u00e9ma de la bdd par notion. Partie de nft Cette partie expliqe toutes les r\u00e8gles m\u00e9tier li\u00e9es aux parties de nft . Raret\u00e9s Toutes les parties de nft ont une raret\u00e9. La somme de la fr\u00e9quence des raret\u00e9 ne doit pas d\u00e9paser 100. Cette r\u00e8gle m\u00e9tier n'est pas contrainte directement dans la bdd car cela pourrait poser des probl\u00e8mes au momnent ou on voudras ajuster les fr\u00e9quences de toutes les raret\u00e9s.Une raret\u00e9 ne peut \u00eatre supprim\u00e9e si elle est utilis\u00e9e. Couleur Certaines parties de nft ont une couleur. Une couleur dans la bdd est represent\u00e9e par un id, un code hexa valide (un # suivis de 3 ou 6 caract\u00e8re alphanum\u00e9rique) et d'indiquateur boolean pour les parites de nft pouvant utliser cette couleur. Les indiquateur boolean sont les suivant: * is_card_color utilis\u00e9 pour v\u00e9rifier si la couleur ajout\u00e9e a un CarBackground est bien une couleur de carte * is_hairiness_color pour v\u00e9rifier si la couleur ajout\u00e9e aux Hairs,Beards,Eyebrows est bien une couleur de poils * is_eye_color pour v\u00e9rifier si la couleur ajout\u00e9e aux yeux est bien une couleur d'oeil * is_skin_color pour v\u00e9rifier si la coouleur ajout\u00e9e aux Necks,Faces,Ears est bien une couleur de paux * is_club_color pour v\u00e9rifier les couleurs primaire et secondaire d'un club est bien une couleur de club. De plus la couleur primaire doit \u00eatre diff\u00e9rente de la couleur secondaire. Ces v\u00e9rifications sont faites \u00e0 l'aide de trigger before insert or update qui bloquent l'insertion si la couleur n'est pas bonne . Une couleur ne peut \u00eatre supprim\u00e9e si elle est utilis\u00e9e. Ecusson et t-shirt Les ecussons et t-shirts sont stock\u00e9s dans la table template. Un template a un type (\u00e9cusson ou t-shirt) et une raret\u00e9. Les \u00e9cusson et t-shirt reprennent la couleur pincipale du club. Utilisateur Les utilisateurs sont sotck\u00e9s dans la table user avec une adresse de portefeuille m\u00e9tamask et un pseudo unique. Un user peut ajouter d'autre tipster en amis.Ces amis sont stock\u00e9 dans la table UserFriends. Pour qu'un amis soit valide les deux identifiants d'user doivent \u00eatre diff\u00e9rent. Les invitations sont stock\u00e9es dans la table UserFriendInvites. Un refus d'inviation pourras \u00eatre r\u00e9pondue par oui, non ou bloquer. Si la reponse est bloquer un ligne seras ajout\u00e9e dans la table BlockedUser.L'imposibilit\u00e9 de r\u00e9-inviter quelqu'un qui \u00e0 d\u00e9j\u00e0 accept\u00e9 n'est pas mise en place sur la bdd pour des raison de performance.M\u00eame si un trigger fait perdre peut de performance, sur une fonctionalit\u00e9 potentielement tr\u00e8s utilis\u00e9 avec des performance requise \u00e9l\u00e9v\u00e9e ce n'est pas recommend\u00e9. La suppression d'un utilisateur entraine la suppression de ses \u00e9quipes, pronostique et amis. La supression d'une \u00e9quipe entraine la suppression de tous ses joueurs ainsi que leur relation. Pronostique Les pronositiques sont sotck\u00e9s dans la table pr\u00e9diction. La contrainte d'un utilisateur peut faire un seul pronositque par match est garantie par une contrainte d'unicit\u00e9 sur le couple id de match et user_id.La contraint de 11 pronostique sans r\u00e9sultat \u00e0 la fois n'est pas mise sur la bdd pour des raisons de performance. On ne peut pas se permattre de perdre en performance sur une des fonctionalit\u00e9 au coeur de notre projet.On ne peut pas supprimer un match si il est associ\u00e9 a un pronostique. et no ne peut pas supprimer un club si il est associ\u00e9 \u00e0 un match ou \u00e0 un pronostique.","title":"Base de donn\u00e9e"},{"location":"bdd/bdd/#base-de-donnees","text":"","title":"Base de donn\u00e9es"},{"location":"bdd/bdd/#installation","text":"Pour l'installation sur window vous pouvez installer pgAdmin ici . Pour linux vous devrez utiliser l'image docker . Une fois pgAdmin install\u00e9 cr\u00e9ez une bdd avec les options de base et utilisez le scipt que vous pouvez r\u00e9cuperer ici pour cr\u00e9er les tables.","title":"Installation"},{"location":"bdd/bdd/#structure","text":"Vous pouveez consulter le sch\u00e9ma de la bdd sur cette page","title":"Structure"},{"location":"bdd/bdd/#regles-metier","text":"Cette partie d\u00e9taille les r\u00e8gles m\u00e9tier non explicites sur le sch\u00e9ma de la bdd par notion.","title":"R\u00e8gles m\u00e9tier"},{"location":"bdd/bdd/#partie-de-nft","text":"Cette partie expliqe toutes les r\u00e8gles m\u00e9tier li\u00e9es aux parties de nft .","title":"Partie de nft"},{"location":"bdd/bdd/#raretes","text":"Toutes les parties de nft ont une raret\u00e9. La somme de la fr\u00e9quence des raret\u00e9 ne doit pas d\u00e9paser 100. Cette r\u00e8gle m\u00e9tier n'est pas contrainte directement dans la bdd car cela pourrait poser des probl\u00e8mes au momnent ou on voudras ajuster les fr\u00e9quences de toutes les raret\u00e9s.Une raret\u00e9 ne peut \u00eatre supprim\u00e9e si elle est utilis\u00e9e.","title":"Raret\u00e9s"},{"location":"bdd/bdd/#couleur","text":"Certaines parties de nft ont une couleur. Une couleur dans la bdd est represent\u00e9e par un id, un code hexa valide (un # suivis de 3 ou 6 caract\u00e8re alphanum\u00e9rique) et d'indiquateur boolean pour les parites de nft pouvant utliser cette couleur. Les indiquateur boolean sont les suivant: * is_card_color utilis\u00e9 pour v\u00e9rifier si la couleur ajout\u00e9e a un CarBackground est bien une couleur de carte * is_hairiness_color pour v\u00e9rifier si la couleur ajout\u00e9e aux Hairs,Beards,Eyebrows est bien une couleur de poils * is_eye_color pour v\u00e9rifier si la couleur ajout\u00e9e aux yeux est bien une couleur d'oeil * is_skin_color pour v\u00e9rifier si la coouleur ajout\u00e9e aux Necks,Faces,Ears est bien une couleur de paux * is_club_color pour v\u00e9rifier les couleurs primaire et secondaire d'un club est bien une couleur de club. De plus la couleur primaire doit \u00eatre diff\u00e9rente de la couleur secondaire. Ces v\u00e9rifications sont faites \u00e0 l'aide de trigger before insert or update qui bloquent l'insertion si la couleur n'est pas bonne . Une couleur ne peut \u00eatre supprim\u00e9e si elle est utilis\u00e9e.","title":"Couleur"},{"location":"bdd/bdd/#ecusson-et-t-shirt","text":"Les ecussons et t-shirts sont stock\u00e9s dans la table template. Un template a un type (\u00e9cusson ou t-shirt) et une raret\u00e9. Les \u00e9cusson et t-shirt reprennent la couleur pincipale du club.","title":"Ecusson et t-shirt"},{"location":"bdd/bdd/#utilisateur","text":"Les utilisateurs sont sotck\u00e9s dans la table user avec une adresse de portefeuille m\u00e9tamask et un pseudo unique. Un user peut ajouter d'autre tipster en amis.Ces amis sont stock\u00e9 dans la table UserFriends. Pour qu'un amis soit valide les deux identifiants d'user doivent \u00eatre diff\u00e9rent. Les invitations sont stock\u00e9es dans la table UserFriendInvites. Un refus d'inviation pourras \u00eatre r\u00e9pondue par oui, non ou bloquer. Si la reponse est bloquer un ligne seras ajout\u00e9e dans la table BlockedUser.L'imposibilit\u00e9 de r\u00e9-inviter quelqu'un qui \u00e0 d\u00e9j\u00e0 accept\u00e9 n'est pas mise en place sur la bdd pour des raison de performance.M\u00eame si un trigger fait perdre peut de performance, sur une fonctionalit\u00e9 potentielement tr\u00e8s utilis\u00e9 avec des performance requise \u00e9l\u00e9v\u00e9e ce n'est pas recommend\u00e9. La suppression d'un utilisateur entraine la suppression de ses \u00e9quipes, pronostique et amis. La supression d'une \u00e9quipe entraine la suppression de tous ses joueurs ainsi que leur relation.","title":"Utilisateur"},{"location":"bdd/bdd/#pronostique","text":"Les pronositiques sont sotck\u00e9s dans la table pr\u00e9diction. La contrainte d'un utilisateur peut faire un seul pronositque par match est garantie par une contrainte d'unicit\u00e9 sur le couple id de match et user_id.La contraint de 11 pronostique sans r\u00e9sultat \u00e0 la fois n'est pas mise sur la bdd pour des raisons de performance. On ne peut pas se permattre de perdre en performance sur une des fonctionalit\u00e9 au coeur de notre projet.On ne peut pas supprimer un match si il est associ\u00e9 a un pronostique. et no ne peut pas supprimer un club si il est associ\u00e9 \u00e0 un match ou \u00e0 un pronostique.","title":"Pronostique"},{"location":"blockchain/blockchain/","text":"Introduction Nous avons besoin de choisir une blockchain pour h\u00e9berger le smart-contract du mint des NFTs remis aux utilisateurs apr\u00e8s un pari r\u00e9ussi. Nous avons choisi de partir sur une blockchain bas\u00e9e sur la blockchain Etherum. Cette \u00e9tude a commenc\u00e9 par le filtrage des blockchains sur les crit\u00e8res suivant: le trafic sur la blockchain, pas trop haut pour \u00e9viter les d\u00e9lais, mais pas trop bas car indicateur du manque de popularit\u00e9 de la blockchain les co\u00fbts d'utilisation Definition layer 2 scaling solution (Solution de scaling de couche 2) : Solution d\u00e9porter le traitement des transactions du mainnet (couche 1) tout en conservant les m\u00eames standards de s\u00e9curit\u00e9 et de d\u00e9centralisation. Ces solutions permettent d'acc\u00e9l\u00e9rer les transactions et r\u00e9duisent les frais de transaction. Plus d'informations ici Faucet: Site web ou application permettant d'obtenir de la cryptomonnaie gratuitement. Certains faucet requi\u00e8rent la r\u00e9alisation de t\u00e2che simple avant d'obtenir la cryptomonnaie. Ces applications sont appel\u00e9es faucet (robinet) car elles permettent d'obtenir la cryptomonnaie en tr\u00e8s faible quantit\u00e9 semblable a une go\u00fbte d'eau. Frais de Gas Binance Blockchains retenues Polygon Polygon est une \"layer 2 scaling solution\" avec une vitesse de transaction tr\u00e8s largement sup\u00e9rieure \u00e0 Etherum et des co\u00fbts environ 10 000 fois inf\u00e9rieurs. Le symbole de la cryptomonnaie utilis\u00e9 par cette blockchain est MATIC.Plus d'informations ici Polygon dispose d'une documentation pour les d\u00e9veloppeurs tr\u00e8s compl\u00e8te et bien organis\u00e9e. Ainsi qu'un testnet officiel permettant de tester les Dapp en condition de production. Compatible avec Binance Polygon valide 7000 transactions pour un co\u00fbt moyen de 0.002$ et une utilisation moyenne d'environ 50%. Liens utiles: - Site officiel - Faucet - Gas tracker - Gas station - Graph du temps moyen des blocs - Convertiseur MATIC vers Gwei - Guide d'utilisation du testnet - Documentation d\u00e9veloppeur - Tutotriel NFT Avalanche Avalanche est une alternative \u00e0 Etherum plus rapide, moins ch\u00e8re et eco-friendly. Elle utilise une nouvelle approche pour la validation des blocs. Cette nouvelle approche offre de fortes garanties de s\u00e9curit\u00e9, des transactions rapides et un grand d\u00e9bit de transaction sans compromettre la d\u00e9centralisation. La cryptomonnaie de ce blockchain est: AVAX.Plus d'information ici La documentation d\u00e9veloppeur est compl\u00e8te, mais peu intuitive. Des testnets officiels sont \u00e0 disposition des d\u00e9veloppeurs. Non compatible avec Binance Liens utiles: - Site officiel - Faucet - Gas tracker - Gas station - Convertisseur - Doc d\u00e9veloppeur Gnosis Gnosis est une blockchain offrant des frais de transaction stable, design\u00e9e pour des transactions rapides et peu co\u00fbteuses. La documentation d\u00e9veloppeur est plus l\u00e9g\u00e8re que les deux exemples pr\u00e9c\u00e9dents. Il n'existe pas de testnet officiel. Moins d'outils mis \u00e0 disposition Compatible avec Binance Liens utiles: - Site officiel - Faucet - Blockscout - Gas station Blockchain choisie Tableau r\u00e9capulatif de la partie pr\u00e9c\u00e9dente Nous avons retenu Polygon pour les raisons suivantes : - Deuxi\u00e8me moins cher de peu - La mieux document\u00e9e des trois - Compatible avec Binance","title":"Choix blockchain"},{"location":"blockchain/blockchain/#introduction","text":"Nous avons besoin de choisir une blockchain pour h\u00e9berger le smart-contract du mint des NFTs remis aux utilisateurs apr\u00e8s un pari r\u00e9ussi. Nous avons choisi de partir sur une blockchain bas\u00e9e sur la blockchain Etherum. Cette \u00e9tude a commenc\u00e9 par le filtrage des blockchains sur les crit\u00e8res suivant: le trafic sur la blockchain, pas trop haut pour \u00e9viter les d\u00e9lais, mais pas trop bas car indicateur du manque de popularit\u00e9 de la blockchain les co\u00fbts d'utilisation","title":"Introduction"},{"location":"blockchain/blockchain/#definition","text":"layer 2 scaling solution (Solution de scaling de couche 2) : Solution d\u00e9porter le traitement des transactions du mainnet (couche 1) tout en conservant les m\u00eames standards de s\u00e9curit\u00e9 et de d\u00e9centralisation. Ces solutions permettent d'acc\u00e9l\u00e9rer les transactions et r\u00e9duisent les frais de transaction. Plus d'informations ici Faucet: Site web ou application permettant d'obtenir de la cryptomonnaie gratuitement. Certains faucet requi\u00e8rent la r\u00e9alisation de t\u00e2che simple avant d'obtenir la cryptomonnaie. Ces applications sont appel\u00e9es faucet (robinet) car elles permettent d'obtenir la cryptomonnaie en tr\u00e8s faible quantit\u00e9 semblable a une go\u00fbte d'eau.","title":"Definition"},{"location":"blockchain/blockchain/#frais-de-gas","text":"","title":"Frais de Gas"},{"location":"blockchain/blockchain/#binance","text":"","title":"Binance"},{"location":"blockchain/blockchain/#blockchains-retenues","text":"","title":"Blockchains retenues"},{"location":"blockchain/blockchain/#polygon","text":"Polygon est une \"layer 2 scaling solution\" avec une vitesse de transaction tr\u00e8s largement sup\u00e9rieure \u00e0 Etherum et des co\u00fbts environ 10 000 fois inf\u00e9rieurs. Le symbole de la cryptomonnaie utilis\u00e9 par cette blockchain est MATIC.Plus d'informations ici Polygon dispose d'une documentation pour les d\u00e9veloppeurs tr\u00e8s compl\u00e8te et bien organis\u00e9e. Ainsi qu'un testnet officiel permettant de tester les Dapp en condition de production. Compatible avec Binance Polygon valide 7000 transactions pour un co\u00fbt moyen de 0.002$ et une utilisation moyenne d'environ 50%. Liens utiles: - Site officiel - Faucet - Gas tracker - Gas station - Graph du temps moyen des blocs - Convertiseur MATIC vers Gwei - Guide d'utilisation du testnet - Documentation d\u00e9veloppeur - Tutotriel NFT","title":"Polygon"},{"location":"blockchain/blockchain/#avalanche","text":"Avalanche est une alternative \u00e0 Etherum plus rapide, moins ch\u00e8re et eco-friendly. Elle utilise une nouvelle approche pour la validation des blocs. Cette nouvelle approche offre de fortes garanties de s\u00e9curit\u00e9, des transactions rapides et un grand d\u00e9bit de transaction sans compromettre la d\u00e9centralisation. La cryptomonnaie de ce blockchain est: AVAX.Plus d'information ici La documentation d\u00e9veloppeur est compl\u00e8te, mais peu intuitive. Des testnets officiels sont \u00e0 disposition des d\u00e9veloppeurs. Non compatible avec Binance Liens utiles: - Site officiel - Faucet - Gas tracker - Gas station - Convertisseur - Doc d\u00e9veloppeur","title":"Avalanche"},{"location":"blockchain/blockchain/#gnosis","text":"Gnosis est une blockchain offrant des frais de transaction stable, design\u00e9e pour des transactions rapides et peu co\u00fbteuses. La documentation d\u00e9veloppeur est plus l\u00e9g\u00e8re que les deux exemples pr\u00e9c\u00e9dents. Il n'existe pas de testnet officiel. Moins d'outils mis \u00e0 disposition Compatible avec Binance Liens utiles: - Site officiel - Faucet - Blockscout - Gas station","title":"Gnosis"},{"location":"blockchain/blockchain/#blockchain-choisie","text":"Tableau r\u00e9capulatif de la partie pr\u00e9c\u00e9dente Nous avons retenu Polygon pour les raisons suivantes : - Deuxi\u00e8me moins cher de peu - La mieux document\u00e9e des trois - Compatible avec Binance","title":"Blockchain choisie"},{"location":"blockchain/oracle/","text":"Introduction Le smart-contract pour le mint des NFTs feras appel \u00e0 notre API de g\u00e9n\u00e9rations de NFT. Pour cela nous utiliserons Chainlink. Chainlink est l'un des oracles les plus utilis\u00e9 est recommandes dans la documentation officielle de polygon Fonctionement Lien utiles Site officiel Utilisation avec HardHat Starter kit Repo git Tuto video Concept","title":"Oracle"},{"location":"blockchain/oracle/#introduction","text":"Le smart-contract pour le mint des NFTs feras appel \u00e0 notre API de g\u00e9n\u00e9rations de NFT. Pour cela nous utiliserons Chainlink. Chainlink est l'un des oracles les plus utilis\u00e9 est recommandes dans la documentation officielle de polygon","title":"Introduction"},{"location":"blockchain/oracle/#fonctionement","text":"","title":"Fonctionement"},{"location":"blockchain/oracle/#lien-utiles","text":"Site officiel Utilisation avec HardHat Starter kit Repo git Tuto video Concept","title":"Lien utiles"},{"location":"generation_nft/","text":"Accueil G\u00e9n\u00e9ration NFT La g\u00e9n\u00e9ration NFT consiste \u00e0 cr\u00e9er une solution permettant de r\u00e9cup\u00e9rer les diff\u00e9rentes parties d'un visage d'une personne r\u00e9elle , styliser ces parties, les stocker dans la blockchain IPFS et par la suite en assembler plusieurs pour r\u00e9aliser une carte NFT avec des niveaux de raret\u00e9s diff\u00e9rents. Cette fonctionnalit\u00e9 est accessible via une API . Getting Started 1 . Installation de Python 3.9 2 . Cr\u00e9ation de l'environnement de d\u00e9veloppement virtuel 2BIS . Version de python d\u00e9j\u00e0 pr\u00e9sente 3 . Activation de l'environnement de d\u00e9veloppement virtuel 4 . Installation des packages 5 . Initialisation des variables d'environnements 6 . Installation et d\u00e9marrage du conteneur docker 7 . Commencer \u00e0 d\u00e9velopper Explication docker Docker-compose - Contenu Dockerfile - Build Docker-compose - Up Warnings Documentations Getting Started Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows uniquement. En ce qui concerne Linux , il vous faudra suivre des tutoriels en ligne pour l'installation de Python et pour conna\u00eetre le chemin du langage ( \u00e9tape 1, 2 et 2BIS ). Le reste des \u00e9tapes seront identiques. 1 . Installation de Python 3.9 . Par d\u00e9faut, python va s'installer sur AppData/Local/Programs/Python39 . Pour y acc\u00e9der Win + R puis saisissez %appdata% . L'explorer de fichiers va s'ouvrir dans AppData/Roaming . Il vous suffira d'aller dans le parent puis remonter le chemin. Ce chemin sera utile si vous avez d\u00e9j\u00e0 une autre version de python install\u00e9e sur votre ordinateur. Si vous avez d\u00e9j\u00e0 une version de Python install\u00e9 autre que la 3.9 : 3.7, 3.8 ... Il y a plusieurs solutions : - D\u00e9sinstaller les autres versions et garder uniquement la 3.9 (conseill\u00e9). - Suivre l'\u00e9tape 2BIS . Python 3.10 Pour information, Python 3.10 est disponible depuis la fin d'ann\u00e9e 2021 , cependant beaucoup de packages ne sont pas encore compatibles et aucun changement n'est r\u00e9ellement impactant sur notre solution. 2 . Cr\u00e9ation de l' environnement de d\u00e9veloppement virtuel Dans les derni\u00e8res versions de Python, l'environnement virtuel venv est install\u00e9 nativement. Rendez-vous dans le dossier de la solution et ouvrez un terminal powershell. Commande pour cr\u00e9er l'environnement ( lire phrase en dessous avant ): python -m venv pronochain D\u00e9j\u00e0 python ? Si vous avez d\u00e9j\u00e0 une version de python , rendez-vous \u00e9tape 2BIS . L'environnement de d\u00e9veloppement virtuel va vous permettre d' installer tous ce qui concerne la solution dans un dossier du projet , ce qui \u00e9vitera d'installer les diff\u00e9rents packages en global sur votre ordinateur et de contr\u00f4ler de A \u00e0 Z ce que l'application utilise. 2BIS . Version de python d\u00e9j\u00e0 pr\u00e9sente Dans la m\u00eame console, rentrez la commande : python -V Si la version qui appara\u00eet est python 3.9.x alors vous pouvez reprendre l'\u00e9tape 3, sinon copier le chemin de la version Python 3.9. Personnellement, le chemin est C:\\Users\\nga\\AppData\\Local\\Programs\\Python\\Python39 . Puis ex\u00e9cutez la commande suivant pour cr\u00e9er l'environnement de d\u00e9veloppement virtuel avec la version de python choisie. COLLER/LE/CHEMIN/ICI -m venv pronochain 3 . Activation de l' environnement de d\u00e9veloppement virtuel Pour pouvoir utiliser l'environnement virtuel, il faut l' activer . IL FAUT TOUJOURS ACTIVER l'environnement avant de commencer \u00e0 d\u00e9velopper pour \u00e9viter d'installer les packages sur votre ordinateur au lieu de les installer dans le dossier. Dans la console, tapez : pronochain/Scripts/activate Vous verrez normalement le nom de votre environnement virtuel en vert devant le chemin dans la console. Sur Visual Studio Code , si vous ouvrez le dossier entier dans le logiciel, vous pouvez afficher un terminal et choisir la version powershell. Il vous sera plus facile d'activer l'environnement pendant que vous d\u00e9veloppez. Pour d\u00e9sactiver l'environnement, il vous suffira de faire : deactivate Enfin, pour supprimer l'environnement, car une manipulation a \u00e9t\u00e9 mal faite, fermer toutes les consoles ou Visual Studio Code et supprimer simplement le dossier pronochain . 4 . Installation des packages Pour installer tous les packages n\u00e9cessaires pour faire fonctionner la solution, il vous suffit de taper ces commandes \u00e0 la suite : python -m pip install -r requirements.txt python -m pip install -r requirements-dev.txt python -m pip install -e . Le fichier requirements.txt est l'\u00e9quivalent du fichier package.json. Il comporte la liste de tous les packages. Lorsque vous souhaitez ajouter un nouveau package, il faudra IMP\u00c9RATIVEMENT ajouter python -m devant pip install puis d'ajouter le nom du package dans requirements.txt . Une documentation a \u00e9t\u00e9 faite uniquement \u00e0 ce sujet, pour la consulter cliquez ici . 5 . Initialisation des variables d'environnements Notre application aura besoin de plusieurs variables d'environnements en guise de configuration . Par exemple, la connexion \u00e0 la base de donn\u00e9es. Une documentation sera cr\u00e9\u00e9e pour conna\u00eetre toutes les variables d'environnements \u00e0 modifier. Un fichier est important pour cette partie : .env.example : contient les variables sans leur vraie valeur avec leur pattern. Le fait qu'il n'y ait pas de valeur est plut\u00f4t logique. Nous n'allons pas mettre dans le r\u00e9pertoire GIT les cl\u00e9s secr\u00e8tes. La premi\u00e8re manipulation est de copier-coller le fichier et de le renommer en supprimant le '.example' . Une fois copi\u00e9, passez \u00e0 l'\u00e9tape suivante. Warning Il faut modifier les variables d'environnements pour que la solution fonctionne . Une documentation a \u00e9t\u00e9 r\u00e9dig\u00e9e. 6 . Installation et d\u00e9marrage du conteneur docker Toujours dans la m\u00eame console, \u00e0 la racine du projet. Un conteneur docker a \u00e9t\u00e9 configur\u00e9 pour vous faciliter le d\u00e9veloppement, mais aussi le d\u00e9ploiement. Une fois le conteneur docker lanc\u00e9 en mode d\u00e9veloppement, vous pourrez d\u00e9marrer l'API ou acc\u00e9der au service externe de la documentation. L' API sera joignable via http://localhost:8000 et le swagger via http://localhost:8000/docs . Pour cr\u00e9er et d\u00e9marrer le conteneur au premier lancement : docker-compose pull && docker-compose -f docker-compose.yml -f docker-compose.dev.yml up --build -d Pour simplement d\u00e9marrer le docker apr\u00e8s le premier lancement : docker-compose up -d Version production Un autre docker-compose existe pour la version de production qui lance automatiquement l'API et qui n'a pas les packages de d\u00e9veloppement ni la documentation. Par la suite, pour d\u00e9marrer l'API en d\u00e9veloppement, il suffira d'ex\u00e9cuter cette commande sur votre terminal VSCode (par exemple) : docker exec -it pronochain_fastapi python app/launch.py -d Ne pas oulier \"-d\" Le fichier launch.py va d\u00e9marrer l'API en version d\u00e9veloppement avec cette commande : uvicorn app.generation_nft_api.main:app --host 0.0.0.0 --debug --reload --host : rends accessible l'application depuis l'IP du serveur ou depuis votre local. --debug : nom assez explicite pour comprendre --reload : si vous modifiez le code dans l'API, celle-ci red\u00e9marrera automatiquement. Attention, si une erreur 500 (erreur serveur) est d\u00e9tect\u00e9e, il faudra red\u00e9marrer l'API avec la commande . Le choix de ne pas d\u00e9marrer l'API au lancement du docker a \u00e9t\u00e9 fait pour plusieurs raisons : - Plus simple \u00e0 debug . Lorsque vous allez print() une variable, celle-ci s'affichera dans votre console. - Plus facilement r\u00e9demarrable . Lorsqu'une erreur syst\u00e8me intervient, l'API est arr\u00eat\u00e9. Si l'API \u00e9tait lanc\u00e9 via le docker, il faudrait relancer \u00e0 chaque fois le conteneur. 7 . Commencer \u00e0 d\u00e9velopper Les six premi\u00e8res \u00e9tapes vont vous permettre de d\u00e9marrer l'application . Cependant, il reste encore une \u00e9tape pour pouvoir d\u00e9velopper correctement : la configuration pre-commit . Lorsque vous allez essayer de push votre code sur le r\u00e9pertoire GIT sur la branche develop ou master, la pipeline va d\u00e9ployer le code sur les serveurs. Lors du build de la solution sur le serveur, une \u00e9tape de v\u00e9rification du respect de diff\u00e9rentes r\u00e8gles du langage python est lanc\u00e9. Si jamais votre code n'est pas conforme , le build ne pourra pas s'effectuer . Pour vous faciliter le travail, plusieurs v\u00e9rifications de votre c\u00f4t\u00e9 vont \u00eatre r\u00e9alis\u00e9es \u00e0 chaque commit. Certaines modifieront le code automatiquement, d'autres vous indiqueront les changements \u00e0 suivre. Toutes ces v\u00e9rifications se font gr\u00e2ce au package pre-commit. Pour l'installer et le configurer, rendez-vous sur la documentation dans la cat\u00e9gorie Configuration/Pre-commit . Explication docker Docker-compose Contenu Le docker-compose est divis\u00e9 en trois fichiers . Le premier fichier docker-compose.yml est un fichier global \u00e0 tous les environnements. Il permet de configurer ce qui sera commun pour la version d\u00e9veloppement et la version production . Voici le contenu du fichier docker-compose.yml : docker-compose.yml version : '3.9' services : api : build : context : . dockerfile : ./dockerfiles/Dockerfile.api env_file : - app/.env ports : - '${API_PORT:-8000}:8000' container_name : pronochain_fastapi Pour faire simple, les deux environnements auront besoin de l'API sur le port 8000, valeur par d\u00e9faut. Pour changer ce port, il suffit de modifier la variable API_PORT dans le fichier .env . Je n'expliquerai pas le fichier pour la production, il est tr\u00e8s similaire \u00e0 celui du fichier de d\u00e9veloppement et il n'est pas n\u00e9cessairement utile pour l'initialisation de l'application en mode d\u00e9velopppement. Je vous invite tout de m\u00eame \u00e0 le consulter : docker-compose.prod.yml docker-compose nous permet de surcharger les diff\u00e9rentes images et leurs propri\u00e9t\u00e9s. Le fichier docker-compose.dev.yml : docker-compose.dev.yml version : '3.9' services : api : build : context : . args : NODE_ENV : development command : tail -f /dev/null volumes : - .:/src Nous surchargeons l'image api avec un argument r\u00e9cup\u00e9rable depuis le Dockerfile et une command . tail -f /dev/null Elle permet de maintenir allum\u00e9 l'image docker. Sans cette commande, le docker s'allumerait et s'\u00e9teindrait directement car il n'aurait rien \u00e0 ex\u00e9cuter. Dockerfile Build Lorsque que l'on va build le conteneur de la solution, le fichier Dockerfile \u00e0 la racine sera appel\u00e9 pour l'image api . Dans le docker-compose, c'est la propri\u00e9t\u00e9 build qui s'occupe d'all\u00e9e chercher ce fameux fichier. Dans notre cas, comme il est \u00e0 la racine, la propri\u00e9t\u00e9 est \u00e9gale \u00e0 \".\" . \u00c0 l'int\u00e9rieur du fichier se trouve plusieurs commandes qui ont pour seul et uniquement but de fournir toute la configuration manquante \u00e0 l'image pour pouvoir fonctionner. Par exemple, pour pouvoir faire fonctionner FastAPI, il faut obligatoirement les packages. C'est le Dockerfile qui va copier les fichiers importants et les coller dans le volume du conteneur. Dockerfile FROM python:3.9 ARG NODE_ENV WORKDIR /src ENV PYTHONPATH = /src ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY . /src RUN --mount = type = cache,target = /root/.cache \\ pip install --upgrade pip RUN --mount = type = cache,target = /root/.cache \\ pip install -r requirements.txt RUN --mount = type = cache,target = /root/.cache \\ if [ \" $NODE_ENV \" = \"development\" ] ; \\ then pip install -r requirements-dev.txt ; \\ fi RUN if [ \" $NODE_ENV \" = \"development\" ] ; \\ then pip install -e . ; \\ else pip install . ; \\ fi Ce script va cr\u00e9er un r\u00e9pertoire dans le volume du conteneur docker , copier-coller les diff\u00e9rents fichiers de la solution dans ce dossier puis lancer l' installation des packages . Gr\u00e2ce \u00e0 l'argument NODE_ENV , le fichier sait s'il doit installer les packages pour le d\u00e9veloppement ou non. En mode d\u00e9veloppement , le volume du docker-compose est le m\u00eame que le WORKDIR de l'image . Cela signifie que la commande COPY est inutile et que lorsque qu'un fichier en local sera modifi\u00e9, le docker r\u00e9cup\u00e9ra automatiquement la mise \u00e0 jour. En version de production , le volume est diff\u00e9rent du WORKDIR de l'image. Dans ce cas, le fichier .dockerignore sera accessible et ne copiera pas tous les fichiers/dossiers. Lorsque vous voulez ajouter un package, il faut obligatoirement rebuild l'image . A chaque fois qu'un build est fait, il va ret\u00e9l\u00e9charger tous ces packages ce qui peut prendre du temps. Pour \u00e9vitez d'attendre, vous pouvez activer la fonctionnalit\u00e9 BuildKi t sur docker. Elle permet de mettre en cache les diff\u00e9rents fichiers que vous souhaitez. --mount=type=cache,target=/root/.cache Cette commande dans le Dockerfile permet de sp\u00e9cifier quels \u00e9lements seront en cache . Pour activer BuildKit , il suffit de modifier le fichier daemon.js (pour linux, il est assez facile de trouver son emplacement gr\u00e2ce \u00e0 internet). En ce qui concerne Windows, si vous utilisez l'interface : Settings -> Docker Engine , sinon dans votre : Users -> TonUser -> .docker Si vous n'avez pas de daemon.json, vous pouvez le cr\u00e9er. Ajouter : \"features\" : { \"buildkit\" : true } Le fichier doit ressembler \u00e0 : { \"builder\" : { \"gc\" : { \"defaultKeepStorage\" : \"20GB\" , \"enabled\" : true } }, \"debug\" : true , \"experimental\" : false , \"insecure-registries\" : [], \"registry-mirrors\" : [], \"features\" : { \"buildkit\" : true } } Docker-compose Up Lorsque l'on ex\u00e9cute la commande docker-compose up -d , le conteneur va \u00eatre cr\u00e9\u00e9, s'il n'existe pas. Warnings Il faut bien distinguer l'environnement de d\u00e9veloppement virtuel au conteneur Docker . Vous d\u00e9velopper sur votre environnement, mais c'est le conteneur docker qui va r\u00e9cup\u00e9rer le code et simuler le serveur. Si vous ajoutez un package, le docker ne l'aura pas, si vous changez le code, le docker peut se mettre \u00e0 jour automatiquement. Documentations Une fois ces \u00e9tapes termin\u00e9es, toute la documentation est disponible ici : Documentation Pronochain ou http://localhost:9090 si vous avez d\u00e9marrer votre documentation. C'est une application destin\u00e9e uniquement \u00e0 r\u00e9aliser des documentations plus esth\u00e9tiques qu'un simple markdown . Si elle n'est pas encore initialis\u00e9e, je vous invite \u00e0 aller voir le r\u00e9pertoire documentation .","title":"Initialisation"},{"location":"generation_nft/#accueil","text":"","title":"Accueil"},{"location":"generation_nft/#generation-nft","text":"La g\u00e9n\u00e9ration NFT consiste \u00e0 cr\u00e9er une solution permettant de r\u00e9cup\u00e9rer les diff\u00e9rentes parties d'un visage d'une personne r\u00e9elle , styliser ces parties, les stocker dans la blockchain IPFS et par la suite en assembler plusieurs pour r\u00e9aliser une carte NFT avec des niveaux de raret\u00e9s diff\u00e9rents. Cette fonctionnalit\u00e9 est accessible via une API . Getting Started 1 . Installation de Python 3.9 2 . Cr\u00e9ation de l'environnement de d\u00e9veloppement virtuel 2BIS . Version de python d\u00e9j\u00e0 pr\u00e9sente 3 . Activation de l'environnement de d\u00e9veloppement virtuel 4 . Installation des packages 5 . Initialisation des variables d'environnements 6 . Installation et d\u00e9marrage du conteneur docker 7 . Commencer \u00e0 d\u00e9velopper Explication docker Docker-compose - Contenu Dockerfile - Build Docker-compose - Up Warnings Documentations","title":"G\u00e9n\u00e9ration NFT"},{"location":"generation_nft/#getting-started","text":"Pour pouvoir commencer \u00e0 d\u00e9velopper dans les meilleures conditions, plusieurs installations sont n\u00e9cessaires. Cette documentation est r\u00e9alis\u00e9e pour Windows uniquement. En ce qui concerne Linux , il vous faudra suivre des tutoriels en ligne pour l'installation de Python et pour conna\u00eetre le chemin du langage ( \u00e9tape 1, 2 et 2BIS ). Le reste des \u00e9tapes seront identiques.","title":"Getting Started"},{"location":"generation_nft/#1-installation-de-python-39","text":"Par d\u00e9faut, python va s'installer sur AppData/Local/Programs/Python39 . Pour y acc\u00e9der Win + R puis saisissez %appdata% . L'explorer de fichiers va s'ouvrir dans AppData/Roaming . Il vous suffira d'aller dans le parent puis remonter le chemin. Ce chemin sera utile si vous avez d\u00e9j\u00e0 une autre version de python install\u00e9e sur votre ordinateur. Si vous avez d\u00e9j\u00e0 une version de Python install\u00e9 autre que la 3.9 : 3.7, 3.8 ... Il y a plusieurs solutions : - D\u00e9sinstaller les autres versions et garder uniquement la 3.9 (conseill\u00e9). - Suivre l'\u00e9tape 2BIS . Python 3.10 Pour information, Python 3.10 est disponible depuis la fin d'ann\u00e9e 2021 , cependant beaucoup de packages ne sont pas encore compatibles et aucun changement n'est r\u00e9ellement impactant sur notre solution.","title":"1. Installation de Python 3.9."},{"location":"generation_nft/#2-creation-de-lenvironnement-de-developpement-virtuel","text":"Dans les derni\u00e8res versions de Python, l'environnement virtuel venv est install\u00e9 nativement. Rendez-vous dans le dossier de la solution et ouvrez un terminal powershell. Commande pour cr\u00e9er l'environnement ( lire phrase en dessous avant ): python -m venv pronochain D\u00e9j\u00e0 python ? Si vous avez d\u00e9j\u00e0 une version de python , rendez-vous \u00e9tape 2BIS . L'environnement de d\u00e9veloppement virtuel va vous permettre d' installer tous ce qui concerne la solution dans un dossier du projet , ce qui \u00e9vitera d'installer les diff\u00e9rents packages en global sur votre ordinateur et de contr\u00f4ler de A \u00e0 Z ce que l'application utilise.","title":"2. Cr\u00e9ation de l'environnement de d\u00e9veloppement virtuel"},{"location":"generation_nft/#2bis-version-de-python-deja-presente","text":"Dans la m\u00eame console, rentrez la commande : python -V Si la version qui appara\u00eet est python 3.9.x alors vous pouvez reprendre l'\u00e9tape 3, sinon copier le chemin de la version Python 3.9. Personnellement, le chemin est C:\\Users\\nga\\AppData\\Local\\Programs\\Python\\Python39 . Puis ex\u00e9cutez la commande suivant pour cr\u00e9er l'environnement de d\u00e9veloppement virtuel avec la version de python choisie. COLLER/LE/CHEMIN/ICI -m venv pronochain","title":"2BIS. Version de python d\u00e9j\u00e0 pr\u00e9sente"},{"location":"generation_nft/#3-activation-de-lenvironnement-de-developpement-virtuel","text":"Pour pouvoir utiliser l'environnement virtuel, il faut l' activer . IL FAUT TOUJOURS ACTIVER l'environnement avant de commencer \u00e0 d\u00e9velopper pour \u00e9viter d'installer les packages sur votre ordinateur au lieu de les installer dans le dossier. Dans la console, tapez : pronochain/Scripts/activate Vous verrez normalement le nom de votre environnement virtuel en vert devant le chemin dans la console. Sur Visual Studio Code , si vous ouvrez le dossier entier dans le logiciel, vous pouvez afficher un terminal et choisir la version powershell. Il vous sera plus facile d'activer l'environnement pendant que vous d\u00e9veloppez. Pour d\u00e9sactiver l'environnement, il vous suffira de faire : deactivate Enfin, pour supprimer l'environnement, car une manipulation a \u00e9t\u00e9 mal faite, fermer toutes les consoles ou Visual Studio Code et supprimer simplement le dossier pronochain .","title":"3. Activation de l'environnement de d\u00e9veloppement virtuel"},{"location":"generation_nft/#4-installation-des-packages","text":"Pour installer tous les packages n\u00e9cessaires pour faire fonctionner la solution, il vous suffit de taper ces commandes \u00e0 la suite : python -m pip install -r requirements.txt python -m pip install -r requirements-dev.txt python -m pip install -e . Le fichier requirements.txt est l'\u00e9quivalent du fichier package.json. Il comporte la liste de tous les packages. Lorsque vous souhaitez ajouter un nouveau package, il faudra IMP\u00c9RATIVEMENT ajouter python -m devant pip install puis d'ajouter le nom du package dans requirements.txt . Une documentation a \u00e9t\u00e9 faite uniquement \u00e0 ce sujet, pour la consulter cliquez ici .","title":"4. Installation des packages"},{"location":"generation_nft/#5-initialisation-des-variables-denvironnements","text":"Notre application aura besoin de plusieurs variables d'environnements en guise de configuration . Par exemple, la connexion \u00e0 la base de donn\u00e9es. Une documentation sera cr\u00e9\u00e9e pour conna\u00eetre toutes les variables d'environnements \u00e0 modifier. Un fichier est important pour cette partie : .env.example : contient les variables sans leur vraie valeur avec leur pattern. Le fait qu'il n'y ait pas de valeur est plut\u00f4t logique. Nous n'allons pas mettre dans le r\u00e9pertoire GIT les cl\u00e9s secr\u00e8tes. La premi\u00e8re manipulation est de copier-coller le fichier et de le renommer en supprimant le '.example' . Une fois copi\u00e9, passez \u00e0 l'\u00e9tape suivante. Warning Il faut modifier les variables d'environnements pour que la solution fonctionne . Une documentation a \u00e9t\u00e9 r\u00e9dig\u00e9e.","title":"5. Initialisation des variables d'environnements"},{"location":"generation_nft/#6-installation-et-demarrage-du-conteneur-docker","text":"Toujours dans la m\u00eame console, \u00e0 la racine du projet. Un conteneur docker a \u00e9t\u00e9 configur\u00e9 pour vous faciliter le d\u00e9veloppement, mais aussi le d\u00e9ploiement. Une fois le conteneur docker lanc\u00e9 en mode d\u00e9veloppement, vous pourrez d\u00e9marrer l'API ou acc\u00e9der au service externe de la documentation. L' API sera joignable via http://localhost:8000 et le swagger via http://localhost:8000/docs . Pour cr\u00e9er et d\u00e9marrer le conteneur au premier lancement : docker-compose pull && docker-compose -f docker-compose.yml -f docker-compose.dev.yml up --build -d Pour simplement d\u00e9marrer le docker apr\u00e8s le premier lancement : docker-compose up -d Version production Un autre docker-compose existe pour la version de production qui lance automatiquement l'API et qui n'a pas les packages de d\u00e9veloppement ni la documentation. Par la suite, pour d\u00e9marrer l'API en d\u00e9veloppement, il suffira d'ex\u00e9cuter cette commande sur votre terminal VSCode (par exemple) : docker exec -it pronochain_fastapi python app/launch.py -d Ne pas oulier \"-d\" Le fichier launch.py va d\u00e9marrer l'API en version d\u00e9veloppement avec cette commande : uvicorn app.generation_nft_api.main:app --host 0.0.0.0 --debug --reload --host : rends accessible l'application depuis l'IP du serveur ou depuis votre local. --debug : nom assez explicite pour comprendre --reload : si vous modifiez le code dans l'API, celle-ci red\u00e9marrera automatiquement. Attention, si une erreur 500 (erreur serveur) est d\u00e9tect\u00e9e, il faudra red\u00e9marrer l'API avec la commande . Le choix de ne pas d\u00e9marrer l'API au lancement du docker a \u00e9t\u00e9 fait pour plusieurs raisons : - Plus simple \u00e0 debug . Lorsque vous allez print() une variable, celle-ci s'affichera dans votre console. - Plus facilement r\u00e9demarrable . Lorsqu'une erreur syst\u00e8me intervient, l'API est arr\u00eat\u00e9. Si l'API \u00e9tait lanc\u00e9 via le docker, il faudrait relancer \u00e0 chaque fois le conteneur.","title":"6. Installation et d\u00e9marrage du conteneur docker"},{"location":"generation_nft/#7-commencer-a-developper","text":"Les six premi\u00e8res \u00e9tapes vont vous permettre de d\u00e9marrer l'application . Cependant, il reste encore une \u00e9tape pour pouvoir d\u00e9velopper correctement : la configuration pre-commit . Lorsque vous allez essayer de push votre code sur le r\u00e9pertoire GIT sur la branche develop ou master, la pipeline va d\u00e9ployer le code sur les serveurs. Lors du build de la solution sur le serveur, une \u00e9tape de v\u00e9rification du respect de diff\u00e9rentes r\u00e8gles du langage python est lanc\u00e9. Si jamais votre code n'est pas conforme , le build ne pourra pas s'effectuer . Pour vous faciliter le travail, plusieurs v\u00e9rifications de votre c\u00f4t\u00e9 vont \u00eatre r\u00e9alis\u00e9es \u00e0 chaque commit. Certaines modifieront le code automatiquement, d'autres vous indiqueront les changements \u00e0 suivre. Toutes ces v\u00e9rifications se font gr\u00e2ce au package pre-commit. Pour l'installer et le configurer, rendez-vous sur la documentation dans la cat\u00e9gorie Configuration/Pre-commit .","title":"7. Commencer \u00e0 d\u00e9velopper"},{"location":"generation_nft/#explication-docker","text":"","title":"Explication docker"},{"location":"generation_nft/#docker-compose","text":"","title":"Docker-compose"},{"location":"generation_nft/#contenu","text":"Le docker-compose est divis\u00e9 en trois fichiers . Le premier fichier docker-compose.yml est un fichier global \u00e0 tous les environnements. Il permet de configurer ce qui sera commun pour la version d\u00e9veloppement et la version production . Voici le contenu du fichier docker-compose.yml : docker-compose.yml version : '3.9' services : api : build : context : . dockerfile : ./dockerfiles/Dockerfile.api env_file : - app/.env ports : - '${API_PORT:-8000}:8000' container_name : pronochain_fastapi Pour faire simple, les deux environnements auront besoin de l'API sur le port 8000, valeur par d\u00e9faut. Pour changer ce port, il suffit de modifier la variable API_PORT dans le fichier .env . Je n'expliquerai pas le fichier pour la production, il est tr\u00e8s similaire \u00e0 celui du fichier de d\u00e9veloppement et il n'est pas n\u00e9cessairement utile pour l'initialisation de l'application en mode d\u00e9velopppement. Je vous invite tout de m\u00eame \u00e0 le consulter : docker-compose.prod.yml docker-compose nous permet de surcharger les diff\u00e9rentes images et leurs propri\u00e9t\u00e9s. Le fichier docker-compose.dev.yml : docker-compose.dev.yml version : '3.9' services : api : build : context : . args : NODE_ENV : development command : tail -f /dev/null volumes : - .:/src Nous surchargeons l'image api avec un argument r\u00e9cup\u00e9rable depuis le Dockerfile et une command . tail -f /dev/null Elle permet de maintenir allum\u00e9 l'image docker. Sans cette commande, le docker s'allumerait et s'\u00e9teindrait directement car il n'aurait rien \u00e0 ex\u00e9cuter.","title":"Contenu"},{"location":"generation_nft/#dockerfile","text":"","title":"Dockerfile"},{"location":"generation_nft/#build","text":"Lorsque que l'on va build le conteneur de la solution, le fichier Dockerfile \u00e0 la racine sera appel\u00e9 pour l'image api . Dans le docker-compose, c'est la propri\u00e9t\u00e9 build qui s'occupe d'all\u00e9e chercher ce fameux fichier. Dans notre cas, comme il est \u00e0 la racine, la propri\u00e9t\u00e9 est \u00e9gale \u00e0 \".\" . \u00c0 l'int\u00e9rieur du fichier se trouve plusieurs commandes qui ont pour seul et uniquement but de fournir toute la configuration manquante \u00e0 l'image pour pouvoir fonctionner. Par exemple, pour pouvoir faire fonctionner FastAPI, il faut obligatoirement les packages. C'est le Dockerfile qui va copier les fichiers importants et les coller dans le volume du conteneur. Dockerfile FROM python:3.9 ARG NODE_ENV WORKDIR /src ENV PYTHONPATH = /src ENV PYTHONDONTWRITEBYTECODE 1 ENV PYTHONUNBUFFERED 1 RUN apt-get update RUN apt-get install ffmpeg libsm6 libxext6 -y COPY . /src RUN --mount = type = cache,target = /root/.cache \\ pip install --upgrade pip RUN --mount = type = cache,target = /root/.cache \\ pip install -r requirements.txt RUN --mount = type = cache,target = /root/.cache \\ if [ \" $NODE_ENV \" = \"development\" ] ; \\ then pip install -r requirements-dev.txt ; \\ fi RUN if [ \" $NODE_ENV \" = \"development\" ] ; \\ then pip install -e . ; \\ else pip install . ; \\ fi Ce script va cr\u00e9er un r\u00e9pertoire dans le volume du conteneur docker , copier-coller les diff\u00e9rents fichiers de la solution dans ce dossier puis lancer l' installation des packages . Gr\u00e2ce \u00e0 l'argument NODE_ENV , le fichier sait s'il doit installer les packages pour le d\u00e9veloppement ou non. En mode d\u00e9veloppement , le volume du docker-compose est le m\u00eame que le WORKDIR de l'image . Cela signifie que la commande COPY est inutile et que lorsque qu'un fichier en local sera modifi\u00e9, le docker r\u00e9cup\u00e9ra automatiquement la mise \u00e0 jour. En version de production , le volume est diff\u00e9rent du WORKDIR de l'image. Dans ce cas, le fichier .dockerignore sera accessible et ne copiera pas tous les fichiers/dossiers. Lorsque vous voulez ajouter un package, il faut obligatoirement rebuild l'image . A chaque fois qu'un build est fait, il va ret\u00e9l\u00e9charger tous ces packages ce qui peut prendre du temps. Pour \u00e9vitez d'attendre, vous pouvez activer la fonctionnalit\u00e9 BuildKi t sur docker. Elle permet de mettre en cache les diff\u00e9rents fichiers que vous souhaitez. --mount=type=cache,target=/root/.cache Cette commande dans le Dockerfile permet de sp\u00e9cifier quels \u00e9lements seront en cache . Pour activer BuildKit , il suffit de modifier le fichier daemon.js (pour linux, il est assez facile de trouver son emplacement gr\u00e2ce \u00e0 internet). En ce qui concerne Windows, si vous utilisez l'interface : Settings -> Docker Engine , sinon dans votre : Users -> TonUser -> .docker Si vous n'avez pas de daemon.json, vous pouvez le cr\u00e9er. Ajouter : \"features\" : { \"buildkit\" : true } Le fichier doit ressembler \u00e0 : { \"builder\" : { \"gc\" : { \"defaultKeepStorage\" : \"20GB\" , \"enabled\" : true } }, \"debug\" : true , \"experimental\" : false , \"insecure-registries\" : [], \"registry-mirrors\" : [], \"features\" : { \"buildkit\" : true } }","title":"Build"},{"location":"generation_nft/#docker-compose_1","text":"","title":"Docker-compose"},{"location":"generation_nft/#up","text":"Lorsque l'on ex\u00e9cute la commande docker-compose up -d , le conteneur va \u00eatre cr\u00e9\u00e9, s'il n'existe pas.","title":"Up"},{"location":"generation_nft/#warnings","text":"Il faut bien distinguer l'environnement de d\u00e9veloppement virtuel au conteneur Docker . Vous d\u00e9velopper sur votre environnement, mais c'est le conteneur docker qui va r\u00e9cup\u00e9rer le code et simuler le serveur. Si vous ajoutez un package, le docker ne l'aura pas, si vous changez le code, le docker peut se mettre \u00e0 jour automatiquement.","title":"Warnings"},{"location":"generation_nft/#documentations","text":"Une fois ces \u00e9tapes termin\u00e9es, toute la documentation est disponible ici : Documentation Pronochain ou http://localhost:9090 si vous avez d\u00e9marrer votre documentation. C'est une application destin\u00e9e uniquement \u00e0 r\u00e9aliser des documentations plus esth\u00e9tiques qu'un simple markdown . Si elle n'est pas encore initialis\u00e9e, je vous invite \u00e0 aller voir le r\u00e9pertoire documentation .","title":"Documentations"},{"location":"generation_nft/configuration/pre_commit/","text":"Pre-Commit pre-commit est un outil bas\u00e9 sur les git-hooks . Un git hook est une action r\u00e9alis\u00e9e avant ou apr\u00e8s une commande git . Dans notre cas, pre-commit va s'appliquer avant la cr\u00e9ation du commit. Lorsque vous rentrerez la commande : git commit Les git-hooks de pre-commit vont se lancer. Si jamais un git-hook d\u00e9tecte que le code n'est pas conforme , vous ne pourrez pas cr\u00e9er le commit. \u00c0 l'inverse, si le code est valide, vous pourrez saisir le nom du commit et le push. Pr\u00e9-requis et listing des r\u00e8gles L'outil pre-commit par d\u00e9faut ne contient aucune r\u00e8gle . Il faut les choisir en fonction du langage et de ce que l'on souhaite. Dans le cas du langage Python, les r\u00e8gles PEP , r\u00e8gle standardisant le langage, doivent \u00eatre respect\u00e9es. De base chaque r\u00e8gle de pre-commit se mat\u00e9rialise sous la forme d'un package ( lien git ). Vous n'avez pas besoin de les installer manuellement. Ils sont tous d\u00e9j\u00e0 dans le fichier requirements-dev.txt . Voici la liste des toutes les r\u00e8gles qui seront appliqu\u00e9es pour notre solution : pre-commit-hooks GIT : Ces r\u00e8gles ne sont pas propres \u00e0 un langage. C'est les r\u00e8gles officielles du cr\u00e9ateur de l'outil pre-commit. Elles permettent de respecter quelques notions globales dans le monde du d\u00e9veloppement. Cependant, ici, nous utilisons pre-commit version python : trailing-whitespace : supprime les espaces en fin de ligne . end-of-file-fixer : s'assure que la derni\u00e8re ligne d'un fichier, m\u00eame vide, soit une nouvelle ligne . check-docstring-first : un fichier doit toujours comporter une documentation en premier et non apr\u00e8s le code. check-merge-conflict : v\u00e9rifie qu' aucun merge conflit n'est pr\u00e9sent dans un fichier. D\u00e9tecte gr\u00e2ce aux balises g\u00e9n\u00e9r\u00e9es par GIT lorsqu'il y'en a un. debug-statements : v\u00e9rifie qu' aucune fonction de debug n'est import\u00e9e. Par exemple, si un print est pr\u00e9sent, il ne sera pas possible de commit le code. fix-encoding-pragma : ajoute # - - coding: utf-8 -*- * au d\u00e9but de chaque fichiers python. black GIT : formate le code automatiquement . Doit \u00eatre combiner avec la configuration de votre visual studio code pour une utilisation optimale. Une documentation est pr\u00e9sente sur ce sujet dans \"Configuration\" -> \"Visual Studio Code\" . flake8 GIT : linter , v\u00e9rifie que les r\u00e8gles principales de python sont respect\u00e9es. Flake8 indiquera uniquement si une r\u00e8gle n'est pas respecter, il ne modifiera en aucun cas le fichier. Il existe, pour lui aussi, une extension Visual Studio Code pour vous faciliter le travail. pydocstyle GIT : v\u00e9rifie que le code python soit bien document\u00e9 . Un raccourci existe pour ajouter un pattern d'un commentaire sous une fonction. isort GIT : r\u00e9organise les imports python avec des r\u00e8gles bien pr\u00e9cises . vulture GIT : d\u00e9tecte les codes inutilis\u00e9s . gitlint GIT : v\u00e9rifie la conformit\u00e9 du message du commit . La v\u00e9rification est bas\u00e9 sur la convention config ( config-conventional ). Example: feat(face-parts): R\u00e9cup\u00e9rer les parties du visages (Task(s): #254) D\u00e9sactiver une r\u00e8gle Il est possible de d\u00e9sactiver une r\u00e8gle. Voir leur documentation pour savoir comment faire (diff\u00e8re selon les r\u00e8gles). Fichier de configuration C'est gr\u00e2ce au fichier .pre-commit-config.yaml que l'on choisit les r\u00e8gles \u00e0 installer. De plus, chaque r\u00e8gles a des param\u00e8tres qui lui est propre. En g\u00e9n\u00e9ral, ils se trouvent dans le fichier pyproject.toml . Liste de toutes les r\u00e8gles existantes Rendez-vous sur le site pre-commit.com pour voir toutes les r\u00e8gles/hooks possibles. Configuration et installation Les fichiers de configuration de pre-commit sont: .pre-commit-config.yaml setup.cfg pyproject.toml Installation Pour installer localement tous les git-hooks , il suffit de faire ces deux commandes \u00e0 la racine du projet : pre-commit install pre-commit install --hook-type commit-msg Mettre \u00e0 jour Pour mettre \u00e0 jour automatiquement la configuration de pre-commit vers les derni\u00e8res versions des d\u00e9p\u00f4ts : pre-commit autoupdate Pour mettre \u00e0 jour le package pre-commit : python -m pip install pre-commit -U Activer l'environnement virtuel avant d'ex\u00e9cuter la commande : pronochain/Scripts/activate Si vous souhaitez remettre \u00e0jour au moment du commit, il suffit de tapez cette commande : pre-commit clean Utilisation Une fois install\u00e9s, les hooks se lanceront \u00e0 chaque commit sur les fichiers modifi\u00e9s seulement . Chaque git-hook va \u00eatre v\u00e9rifi\u00e9 avant de pouvoir cr\u00e9er le commit , si un fichier modifi\u00e9 comporte une erreur, vous ne pourrez pas cr\u00e9er ce commit. Comme d\u00e9j\u00e0 expliquer pr\u00e9c\u00e9demment, certains git-hook vont modifier le code directement , certains vont juste vous indiquer les modifications \u00e0 faire ou les erreurs detect\u00e9es . Modification par un hook Si le commit est modifi\u00e9 par un hook avec pre-commit, il suffit de staged \u00e0 nouveau et de recommiter . Lancer pre-commit ind\u00e9pendamment d'un commit Vous pouvez aussi utiliser pre-commit sans les git hooks pre-commit run lance pre-commit sur les fichiers modifi\u00e9s. pre-commit run --files [file] lance pre-commit sur les fichiers indiqu\u00e9s. pre-commit run --all-files lance pre-commit sur tous les fichiers. pre-commit run flake8 lance pre-commit avec le hook indiqu\u00e9 (ici, flake8).","title":"Pre-commit"},{"location":"generation_nft/configuration/pre_commit/#pre-commit","text":"pre-commit est un outil bas\u00e9 sur les git-hooks . Un git hook est une action r\u00e9alis\u00e9e avant ou apr\u00e8s une commande git . Dans notre cas, pre-commit va s'appliquer avant la cr\u00e9ation du commit. Lorsque vous rentrerez la commande : git commit Les git-hooks de pre-commit vont se lancer. Si jamais un git-hook d\u00e9tecte que le code n'est pas conforme , vous ne pourrez pas cr\u00e9er le commit. \u00c0 l'inverse, si le code est valide, vous pourrez saisir le nom du commit et le push.","title":"Pre-Commit"},{"location":"generation_nft/configuration/pre_commit/#pre-requis-et-listing-des-regles","text":"L'outil pre-commit par d\u00e9faut ne contient aucune r\u00e8gle . Il faut les choisir en fonction du langage et de ce que l'on souhaite. Dans le cas du langage Python, les r\u00e8gles PEP , r\u00e8gle standardisant le langage, doivent \u00eatre respect\u00e9es. De base chaque r\u00e8gle de pre-commit se mat\u00e9rialise sous la forme d'un package ( lien git ). Vous n'avez pas besoin de les installer manuellement. Ils sont tous d\u00e9j\u00e0 dans le fichier requirements-dev.txt . Voici la liste des toutes les r\u00e8gles qui seront appliqu\u00e9es pour notre solution : pre-commit-hooks GIT : Ces r\u00e8gles ne sont pas propres \u00e0 un langage. C'est les r\u00e8gles officielles du cr\u00e9ateur de l'outil pre-commit. Elles permettent de respecter quelques notions globales dans le monde du d\u00e9veloppement. Cependant, ici, nous utilisons pre-commit version python : trailing-whitespace : supprime les espaces en fin de ligne . end-of-file-fixer : s'assure que la derni\u00e8re ligne d'un fichier, m\u00eame vide, soit une nouvelle ligne . check-docstring-first : un fichier doit toujours comporter une documentation en premier et non apr\u00e8s le code. check-merge-conflict : v\u00e9rifie qu' aucun merge conflit n'est pr\u00e9sent dans un fichier. D\u00e9tecte gr\u00e2ce aux balises g\u00e9n\u00e9r\u00e9es par GIT lorsqu'il y'en a un. debug-statements : v\u00e9rifie qu' aucune fonction de debug n'est import\u00e9e. Par exemple, si un print est pr\u00e9sent, il ne sera pas possible de commit le code. fix-encoding-pragma : ajoute # - - coding: utf-8 -*- * au d\u00e9but de chaque fichiers python. black GIT : formate le code automatiquement . Doit \u00eatre combiner avec la configuration de votre visual studio code pour une utilisation optimale. Une documentation est pr\u00e9sente sur ce sujet dans \"Configuration\" -> \"Visual Studio Code\" . flake8 GIT : linter , v\u00e9rifie que les r\u00e8gles principales de python sont respect\u00e9es. Flake8 indiquera uniquement si une r\u00e8gle n'est pas respecter, il ne modifiera en aucun cas le fichier. Il existe, pour lui aussi, une extension Visual Studio Code pour vous faciliter le travail. pydocstyle GIT : v\u00e9rifie que le code python soit bien document\u00e9 . Un raccourci existe pour ajouter un pattern d'un commentaire sous une fonction. isort GIT : r\u00e9organise les imports python avec des r\u00e8gles bien pr\u00e9cises . vulture GIT : d\u00e9tecte les codes inutilis\u00e9s . gitlint GIT : v\u00e9rifie la conformit\u00e9 du message du commit . La v\u00e9rification est bas\u00e9 sur la convention config ( config-conventional ). Example: feat(face-parts): R\u00e9cup\u00e9rer les parties du visages (Task(s): #254) D\u00e9sactiver une r\u00e8gle Il est possible de d\u00e9sactiver une r\u00e8gle. Voir leur documentation pour savoir comment faire (diff\u00e8re selon les r\u00e8gles). Fichier de configuration C'est gr\u00e2ce au fichier .pre-commit-config.yaml que l'on choisit les r\u00e8gles \u00e0 installer. De plus, chaque r\u00e8gles a des param\u00e8tres qui lui est propre. En g\u00e9n\u00e9ral, ils se trouvent dans le fichier pyproject.toml . Liste de toutes les r\u00e8gles existantes Rendez-vous sur le site pre-commit.com pour voir toutes les r\u00e8gles/hooks possibles.","title":"Pr\u00e9-requis et listing des r\u00e8gles"},{"location":"generation_nft/configuration/pre_commit/#configuration-et-installation","text":"Les fichiers de configuration de pre-commit sont: .pre-commit-config.yaml setup.cfg pyproject.toml","title":"Configuration et installation"},{"location":"generation_nft/configuration/pre_commit/#installation","text":"Pour installer localement tous les git-hooks , il suffit de faire ces deux commandes \u00e0 la racine du projet : pre-commit install pre-commit install --hook-type commit-msg","title":"Installation"},{"location":"generation_nft/configuration/pre_commit/#mettre-a-jour","text":"Pour mettre \u00e0 jour automatiquement la configuration de pre-commit vers les derni\u00e8res versions des d\u00e9p\u00f4ts : pre-commit autoupdate Pour mettre \u00e0 jour le package pre-commit : python -m pip install pre-commit -U Activer l'environnement virtuel avant d'ex\u00e9cuter la commande : pronochain/Scripts/activate Si vous souhaitez remettre \u00e0jour au moment du commit, il suffit de tapez cette commande : pre-commit clean","title":"Mettre \u00e0 jour"},{"location":"generation_nft/configuration/pre_commit/#utilisation","text":"Une fois install\u00e9s, les hooks se lanceront \u00e0 chaque commit sur les fichiers modifi\u00e9s seulement . Chaque git-hook va \u00eatre v\u00e9rifi\u00e9 avant de pouvoir cr\u00e9er le commit , si un fichier modifi\u00e9 comporte une erreur, vous ne pourrez pas cr\u00e9er ce commit. Comme d\u00e9j\u00e0 expliquer pr\u00e9c\u00e9demment, certains git-hook vont modifier le code directement , certains vont juste vous indiquer les modifications \u00e0 faire ou les erreurs detect\u00e9es . Modification par un hook Si le commit est modifi\u00e9 par un hook avec pre-commit, il suffit de staged \u00e0 nouveau et de recommiter . Lancer pre-commit ind\u00e9pendamment d'un commit Vous pouvez aussi utiliser pre-commit sans les git hooks pre-commit run lance pre-commit sur les fichiers modifi\u00e9s. pre-commit run --files [file] lance pre-commit sur les fichiers indiqu\u00e9s. pre-commit run --all-files lance pre-commit sur tous les fichiers. pre-commit run flake8 lance pre-commit avec le hook indiqu\u00e9 (ici, flake8).","title":"Utilisation"},{"location":"generation_nft/configuration/variables_environnements/","text":"Explication La g\u00e9n\u00e9ration NFT doit se connecter \u00e0 plusieurs services externes et d\u00e9marrer plusieurs services internes . Tous ces services doivent \u00eatre configurer pour s'y connecter, le param\u00e9trer et le d\u00e9marrer/utiliser . Pour ce faire, un fichier va permettre d'ajouter et de modifier les variables d'environnements. .env : toutes les variables d'environnements en commun puis les variables pour la version d\u00e9veloppement prefix\u00e9 de DEV et les variables de la version production pr\u00e9fix\u00e9 PROD . Ajouter une variable d'environnement Plusieurs \u00e9tapes sont n\u00e9cessaires pour ajouter une variables d'environnements. Modifier le fichier .env . Si c'est une variable utilis\u00e9e en production et en d\u00e9veloppement pas besoin de pr\u00e9fix\u00e9 le nom de la variable. Sinon pr\u00e9fix\u00e9 le pour identifier sur quel environnement doit \u00eatre appliqu\u00e9e la variable. Pour atteindre une variable pr\u00e9fix\u00e9, il suffira d'enlever le pr\u00e9fix. Ne pas nommer les variables d'environnements pr\u00e9fix\u00e9 comme les variables d'environnements communes. Acc\u00e9der au fichier app/settings.py et ajouter la variable dans la classe GlobalSettings . Si c'est une variable commune : VAR_COMMUNE : Optional [ TYPE DE LA VAR ] = Field ( None , env = \"VAR_COMMUNE\" ) Si c'est une variable pr\u00e9fix\u00e9e : VAR_SANS_LE_PREFIX : Optional [ int ] = None R\u00e9d\u00e9marrer le conteneur docker . Pour utiliser une variable d'environnement dans le code, il vous suffira d'importer la variable settings dans le fichier settings.py from app.settings import settings debug = settings . DEBUG Listing Cette page nous permet de faire la liste de toutes les variables d'environnements, d'expliquer leur utilit\u00e9es et de renvoyer vers un fichier externe comportant les valeurs \u00e0 remplir pour faire fonctionner la solution. Variables communes DEBUG Si DEBUG = True signifie que la solution d\u00e9marre sur la version de d\u00e9veloppement. C'est la propri\u00e9t\u00e9 qui permet de savoir sur quel environnement est simul\u00e9e l'application. API_PORT Port de l'API, par d\u00e9faut 8000. Laisser cette valeur sauf si ce port est d\u00e9j\u00e0 utilis\u00e9 sur votre syst\u00e8me. API_IP IP l'API, par d\u00e9faut 0.0.0.0. Variables d\u00e9veloppements Variables productions","title":"Variables d'environnements"},{"location":"generation_nft/configuration/variables_environnements/#explication","text":"La g\u00e9n\u00e9ration NFT doit se connecter \u00e0 plusieurs services externes et d\u00e9marrer plusieurs services internes . Tous ces services doivent \u00eatre configurer pour s'y connecter, le param\u00e9trer et le d\u00e9marrer/utiliser . Pour ce faire, un fichier va permettre d'ajouter et de modifier les variables d'environnements. .env : toutes les variables d'environnements en commun puis les variables pour la version d\u00e9veloppement prefix\u00e9 de DEV et les variables de la version production pr\u00e9fix\u00e9 PROD .","title":"Explication"},{"location":"generation_nft/configuration/variables_environnements/#ajouter-une-variable-denvironnement","text":"Plusieurs \u00e9tapes sont n\u00e9cessaires pour ajouter une variables d'environnements. Modifier le fichier .env . Si c'est une variable utilis\u00e9e en production et en d\u00e9veloppement pas besoin de pr\u00e9fix\u00e9 le nom de la variable. Sinon pr\u00e9fix\u00e9 le pour identifier sur quel environnement doit \u00eatre appliqu\u00e9e la variable. Pour atteindre une variable pr\u00e9fix\u00e9, il suffira d'enlever le pr\u00e9fix. Ne pas nommer les variables d'environnements pr\u00e9fix\u00e9 comme les variables d'environnements communes. Acc\u00e9der au fichier app/settings.py et ajouter la variable dans la classe GlobalSettings . Si c'est une variable commune : VAR_COMMUNE : Optional [ TYPE DE LA VAR ] = Field ( None , env = \"VAR_COMMUNE\" ) Si c'est une variable pr\u00e9fix\u00e9e : VAR_SANS_LE_PREFIX : Optional [ int ] = None R\u00e9d\u00e9marrer le conteneur docker . Pour utiliser une variable d'environnement dans le code, il vous suffira d'importer la variable settings dans le fichier settings.py from app.settings import settings debug = settings . DEBUG","title":"Ajouter une variable d'environnement"},{"location":"generation_nft/configuration/variables_environnements/#listing","text":"Cette page nous permet de faire la liste de toutes les variables d'environnements, d'expliquer leur utilit\u00e9es et de renvoyer vers un fichier externe comportant les valeurs \u00e0 remplir pour faire fonctionner la solution.","title":"Listing"},{"location":"generation_nft/configuration/variables_environnements/#variables-communes","text":"DEBUG Si DEBUG = True signifie que la solution d\u00e9marre sur la version de d\u00e9veloppement. C'est la propri\u00e9t\u00e9 qui permet de savoir sur quel environnement est simul\u00e9e l'application. API_PORT Port de l'API, par d\u00e9faut 8000. Laisser cette valeur sauf si ce port est d\u00e9j\u00e0 utilis\u00e9 sur votre syst\u00e8me. API_IP IP l'API, par d\u00e9faut 0.0.0.0.","title":"Variables communes"},{"location":"generation_nft/configuration/variables_environnements/#variables-developpements","text":"","title":"Variables d\u00e9veloppements"},{"location":"generation_nft/configuration/variables_environnements/#variables-productions","text":"","title":"Variables productions"},{"location":"generation_nft/configuration/vs_code/","text":"Visual Studio Code Extensions VSCode Visual Studio Code permet d' installer plusieurs extensions afin de simplifier le d\u00e9veloppement . Voici une liste exhaustive de plusieurs d'entre elles pour faciliter le d\u00e9veloppement sur la solution python. Pre-commit Pour que votre Visual Studo Code affiche les erreurs dans votre code et qu'il formate les fichiers \u00e0 chaque sauvegarde , installez l'extension Python . Cette extension va permettre de faire fonctionner le linter flake8 et le formatter black . Cependant pour que Visual Studio Code applique les r\u00e8gles, il faut que les deux packages soient install\u00e9s sur le python de votre ordinateur. pip install flake8 pip install black D\u00e9sactiver l'environnement virtuel avant d'ex\u00e9cuter les commandes : deactivate Une fois ces deux installations effectu\u00e9es, faites Ctrl + Shift + P puis tapez Settings dans la barre de recherche. S\u00e9lectionnez \"Preferences: Open Settings (JSON)\" (version anglaise). Ins\u00e9rez ce bout de code JSON et sauvegardez. \"python.linting.enabled\" : true , \"python.linting.flake8Enabled\" : true , \"python.formatting.provider\" : \"black\" , \"[python]\" : { \"editor.formatOnSave\" : true , \"editor.insertSpaces\" : true , \"editor.tabSize\" : 4 }, \"editor.rulers\" : [ 88 ], Avec la configuration ci-dessus, Visual Studio Code affichera les erreurs dans votre code et formatera le code \u00e0 chaque sauvegarde . Identifier votre fichier en tant que fichier Python Commentaire Une extension Visual Studio Code existe pour g\u00e9n\u00e9rer un template d'un commentaire . Tr\u00e8s utile sous les fonctions et les classes . L'extension s'appel Python Docstring Generator . Apr\u00e8s l'installation positionnez vous en dessous d'une fonction et faites Ctrl + Shift + 2 . Nous utilisons la convention de google pour g\u00e9n\u00e9rer les commentaires","title":"Visual Studio Code"},{"location":"generation_nft/configuration/vs_code/#visual-studio-code","text":"","title":"Visual Studio Code"},{"location":"generation_nft/configuration/vs_code/#extensions-vscode","text":"Visual Studio Code permet d' installer plusieurs extensions afin de simplifier le d\u00e9veloppement . Voici une liste exhaustive de plusieurs d'entre elles pour faciliter le d\u00e9veloppement sur la solution python.","title":"Extensions VSCode"},{"location":"generation_nft/configuration/vs_code/#pre-commit","text":"Pour que votre Visual Studo Code affiche les erreurs dans votre code et qu'il formate les fichiers \u00e0 chaque sauvegarde , installez l'extension Python . Cette extension va permettre de faire fonctionner le linter flake8 et le formatter black . Cependant pour que Visual Studio Code applique les r\u00e8gles, il faut que les deux packages soient install\u00e9s sur le python de votre ordinateur. pip install flake8 pip install black D\u00e9sactiver l'environnement virtuel avant d'ex\u00e9cuter les commandes : deactivate Une fois ces deux installations effectu\u00e9es, faites Ctrl + Shift + P puis tapez Settings dans la barre de recherche. S\u00e9lectionnez \"Preferences: Open Settings (JSON)\" (version anglaise). Ins\u00e9rez ce bout de code JSON et sauvegardez. \"python.linting.enabled\" : true , \"python.linting.flake8Enabled\" : true , \"python.formatting.provider\" : \"black\" , \"[python]\" : { \"editor.formatOnSave\" : true , \"editor.insertSpaces\" : true , \"editor.tabSize\" : 4 }, \"editor.rulers\" : [ 88 ], Avec la configuration ci-dessus, Visual Studio Code affichera les erreurs dans votre code et formatera le code \u00e0 chaque sauvegarde . Identifier votre fichier en tant que fichier Python","title":"Pre-commit"},{"location":"generation_nft/configuration/vs_code/#commentaire","text":"Une extension Visual Studio Code existe pour g\u00e9n\u00e9rer un template d'un commentaire . Tr\u00e8s utile sous les fonctions et les classes . L'extension s'appel Python Docstring Generator . Apr\u00e8s l'installation positionnez vous en dessous d'une fonction et faites Ctrl + Shift + 2 . Nous utilisons la convention de google pour g\u00e9n\u00e9rer les commentaires","title":"Commentaire"},{"location":"generation_nft/fonctionnalites/face_aligner/","text":"Alignement du visage D\u00e9finition Le face aligner est un algorithme permettant d'aligner les yeux sur une ligne compl\u00e8tement horizontale . Cet algorithme intervient juste apr\u00e8s la d\u00e9tection des diff\u00e9rents points du visage. Il est n\u00e9cessaire de passer par cette \u00e9tape pour s'assurer d'un r\u00e9sultat convenable . Mauvais MBAPPE Bon MBAPPE Contexte Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV et MediaPipe , il a \u00e9t\u00e9 tr\u00e8s facile de r\u00e9cup\u00e9rer plusieurs points importants du visage afin de tracer les contours . Cependant, la question de l' inclinaison de la t\u00eate devait \u00eatre prise en compte. En soit, une t\u00eate inclin\u00e9e n'est pas un probl\u00e8me lorsque l'on g\u00e9n\u00e8re un dessin du visage. Le dessin pourra toujours \u00eatre r\u00e9align\u00e9 par rapport \u00e0 une ligne horizontale par la suite. Cependant, le machine learning sur la direction du regard sera fortement impact\u00e9e si tous les visages ne sont pas inclin\u00e9s de la m\u00eame fa\u00e7on . Il aurait \u00e9t\u00e9 beaucoup plus compliqu\u00e9 de faire du machine learning sur des t\u00eates inclin\u00e9es , les r\u00e9sultats auraient \u00e9t\u00e9 des faux positifs ou n\u00e9gatifs . Aligner le visage Pour pouvoir aligner un visage, il faut avoir des points de r\u00e9f\u00e9rences , pour savoir si le visage est inclin\u00e9. Le principe de l'algorithme est de reprendre les points des deux yeux . Pour chaque oeil, on reprend le centre entre ces deux points. Il y aura donc deux centres pour chaque oeil , pour un visage. Derni\u00e8re \u00e9tape, tracer une ligne entre ces deux points centrales . La ligne qui appara\u00eetra aura un angle par rapport \u00e0 une ligne compl\u00e8tement horizontale. Le but ensuite est de redresser l'image pour que la barre soit horizontale . Informations compl\u00e9mentaires Malheureusement, je pense qu'il n'est pas n\u00e9cessaire de montrer l'algorithme, c'est un calcul complexe math\u00e9matique m\u00e9langeant manipulation de pixel et de distance. Si vous souhaitez comprendre l'algorithme, je vous conseille ce tutoriel .","title":"Alignement du visage"},{"location":"generation_nft/fonctionnalites/face_aligner/#alignement-du-visage","text":"","title":"Alignement du visage"},{"location":"generation_nft/fonctionnalites/face_aligner/#definition","text":"Le face aligner est un algorithme permettant d'aligner les yeux sur une ligne compl\u00e8tement horizontale . Cet algorithme intervient juste apr\u00e8s la d\u00e9tection des diff\u00e9rents points du visage. Il est n\u00e9cessaire de passer par cette \u00e9tape pour s'assurer d'un r\u00e9sultat convenable .","title":"D\u00e9finition"},{"location":"generation_nft/fonctionnalites/face_aligner/#contexte","text":"Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV et MediaPipe , il a \u00e9t\u00e9 tr\u00e8s facile de r\u00e9cup\u00e9rer plusieurs points importants du visage afin de tracer les contours . Cependant, la question de l' inclinaison de la t\u00eate devait \u00eatre prise en compte. En soit, une t\u00eate inclin\u00e9e n'est pas un probl\u00e8me lorsque l'on g\u00e9n\u00e8re un dessin du visage. Le dessin pourra toujours \u00eatre r\u00e9align\u00e9 par rapport \u00e0 une ligne horizontale par la suite. Cependant, le machine learning sur la direction du regard sera fortement impact\u00e9e si tous les visages ne sont pas inclin\u00e9s de la m\u00eame fa\u00e7on . Il aurait \u00e9t\u00e9 beaucoup plus compliqu\u00e9 de faire du machine learning sur des t\u00eates inclin\u00e9es , les r\u00e9sultats auraient \u00e9t\u00e9 des faux positifs ou n\u00e9gatifs .","title":"Contexte"},{"location":"generation_nft/fonctionnalites/face_aligner/#aligner-le-visage","text":"Pour pouvoir aligner un visage, il faut avoir des points de r\u00e9f\u00e9rences , pour savoir si le visage est inclin\u00e9. Le principe de l'algorithme est de reprendre les points des deux yeux . Pour chaque oeil, on reprend le centre entre ces deux points. Il y aura donc deux centres pour chaque oeil , pour un visage. Derni\u00e8re \u00e9tape, tracer une ligne entre ces deux points centrales . La ligne qui appara\u00eetra aura un angle par rapport \u00e0 une ligne compl\u00e8tement horizontale. Le but ensuite est de redresser l'image pour que la barre soit horizontale . Informations compl\u00e9mentaires Malheureusement, je pense qu'il n'est pas n\u00e9cessaire de montrer l'algorithme, c'est un calcul complexe math\u00e9matique m\u00e9langeant manipulation de pixel et de distance. Si vous souhaitez comprendre l'algorithme, je vous conseille ce tutoriel .","title":"Aligner le visage"},{"location":"generation_nft/fonctionnalites/face_detect/","text":"D\u00e9tection du visage D\u00e9finition La d\u00e9tection d'un visage est tr\u00e8s r\u00e9pandue dans le monde du machine learning . Il existe plusieurs types de mod\u00e8les de machine learning afin de d\u00e9tecter un visage sur une image. Lors de cette documentation, nous verrons la liste des mod\u00e8les en \u00e9num\u00e9rant leurs avatnages et leurs inconv\u00e9nients afin d'en choisir un. Pas de visage VISAGE Contexte Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Il existe plusieurs mod\u00e8les pour d\u00e9tecter un visage. Ce qui a \u00e9t\u00e9 le plus compliqu\u00e9, c'est de choisir le mod\u00e8le. Pour information, les crit\u00e8res pour le choix d'un mod\u00e8le ont \u00e9t\u00e9 : la pr\u00e9cision et la performance . Il faut que le mod\u00e8le soit tr\u00e8s pr\u00e9cis , c'est-\u00e0-dire qu'il ne d\u00e9tecte pas des visages quand il n'y en a pas et vice-versa . De plus, il faut que la d\u00e9tection du visage soit rapide . Cette rapidit\u00e9 d\u00e9pend notamment du mat\u00e9riel informatique qui est utilis\u00e9 pour faire tourner l'algorithme. Dans les cas d'algorithme sur des images, donc du graphisme, c'est la carte graphique qui sera la plus puissante : le GPU . Cependant, ce syst\u00e8me doit \u00eatre activ\u00e9 pour certains packages comme OpenCV . Carte graphique et compatibilit\u00e9 Toutes les cartes graphiques ne sont pas compatibles pour pouvoir r\u00e9aliser ce genre de t\u00e2che. Il est donc pr\u00e9f\u00e9rable que la d\u00e9tection du visage se fasse sur le processeur : le CPU . OpenCV et Dlib Petit rappel pour la suite, OpenCV est une librairie python. Elle est tr\u00e8s utilis\u00e9e dans la manipulation d'image . La librairie Dlib est similaire, mais est plus tourn\u00e9e vers le machine learning. Les diff\u00e9rents mod\u00e8les OpenCV et Haar cascades Deep learning de d\u00e9tection de visager int\u00e9gr\u00e9 \u00e0 la librairie OpenCV Dlib HOG + Linear SVM implementation Dlib CNN d\u00e9tection de visage Haar cascades L' haar cascades est un algorithme permettant de trouver plusieurs caract\u00e9ristiques pour une image. Il ne d\u00e9tecte pas uniquement les visages. Il se base notamment sur le mod\u00e8le de machine learning Adaboost . Le principe est plut\u00f4t simple, il a \u00e9t\u00e9 entrain\u00e9 avec plusieurs cat\u00e9gories d'image, avec pour chaque image , une description . Il aura ensuite la facult\u00e9 de pr\u00e9dire des objets dans l'image gr\u00e2ce aux caract\u00e9ristiques similaires aux images d'entra\u00eenements . Pour : Tr\u00e8s rapide . Pas besoin de le faire tourner sur un GPU . Contre : Haut pourcentage de faux n\u00e9gatif , en d'autres termes, une pr\u00e9cision m\u00e9diocre . Dois \u00eatre configur\u00e9 pour qu'il fonctionne correctement (correctement signifie m\u00e9diocre). OpenCV face detector int\u00e9gr\u00e9 La d\u00e9tection du visage int\u00e9gr\u00e9 dans la librairie d' OpenCV utilise le syst\u00e8me de SSD ( Single Shot Detector ). Comme son nom l'indique, en un seul essai, il peut d\u00e9tecter plusieurs objets . Il ne d\u00e9tecte pas seulement les visages, mais peut d\u00e9tecter une multitude d'objets sur la m\u00eame image en une seule fois. Il est plus compliqu\u00e9 d'expliquer le fonctionnement de cet algorithme, si vous voulez avoir plus d'information rendez-vous sur ce tutoriel . Pour : Efficace . Utilise un mod\u00e8le de deep learning r\u00e9cent . Pas de param\u00e9trage . Pas besoin de faire tourner l'algorithme sur un GPU . Mais sera tout de m\u00eame plus rapide dessus. Contre : Plus efficace que le mod\u00e8le Haar cascades , mais reste encore assez peu pr\u00e9cis . Possibilit\u00e9 de biais et d' erreurs \u00e0 cause des couleurs des images. Dlib HOG + Linear SVM HOG signifie Histograms of Oriented Gradients et linear SVM est le mod\u00e8le de machine learning utilis\u00e9 pour cette m\u00e9thode de d\u00e9tection de visage. Le concept HOG est plus compliqu\u00e9 \u00e0 comprendre que le linear SVM. Pour ces deux concepts, je vous conseille d'aller voir ce tutoriel . Pour : Plus pr\u00e9cis que le mod\u00e8le Haar cascades . Beaucoup plus stable que les deux mod\u00e8les d'avant. Facile d'utilisation et continue d'\u00eatre mise \u00e0 jour par son cr\u00e9ateur gr\u00e2ce \u00e0 la librairie Dlib . Extr\u00eamement bien document\u00e9 . Contre : Fonctionne uniquement sur les visages frontales ( nous arrange grandement ). Pas aussi pr\u00e9cis que d'autres mod\u00e8les bas\u00e9s sur le deep learning ( reste assez n\u00e9gligeable, se focus sur les visages de face ). Assez peu co\u00fbteux en termes de performance, mais reste un peu plus long que les deux mod\u00e8les pr\u00e9c\u00e9dant. De l'ordre de la milliseconde de plus (0.001 seconde). Dlib CNN face detector CNN signifie Convolutional Neural Network , comme son nom l'indique, il utilise un syst\u00e8me de r\u00e9seau neuronal pour d\u00e9tecter le visage. Dans le monde de l'intelligence artificielle, voici l'ordre des performances : machine learning < deep learning < r\u00e9seau neuronal . La d\u00e9tection sera tr\u00e8s pr\u00e9cise. Cependant, les r\u00e9seaux neuronaux sont tr\u00e8s gourmands . Pour : Tr\u00e8s pr\u00e9cis pour d\u00e9tecter les visages. Taille du mod\u00e8le (fichier) inf\u00e9rieur \u00e0 1MB . Facile \u00e0 impl\u00e9menter et tr\u00e8s document\u00e9 . Contre : Le code est plus verbeux et n\u00e9cessite de multiples de conversions de variables. Tr\u00e8s lent sur CPU. Choix Il a \u00e9t\u00e9 d\u00e9cid\u00e9 de prendre le mod\u00e8le HOG + linear SVM . Plusieurs raisons \u00e0 ce choix : Il ne d\u00e9tecte pas les visages de profil . Ce qui nous facilite le machine learning : tilt learning ( voir la documentation ). Il est rapide sur CPU . Il est plus pr\u00e9cis que les deux autres mod\u00e8les fonctionnant sur CPU. Facile d'impl\u00e9mentation avec beaucoup de documentation . Enfin, il est encore maintenu . Utilisation du mod\u00e8le Comme indiqu\u00e9 dans les diff\u00e9rentes raisons du choix du mod\u00e8le, il est tr\u00e8s facile d'impl\u00e9menter le mod\u00e8le HOG + linear SVM . Cela signifie entre autres, qu'en tr\u00e8s peu de ligne de code , le mod\u00e8le va d\u00e9tecter un visage sur une image et retourner les coordonn\u00e9es de celui-ci afin de pouvoir l'isoler du reste de l'image. Il faut tout d'abord avoir le fichier du mod\u00e8le. Vous le trouverez dans le chemin \"app/generation_nft/librairies/face_detect/pre_trained\" . T\u00e9l\u00e9chargement du fichier Le fichier n'est pas dans le r\u00e9pertoire GIT . Une v\u00e9rification au premier lancement est r\u00e9alis\u00e9e pour d\u00e9tecter la pr\u00e9sence du mod\u00e8le. Si ce n'est pas le cas, le script le t\u00e9l\u00e9charge . Ce mod\u00e8le est h\u00e9berg\u00e9 sur notre Google Drive . Puis l'importer dans le code : detector = open_cv . dnn . readNetFromCaffe ( caffe_prototxt_path , caffe_model_path ) Pour pouvoir utiliser ce mod\u00e8le, il faut imp\u00e9rativement convertir l'image en blob , pour ce faire, la librairie OpenCV comporte une fonction pour le faire simplement. image = open_cv . imread ( image_path ) # R\u00e9cup\u00e8re l'image est le converti en matrice numpy ( height_image , width_image ) = image . shape [: 2 ] blob_image = open_cv . dnn . blobFromImage ( open_cv . resize ( image , ( width_image , height_image )), 1.0 , ( width_image , height_image ), ( 104.0 , 177.0 , 123.0 ), ) 1.0 correspond \u00e0 un changement d'\u00e9chelle . Ici, aucune modification. (104.0, 177.0, 123.0) correspond \u00e0 une colorisation de l'image . Il ne reste plus qu'\u00e0 int\u00e9grer le blob dans le mod\u00e8le de d\u00e9tection et de r\u00e9cup\u00e9rer les coordonn\u00e9es du/des visages d\u00e9tect\u00e9s . detector . setInput ( blob_image ) face_detections = detector . forward () Par la suite, avec chaque coordonn\u00e9e, on pourra isoler une partie de l'image . Bien s\u00fbr, il faudra prendre une marge car la d\u00e9tection du visage se focalise sur les \u00e9l\u00e9ments du visage et non la t\u00eate enti\u00e8re . Calcul de la marge Pour \u00eatre s\u00fbr d'avoir la totalit\u00e9 de la t\u00eate , la marge est calcul\u00e9e en fonction de la taille du visage d\u00e9tect\u00e9 .","title":"D\u00e9tection d'un visage"},{"location":"generation_nft/fonctionnalites/face_detect/#detection-du-visage","text":"","title":"D\u00e9tection du visage"},{"location":"generation_nft/fonctionnalites/face_detect/#definition","text":"La d\u00e9tection d'un visage est tr\u00e8s r\u00e9pandue dans le monde du machine learning . Il existe plusieurs types de mod\u00e8les de machine learning afin de d\u00e9tecter un visage sur une image. Lors de cette documentation, nous verrons la liste des mod\u00e8les en \u00e9num\u00e9rant leurs avatnages et leurs inconv\u00e9nients afin d'en choisir un.","title":"D\u00e9finition"},{"location":"generation_nft/fonctionnalites/face_detect/#contexte","text":"Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Il existe plusieurs mod\u00e8les pour d\u00e9tecter un visage. Ce qui a \u00e9t\u00e9 le plus compliqu\u00e9, c'est de choisir le mod\u00e8le. Pour information, les crit\u00e8res pour le choix d'un mod\u00e8le ont \u00e9t\u00e9 : la pr\u00e9cision et la performance . Il faut que le mod\u00e8le soit tr\u00e8s pr\u00e9cis , c'est-\u00e0-dire qu'il ne d\u00e9tecte pas des visages quand il n'y en a pas et vice-versa . De plus, il faut que la d\u00e9tection du visage soit rapide . Cette rapidit\u00e9 d\u00e9pend notamment du mat\u00e9riel informatique qui est utilis\u00e9 pour faire tourner l'algorithme. Dans les cas d'algorithme sur des images, donc du graphisme, c'est la carte graphique qui sera la plus puissante : le GPU . Cependant, ce syst\u00e8me doit \u00eatre activ\u00e9 pour certains packages comme OpenCV . Carte graphique et compatibilit\u00e9 Toutes les cartes graphiques ne sont pas compatibles pour pouvoir r\u00e9aliser ce genre de t\u00e2che. Il est donc pr\u00e9f\u00e9rable que la d\u00e9tection du visage se fasse sur le processeur : le CPU . OpenCV et Dlib Petit rappel pour la suite, OpenCV est une librairie python. Elle est tr\u00e8s utilis\u00e9e dans la manipulation d'image . La librairie Dlib est similaire, mais est plus tourn\u00e9e vers le machine learning.","title":"Contexte"},{"location":"generation_nft/fonctionnalites/face_detect/#les-differents-modeles","text":"OpenCV et Haar cascades Deep learning de d\u00e9tection de visager int\u00e9gr\u00e9 \u00e0 la librairie OpenCV Dlib HOG + Linear SVM implementation Dlib CNN d\u00e9tection de visage","title":"Les diff\u00e9rents mod\u00e8les"},{"location":"generation_nft/fonctionnalites/face_detect/#haar-cascades","text":"L' haar cascades est un algorithme permettant de trouver plusieurs caract\u00e9ristiques pour une image. Il ne d\u00e9tecte pas uniquement les visages. Il se base notamment sur le mod\u00e8le de machine learning Adaboost . Le principe est plut\u00f4t simple, il a \u00e9t\u00e9 entrain\u00e9 avec plusieurs cat\u00e9gories d'image, avec pour chaque image , une description . Il aura ensuite la facult\u00e9 de pr\u00e9dire des objets dans l'image gr\u00e2ce aux caract\u00e9ristiques similaires aux images d'entra\u00eenements . Pour : Tr\u00e8s rapide . Pas besoin de le faire tourner sur un GPU . Contre : Haut pourcentage de faux n\u00e9gatif , en d'autres termes, une pr\u00e9cision m\u00e9diocre . Dois \u00eatre configur\u00e9 pour qu'il fonctionne correctement (correctement signifie m\u00e9diocre).","title":"Haar cascades"},{"location":"generation_nft/fonctionnalites/face_detect/#opencv-face-detector-integre","text":"La d\u00e9tection du visage int\u00e9gr\u00e9 dans la librairie d' OpenCV utilise le syst\u00e8me de SSD ( Single Shot Detector ). Comme son nom l'indique, en un seul essai, il peut d\u00e9tecter plusieurs objets . Il ne d\u00e9tecte pas seulement les visages, mais peut d\u00e9tecter une multitude d'objets sur la m\u00eame image en une seule fois. Il est plus compliqu\u00e9 d'expliquer le fonctionnement de cet algorithme, si vous voulez avoir plus d'information rendez-vous sur ce tutoriel . Pour : Efficace . Utilise un mod\u00e8le de deep learning r\u00e9cent . Pas de param\u00e9trage . Pas besoin de faire tourner l'algorithme sur un GPU . Mais sera tout de m\u00eame plus rapide dessus. Contre : Plus efficace que le mod\u00e8le Haar cascades , mais reste encore assez peu pr\u00e9cis . Possibilit\u00e9 de biais et d' erreurs \u00e0 cause des couleurs des images.","title":"OpenCV face detector int\u00e9gr\u00e9"},{"location":"generation_nft/fonctionnalites/face_detect/#dlib-hog-linear-svm","text":"HOG signifie Histograms of Oriented Gradients et linear SVM est le mod\u00e8le de machine learning utilis\u00e9 pour cette m\u00e9thode de d\u00e9tection de visage. Le concept HOG est plus compliqu\u00e9 \u00e0 comprendre que le linear SVM. Pour ces deux concepts, je vous conseille d'aller voir ce tutoriel . Pour : Plus pr\u00e9cis que le mod\u00e8le Haar cascades . Beaucoup plus stable que les deux mod\u00e8les d'avant. Facile d'utilisation et continue d'\u00eatre mise \u00e0 jour par son cr\u00e9ateur gr\u00e2ce \u00e0 la librairie Dlib . Extr\u00eamement bien document\u00e9 . Contre : Fonctionne uniquement sur les visages frontales ( nous arrange grandement ). Pas aussi pr\u00e9cis que d'autres mod\u00e8les bas\u00e9s sur le deep learning ( reste assez n\u00e9gligeable, se focus sur les visages de face ). Assez peu co\u00fbteux en termes de performance, mais reste un peu plus long que les deux mod\u00e8les pr\u00e9c\u00e9dant. De l'ordre de la milliseconde de plus (0.001 seconde).","title":"Dlib HOG + Linear SVM"},{"location":"generation_nft/fonctionnalites/face_detect/#dlib-cnn-face-detector","text":"CNN signifie Convolutional Neural Network , comme son nom l'indique, il utilise un syst\u00e8me de r\u00e9seau neuronal pour d\u00e9tecter le visage. Dans le monde de l'intelligence artificielle, voici l'ordre des performances : machine learning < deep learning < r\u00e9seau neuronal . La d\u00e9tection sera tr\u00e8s pr\u00e9cise. Cependant, les r\u00e9seaux neuronaux sont tr\u00e8s gourmands . Pour : Tr\u00e8s pr\u00e9cis pour d\u00e9tecter les visages. Taille du mod\u00e8le (fichier) inf\u00e9rieur \u00e0 1MB . Facile \u00e0 impl\u00e9menter et tr\u00e8s document\u00e9 . Contre : Le code est plus verbeux et n\u00e9cessite de multiples de conversions de variables. Tr\u00e8s lent sur CPU.","title":"Dlib CNN face detector"},{"location":"generation_nft/fonctionnalites/face_detect/#choix","text":"Il a \u00e9t\u00e9 d\u00e9cid\u00e9 de prendre le mod\u00e8le HOG + linear SVM . Plusieurs raisons \u00e0 ce choix : Il ne d\u00e9tecte pas les visages de profil . Ce qui nous facilite le machine learning : tilt learning ( voir la documentation ). Il est rapide sur CPU . Il est plus pr\u00e9cis que les deux autres mod\u00e8les fonctionnant sur CPU. Facile d'impl\u00e9mentation avec beaucoup de documentation . Enfin, il est encore maintenu .","title":"Choix"},{"location":"generation_nft/fonctionnalites/face_detect/#utilisation-du-modele","text":"Comme indiqu\u00e9 dans les diff\u00e9rentes raisons du choix du mod\u00e8le, il est tr\u00e8s facile d'impl\u00e9menter le mod\u00e8le HOG + linear SVM . Cela signifie entre autres, qu'en tr\u00e8s peu de ligne de code , le mod\u00e8le va d\u00e9tecter un visage sur une image et retourner les coordonn\u00e9es de celui-ci afin de pouvoir l'isoler du reste de l'image. Il faut tout d'abord avoir le fichier du mod\u00e8le. Vous le trouverez dans le chemin \"app/generation_nft/librairies/face_detect/pre_trained\" . T\u00e9l\u00e9chargement du fichier Le fichier n'est pas dans le r\u00e9pertoire GIT . Une v\u00e9rification au premier lancement est r\u00e9alis\u00e9e pour d\u00e9tecter la pr\u00e9sence du mod\u00e8le. Si ce n'est pas le cas, le script le t\u00e9l\u00e9charge . Ce mod\u00e8le est h\u00e9berg\u00e9 sur notre Google Drive . Puis l'importer dans le code : detector = open_cv . dnn . readNetFromCaffe ( caffe_prototxt_path , caffe_model_path ) Pour pouvoir utiliser ce mod\u00e8le, il faut imp\u00e9rativement convertir l'image en blob , pour ce faire, la librairie OpenCV comporte une fonction pour le faire simplement. image = open_cv . imread ( image_path ) # R\u00e9cup\u00e8re l'image est le converti en matrice numpy ( height_image , width_image ) = image . shape [: 2 ] blob_image = open_cv . dnn . blobFromImage ( open_cv . resize ( image , ( width_image , height_image )), 1.0 , ( width_image , height_image ), ( 104.0 , 177.0 , 123.0 ), ) 1.0 correspond \u00e0 un changement d'\u00e9chelle . Ici, aucune modification. (104.0, 177.0, 123.0) correspond \u00e0 une colorisation de l'image . Il ne reste plus qu'\u00e0 int\u00e9grer le blob dans le mod\u00e8le de d\u00e9tection et de r\u00e9cup\u00e9rer les coordonn\u00e9es du/des visages d\u00e9tect\u00e9s . detector . setInput ( blob_image ) face_detections = detector . forward () Par la suite, avec chaque coordonn\u00e9e, on pourra isoler une partie de l'image . Bien s\u00fbr, il faudra prendre une marge car la d\u00e9tection du visage se focalise sur les \u00e9l\u00e9ments du visage et non la t\u00eate enti\u00e8re . Calcul de la marge Pour \u00eatre s\u00fbr d'avoir la totalit\u00e9 de la t\u00eate , la marge est calcul\u00e9e en fonction de la taille du visage d\u00e9tect\u00e9 .","title":"Utilisation du mod\u00e8le"},{"location":"generation_nft/fonctionnalites/face_landmarks/","text":"Masques du visage D\u00e9finition Le face landmarks , en fran\u00e7ais, points de rep\u00e8re du visage, est un algorithme permettant de r\u00e9cup\u00e9rer plusieurs points (468) d'un visage . Cet algorithme intervient juste apr\u00e8s la d\u00e9tection du visage et une nouvelle fois apr\u00e8s la rotation du visage. Contexte Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV . Outre le fait de d\u00e9tecter un visage, il fallait obligatoirement r\u00e9cup\u00e9rer les \u00e9l\u00e9ments du visage afin de pouvoir r\u00e9aliser un dessin fid\u00e8le . Pour ce faire, il existe plusieurs librairies plus ou moins pr\u00e9cises. Cependant, contrairement \u00e0 la d\u00e9tection du visage, le but est uniquement de reprendre le plus de points possibles pour reproduire fid\u00e8lement le visage. La librairie MediaPipe est l'une des meilleures dans ce domaine, elle permet d'avoir 468 points et est extr\u00eamement simple et pr\u00e9cise . D\u00e9tecter des landmarks Le code pour pouvoir r\u00e9cup\u00e9rer les points tient en 4 lignes . Il faut tout simplement importer la fonction de MediaPipe appel\u00e9e face_mesh . Elle prend en param\u00e8tre une image. Il est pr\u00e9f\u00e9rable de coloriser l'image en nuances de gris afin que la fonction soit la plus performantes possible. import cv2 as open_cv from mediapipe.python.solutions import face_mesh as mediapipe_fm , results = face_mesh . process ( open_cv . cvtColor ( image , open_cv . COLOR_BGR2RGB ) ) face_landmarks = results . multi_face_landmarks [ 0 ] . landmark # LISTE DES POINTS face_landmarks va comporter une liste de plusieurs points tri\u00e9s dans un ordre tr\u00e8s pr\u00e9cis . [ { \"x\" : \"0.14771\" , \"y\" : \"0.48782\" , \"z\" : \"0.1563\" } ] L'ordre correspond aux num\u00e9ros de chaque point . Pour voir en quelle position se trouve la pointe du nez ou d'autres parties, il suffit de regarder cette image. Voici le code pour r\u00e9cup\u00e9rer un point pr\u00e9cis . Je souhaite r\u00e9cup\u00e9rer dans cette documentation le point gauche de la bouche , qui est le num\u00e9ro 61 . Attention, c'est tr\u00e8s compliqu\u00e9 left_mouth = face_landmarks [ 61 ] Pour finir, les valeurs renvoy\u00e9es sont normalis\u00e9es , elles sont toutes mises sur la m\u00eame \u00e9chelle. En ce qui concerne le X et le Y , il suffit de multiplier la valeur par la longueur et par la largeur de l'image pour retrouver les valeurs en pixels. Pour la valeur Z , la profondeur, elle d\u00e9pend d'une feuille \u00e0 plat plac\u00e9e en parall\u00e8le au visage, qui fera office du 0. Tout ce qui sera avant sera positif , tout ce qui sera apr\u00e8s sera n\u00e9gatif .","title":"Masques du visage"},{"location":"generation_nft/fonctionnalites/face_landmarks/#masques-du-visage","text":"","title":"Masques du visage"},{"location":"generation_nft/fonctionnalites/face_landmarks/#definition","text":"Le face landmarks , en fran\u00e7ais, points de rep\u00e8re du visage, est un algorithme permettant de r\u00e9cup\u00e9rer plusieurs points (468) d'un visage . Cet algorithme intervient juste apr\u00e8s la d\u00e9tection du visage et une nouvelle fois apr\u00e8s la rotation du visage.","title":"D\u00e9finition"},{"location":"generation_nft/fonctionnalites/face_landmarks/#contexte","text":"Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV . Outre le fait de d\u00e9tecter un visage, il fallait obligatoirement r\u00e9cup\u00e9rer les \u00e9l\u00e9ments du visage afin de pouvoir r\u00e9aliser un dessin fid\u00e8le . Pour ce faire, il existe plusieurs librairies plus ou moins pr\u00e9cises. Cependant, contrairement \u00e0 la d\u00e9tection du visage, le but est uniquement de reprendre le plus de points possibles pour reproduire fid\u00e8lement le visage. La librairie MediaPipe est l'une des meilleures dans ce domaine, elle permet d'avoir 468 points et est extr\u00eamement simple et pr\u00e9cise .","title":"Contexte"},{"location":"generation_nft/fonctionnalites/face_landmarks/#detecter-des-landmarks","text":"Le code pour pouvoir r\u00e9cup\u00e9rer les points tient en 4 lignes . Il faut tout simplement importer la fonction de MediaPipe appel\u00e9e face_mesh . Elle prend en param\u00e8tre une image. Il est pr\u00e9f\u00e9rable de coloriser l'image en nuances de gris afin que la fonction soit la plus performantes possible. import cv2 as open_cv from mediapipe.python.solutions import face_mesh as mediapipe_fm , results = face_mesh . process ( open_cv . cvtColor ( image , open_cv . COLOR_BGR2RGB ) ) face_landmarks = results . multi_face_landmarks [ 0 ] . landmark # LISTE DES POINTS face_landmarks va comporter une liste de plusieurs points tri\u00e9s dans un ordre tr\u00e8s pr\u00e9cis . [ { \"x\" : \"0.14771\" , \"y\" : \"0.48782\" , \"z\" : \"0.1563\" } ] L'ordre correspond aux num\u00e9ros de chaque point . Pour voir en quelle position se trouve la pointe du nez ou d'autres parties, il suffit de regarder cette image. Voici le code pour r\u00e9cup\u00e9rer un point pr\u00e9cis . Je souhaite r\u00e9cup\u00e9rer dans cette documentation le point gauche de la bouche , qui est le num\u00e9ro 61 . Attention, c'est tr\u00e8s compliqu\u00e9 left_mouth = face_landmarks [ 61 ] Pour finir, les valeurs renvoy\u00e9es sont normalis\u00e9es , elles sont toutes mises sur la m\u00eame \u00e9chelle. En ce qui concerne le X et le Y , il suffit de multiplier la valeur par la longueur et par la largeur de l'image pour retrouver les valeurs en pixels. Pour la valeur Z , la profondeur, elle d\u00e9pend d'une feuille \u00e0 plat plac\u00e9e en parall\u00e8le au visage, qui fera office du 0. Tout ce qui sera avant sera positif , tout ce qui sera apr\u00e8s sera n\u00e9gatif .","title":"D\u00e9tecter des landmarks"},{"location":"generation_nft/fonctionnalites/face_parsing/","text":"R\u00e9cup\u00e9ration des parties D\u00e9finition La r\u00e9cup\u00e9ration des parties d'un visage intervient en plus du module face landmarks . Pour rappel, face landmarks permet d'avoir 468 points tr\u00e8s pr\u00e9cis pour des \u00e9l\u00e9ments du visage. Cependant, les oreilles , la forme de la t\u00eate , les cheveux et la barbe ne sont pas identifi\u00e9s. La m\u00e9thode pour identifier des \u00e9l\u00e9ments sur une image s'appelle la segmentation . Cette m\u00e9thode est notamment utilis\u00e9e dans les algorithmes pour la conduite autonome de Tesla (en bien plus \u00e9volu\u00e9e). Image originale Masque de l'image Contours Pour le bien de la documentation, les zones ont \u00e9t\u00e9 color\u00e9es. Dans l'algorithme r\u00e9el, nous reprenons uniquement les contours liss\u00e9s pour am\u00e9liorer le rendu. Le mod\u00e8le utilis\u00e9 ne d\u00e9tecte pas encore la barbe ! Contexte Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de r\u00e9cup\u00e9rer les parties manquantes du visage que n' identifiait pas le module face landmarks . De nombreux mod\u00e8les existent plus ou moins complexes et pr\u00e9cis existent pour effectuer la \"segmentation\" . Ce qui a \u00e9t\u00e9 le plus compliqu\u00e9, c'est de choisir le mod\u00e8le . Heureusement, il existe un r\u00e9pertoire GIT nomm\u00e9 face_parsing qui nous permet de r\u00e9aliser ce que l'on souhaite de fa\u00e7on performante pour une premi\u00e8re version . Comme vous avez pu le voir, le mod\u00e8le choisi dans un premier temps ne r\u00e9cup\u00e8re pas la barbe . C'est un mod\u00e8le pr\u00e9-entrain\u00e9 par un organisme externe . Le plus optimal serait de r\u00e9entrainer un mod\u00e8le similaire avec l'ajout de la barbe. Cependant, l'entra\u00eenement d'un mod\u00e8le est tr\u00e8s long . Il doit \u00eatre entrain\u00e9 sur des millions de donn\u00e9es et doit \u00eatre param\u00e9tr\u00e9 de fa\u00e7on optimale pour un rendu optimale . Utilisation du mod\u00e8le L'utilisation du mod\u00e8le peut \u00eatre d\u00e9coup\u00e9e en 3 \u00e9tapes . R\u00e9cup\u00e9ration du mod\u00e8le pr\u00e9-entrain\u00e9 Dans un premier temps, il faut t\u00e9l\u00e9charger ce mod\u00e8le pr\u00e9-entrain\u00e9. Il se pr\u00e9sente sous la forme d'un fichier .pth . Il est h\u00e9berg\u00e9 sur le Google Drive de l'\u00e9quipe et est automatiquement t\u00e9l\u00e9charg\u00e9 lorsque la solution est lanc\u00e9e s'il n'est pas encore pr\u00e9sent. Situ\u00e9 dans le dossier app/generation_nft/libraries/face_parsin/pre-trained/face_parts.pth , il est appel\u00e9 dans la fonction load_state_dict du package pytorch . model = BiSeNet ( resnet = \"https://drive.google.com/u/1/uc?id=1e8HiZK_0C_56wYXc9nbDl9CyFVMfp151&export=download\" , # mod\u00e8le pour \u00e9quilibrer le BiSeNet n_classes = self . NUM_CLASSES , # Nombre de parties \u00e0 d\u00e9tecter (par d\u00e9faut 19) ) model . to ( device ) model . load_state_dict ( torch . load ( f \" { pretrained_model_path . parent } / { pretrained_model_path . name } \" , map_location = device , ) ) model . eval () Le mod\u00e8le pr\u00e9-entrain\u00e9 a utilis\u00e9 BiSeNet comme mod\u00e8le de segmentation . Il est n\u00e9cessaire de reprendre exactement les m\u00eames param\u00e8tres . L'ex\u00e9cution du mod\u00e8le s'effectue sur le CPU et se lance en mode \u00e9valuation. Traitement de la donn\u00e9e La donn\u00e9e , ou ici, le visage , dois avoir un format pr\u00e9cis afin qu'il puisse \u00eatre trait\u00e9 par le mod\u00e8le . Pytorch , le package de deep learning, utilise le type Tensor , qui s'apparente grandement \u00e0 un tableau de matrice comme pour le package numpy et qui repr\u00e9sente l'image. En entr\u00e9e , notre image est sous le format du tableau numpy , il faut le convertir en Tensor puis supprimer la premi\u00e8re dimension (colonne) qui correspond au nombre de classes . infer_transforms = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.485 , 0.456 , 0.406 ), ( 0.229 , 0.224 , 0.225 )), ] ) initialH , initialW , _ = face . shape if initialH != input_size or initialW != input_size : face = open_cv . resize ( face , ( input_size , input_size ), interpolation = open_cv . INTER_NEAREST ) # redimensionne le visage face_tensor = infer_transforms ( face ) . unsqueeze ( 0 ) . to ( device ) Les op\u00e9rations sur le type Tensor, sont effectu\u00e9es sur le CPU . Pr\u00e9diction La pr\u00e9diction est simplement l'\u00e9tape o\u00f9 l' image trait\u00e9e est fournie au mod\u00e8le. Ce mod\u00e8le retournera une pr\u00e9diction , toujours dans le format Tensor . prediction = model ( face_tensor )[ 0 ] prediction = F . interpolate ( prediction , size = ( initialW , initialH ), mode = \"bilinear\" , align_corners = True , ) prediction = prediction . squeeze ( 0 ) . cpu () . numpy () . argmax ( 0 ) La pr\u00e9diction se pr\u00e9sente sous cette forme : Nettoyage de la pr\u00e9diction Comme vous l'aurez remarqu\u00e9, la pr\u00e9diction concerne 19 zones . Seulement 4 nous int\u00e9ressent : les oreilles (2), la forme de la t\u00eate et les cheveux . Il faut coloriser de la bonne couleur les parties \u00e0 l' int\u00e9rieur de la zone bleue et supprimer (mettre en blanc) les zones non souhait\u00e9es ( cou et v\u00eatement ). Une fois que seules les zones souhait\u00e9es sont r\u00e9cup\u00e9r\u00e9es, nous r\u00e9cup\u00e9rons uniquement leur contour . cleaned_prediction = self . clean_mask ( prediction ) visual_mask_color = self . colorize_parts ( cleaned_prediction ) visual_parts = self . get_parts ( visual_mask_color ) Code trop technique. R\u00e9sultat Pour finir, la fonction get_parts au-dessus, stocke dans une variable chaque contour avec leur correspondance .","title":"R\u00e9cup\u00e9ration des parties du visage"},{"location":"generation_nft/fonctionnalites/face_parsing/#recuperation-des-parties","text":"","title":"R\u00e9cup\u00e9ration des parties"},{"location":"generation_nft/fonctionnalites/face_parsing/#definition","text":"La r\u00e9cup\u00e9ration des parties d'un visage intervient en plus du module face landmarks . Pour rappel, face landmarks permet d'avoir 468 points tr\u00e8s pr\u00e9cis pour des \u00e9l\u00e9ments du visage. Cependant, les oreilles , la forme de la t\u00eate , les cheveux et la barbe ne sont pas identifi\u00e9s. La m\u00e9thode pour identifier des \u00e9l\u00e9ments sur une image s'appelle la segmentation . Cette m\u00e9thode est notamment utilis\u00e9e dans les algorithmes pour la conduite autonome de Tesla (en bien plus \u00e9volu\u00e9e).","title":"D\u00e9finition"},{"location":"generation_nft/fonctionnalites/face_parsing/#contexte","text":"Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de r\u00e9cup\u00e9rer les parties manquantes du visage que n' identifiait pas le module face landmarks . De nombreux mod\u00e8les existent plus ou moins complexes et pr\u00e9cis existent pour effectuer la \"segmentation\" . Ce qui a \u00e9t\u00e9 le plus compliqu\u00e9, c'est de choisir le mod\u00e8le . Heureusement, il existe un r\u00e9pertoire GIT nomm\u00e9 face_parsing qui nous permet de r\u00e9aliser ce que l'on souhaite de fa\u00e7on performante pour une premi\u00e8re version . Comme vous avez pu le voir, le mod\u00e8le choisi dans un premier temps ne r\u00e9cup\u00e8re pas la barbe . C'est un mod\u00e8le pr\u00e9-entrain\u00e9 par un organisme externe . Le plus optimal serait de r\u00e9entrainer un mod\u00e8le similaire avec l'ajout de la barbe. Cependant, l'entra\u00eenement d'un mod\u00e8le est tr\u00e8s long . Il doit \u00eatre entrain\u00e9 sur des millions de donn\u00e9es et doit \u00eatre param\u00e9tr\u00e9 de fa\u00e7on optimale pour un rendu optimale .","title":"Contexte"},{"location":"generation_nft/fonctionnalites/face_parsing/#utilisation-du-modele","text":"L'utilisation du mod\u00e8le peut \u00eatre d\u00e9coup\u00e9e en 3 \u00e9tapes .","title":"Utilisation du mod\u00e8le"},{"location":"generation_nft/fonctionnalites/face_parsing/#recuperation-du-modele-pre-entraine","text":"Dans un premier temps, il faut t\u00e9l\u00e9charger ce mod\u00e8le pr\u00e9-entrain\u00e9. Il se pr\u00e9sente sous la forme d'un fichier .pth . Il est h\u00e9berg\u00e9 sur le Google Drive de l'\u00e9quipe et est automatiquement t\u00e9l\u00e9charg\u00e9 lorsque la solution est lanc\u00e9e s'il n'est pas encore pr\u00e9sent. Situ\u00e9 dans le dossier app/generation_nft/libraries/face_parsin/pre-trained/face_parts.pth , il est appel\u00e9 dans la fonction load_state_dict du package pytorch . model = BiSeNet ( resnet = \"https://drive.google.com/u/1/uc?id=1e8HiZK_0C_56wYXc9nbDl9CyFVMfp151&export=download\" , # mod\u00e8le pour \u00e9quilibrer le BiSeNet n_classes = self . NUM_CLASSES , # Nombre de parties \u00e0 d\u00e9tecter (par d\u00e9faut 19) ) model . to ( device ) model . load_state_dict ( torch . load ( f \" { pretrained_model_path . parent } / { pretrained_model_path . name } \" , map_location = device , ) ) model . eval () Le mod\u00e8le pr\u00e9-entrain\u00e9 a utilis\u00e9 BiSeNet comme mod\u00e8le de segmentation . Il est n\u00e9cessaire de reprendre exactement les m\u00eames param\u00e8tres . L'ex\u00e9cution du mod\u00e8le s'effectue sur le CPU et se lance en mode \u00e9valuation.","title":"R\u00e9cup\u00e9ration du mod\u00e8le pr\u00e9-entrain\u00e9"},{"location":"generation_nft/fonctionnalites/face_parsing/#traitement-de-la-donnee","text":"La donn\u00e9e , ou ici, le visage , dois avoir un format pr\u00e9cis afin qu'il puisse \u00eatre trait\u00e9 par le mod\u00e8le . Pytorch , le package de deep learning, utilise le type Tensor , qui s'apparente grandement \u00e0 un tableau de matrice comme pour le package numpy et qui repr\u00e9sente l'image. En entr\u00e9e , notre image est sous le format du tableau numpy , il faut le convertir en Tensor puis supprimer la premi\u00e8re dimension (colonne) qui correspond au nombre de classes . infer_transforms = transforms . Compose ( [ transforms . ToTensor (), transforms . Normalize (( 0.485 , 0.456 , 0.406 ), ( 0.229 , 0.224 , 0.225 )), ] ) initialH , initialW , _ = face . shape if initialH != input_size or initialW != input_size : face = open_cv . resize ( face , ( input_size , input_size ), interpolation = open_cv . INTER_NEAREST ) # redimensionne le visage face_tensor = infer_transforms ( face ) . unsqueeze ( 0 ) . to ( device ) Les op\u00e9rations sur le type Tensor, sont effectu\u00e9es sur le CPU .","title":"Traitement de la donn\u00e9e"},{"location":"generation_nft/fonctionnalites/face_parsing/#prediction","text":"La pr\u00e9diction est simplement l'\u00e9tape o\u00f9 l' image trait\u00e9e est fournie au mod\u00e8le. Ce mod\u00e8le retournera une pr\u00e9diction , toujours dans le format Tensor . prediction = model ( face_tensor )[ 0 ] prediction = F . interpolate ( prediction , size = ( initialW , initialH ), mode = \"bilinear\" , align_corners = True , ) prediction = prediction . squeeze ( 0 ) . cpu () . numpy () . argmax ( 0 ) La pr\u00e9diction se pr\u00e9sente sous cette forme :","title":"Pr\u00e9diction"},{"location":"generation_nft/fonctionnalites/face_parsing/#nettoyage-de-la-prediction","text":"Comme vous l'aurez remarqu\u00e9, la pr\u00e9diction concerne 19 zones . Seulement 4 nous int\u00e9ressent : les oreilles (2), la forme de la t\u00eate et les cheveux . Il faut coloriser de la bonne couleur les parties \u00e0 l' int\u00e9rieur de la zone bleue et supprimer (mettre en blanc) les zones non souhait\u00e9es ( cou et v\u00eatement ). Une fois que seules les zones souhait\u00e9es sont r\u00e9cup\u00e9r\u00e9es, nous r\u00e9cup\u00e9rons uniquement leur contour . cleaned_prediction = self . clean_mask ( prediction ) visual_mask_color = self . colorize_parts ( cleaned_prediction ) visual_parts = self . get_parts ( visual_mask_color ) Code trop technique.","title":"Nettoyage de la pr\u00e9diction"},{"location":"generation_nft/fonctionnalites/face_parsing/#resultat","text":"Pour finir, la fonction get_parts au-dessus, stocke dans une variable chaque contour avec leur correspondance .","title":"R\u00e9sultat"},{"location":"generation_nft/fonctionnalites/tilt_learning/","text":"Direction du regard D\u00e9finition Le tilt learning est un algorithme de machine learning . Il a \u00e9t\u00e9 cr\u00e9\u00e9 pour savoir si le visage d\u00e9tect\u00e9 est orient\u00e9 de la bonne fa\u00e7on. Un visage correctement orient\u00e9 est un visage qui regarde droit devant . Mauvais MBAPPE Bon MBAPPE Contexte Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV et MediaPipe , il a \u00e9t\u00e9 tr\u00e8s facile de r\u00e9cup\u00e9rer plusieurs points importants du visage afin de tracer les contours . Cependant, la question de la direction du regard s'est vite pos\u00e9e. Il n'y a aucun package externe pour r\u00e9aliser cette v\u00e9rification de fa\u00e7on optimale . Il existe des solutions similaires, mais uniquement pour des vid\u00e9os . L'une des solutions les moins co\u00fbteuses en termes de temps que ce soit au niveau du temps de veille , du temps de mont\u00e9 en comp\u00e9tence et du temps de mise en place est le machine learning . Il existe d'autres notions efficaces : le deep learning et les r\u00e9seaux neuronaux . Ces deux notions sont cependant bien plus complexes et ont une pr\u00e9cision similaire compar\u00e9e \u00e0 un bon machine learning bien entrain\u00e9. Ce qu'il faut savoir sur le machine learning Par d\u00e9finition, le machine learning est une technologie d\u2019intelligence artificielle permettant aux ordinateurs d\u2019apprendre sans avoir \u00e9t\u00e9 programm\u00e9s explicitement \u00e0 cet effet. Pour apprendre et se d\u00e9velopper, les ordinateurs ont toutefois besoin de donn\u00e9es \u00e0 analyser et sur lesquelles s\u2019entra\u00eener. La plupart du temps, le machine learning est pour r\u00e9aliser une pr\u00e9diction . En r\u00e9alit\u00e9, ces technologies d'intelligence artificielle sont des algorithmes (mod\u00e8les) qui interpr\u00e8tent plusieurs donn\u00e9es et les regroupent sous forme logique . Il est possible de repr\u00e9senter ce que fait le mod\u00e8le de machine learning sous forme d'un graphique . Prenons l'exemple avec le mod\u00e8le de type clustering : On peut voir plusieurs types d'algorithmes clustering (colonne). Ce sont des algorithmes qui vont regrouper de mani\u00e8re logique diff\u00e9rentes donn\u00e9es afin d'en faire une cat\u00e9gorie . Ces donn\u00e9es sont consid\u00e9r\u00e9es comme des donn\u00e9es d'entrainement . Chacune de ces donn\u00e9es peut \u00eatre identifi\u00e9e afin de fournir une base \u00e0 l'algorithme. Dans notre exemple, il n'est pas obligatoire de lui fournir une cat\u00e9gorisation. Le clustering peut regroup\u00e9 de mani\u00e8re math\u00e9matique les donn\u00e9es. Lorsque l'on testera une nouvelle donn\u00e9e pour faire une pr\u00e9diction, l'algorithme analysera les donn\u00e9es de la m\u00eame mani\u00e8re que les donn\u00e9es qui ont servi pour tester le mod\u00e8le . Il pourra par la suite dire que cette donn\u00e9e correspond aux donn\u00e9es de la cat\u00e9gorie x. Type de machine learning Il existe beaucoup de types de machine learning qui eu m\u00eame ont plusieurs sous types . Dans notre solution, nous utilisons le package scikit-learn qui permet de facilit\u00e9 l'utilisation de ces mod\u00e8les. Scikit-learn fourni une tr\u00e8s bonne documentation pour conna\u00eetre tous les param\u00e8tres de chaque mod\u00e8le et leur fonctionnement. Chaque mod\u00e8le est optimis\u00e9 pour des types de donn\u00e9es , quantit\u00e9 de donn\u00e9es et l'objectif de la pr\u00e9diction. Voici un sch\u00e9ma pour savoir quel mod\u00e8le sera le plus adapt\u00e9 \u00e0 ce que l'on souhaite faire : Nos mod\u00e8les Dans notre solution, si on suit le sch\u00e9ma au-dessus, nous avons plus de 50 donn\u00e9es de test (diff\u00e9rentes images de plusieurs visages regardant dans des directions diff\u00e9rentes). Nous souhaitons pr\u00e9dire une cat\u00e9gorie . Nos cat\u00e9gories ici seront les diff\u00e9rentes directions : haut, bas, gauche, droit, en face . Le fait de vouloir pr\u00e9dire une cat\u00e9gorie signifie que les mod\u00e8les de type regression et dimensionality reduction ne seront pas tr\u00e8s utiles. Par la suite, le sch\u00e9ma nous demande s'il y a la pr\u00e9sence de texte : oui (haut, bas ...) . Les mod\u00e8les clustering ne sont donc pas conseill\u00e9s. Ensuite, nous rentrons enfin dans un type de machine learning : classification . La premi\u00e8re information importante est de conna\u00eetre le nombre de donn\u00e9es de test . Malheureusement, r\u00e9unir plus de 100 000 donn\u00e9es quand il s'agit d'image est une mission quasiment impossible, sachant qu'il faut toutes les identifier en termes de direction. ???+ info \"Augmentation d'image\" Cependant, il est possible de faire de **l'augmentation d'image**, c'est-\u00e0-dire **multiplier** le nombre d'images avec **celle que l'on a d\u00e9j\u00e0**. Par exemple, retourner l'image horizontalement. Ce qui multipliera par deux le nombre d'images. <div style=\"display:flex;flex-direction:row;justify-content:center;margin-bottom:10px;\"> <img src=\"../../../pictures/generation_nft/tilt_learning/kilian-left.jpg\" style=\"width:321px;height:191px;margin-right:10px;\"> <img src=\"../../../pictures/generation_nft/tilt_learning/kilian-right.jpg\" style=\"width:321px;height:191px;margin-left:10px;\"> </div> Nous croisons un premier mod\u00e8le : LinearSVC . Nous pourrions nous arr\u00eater ici et tester ce mod\u00e8le avec nos donn\u00e9es. Mais pour que notre pr\u00e9diction soit la plus pr\u00e9cise possible, nous pouvons s\u00e9lectionner plusieurs mod\u00e8les . Nous allons donc s\u00e9lectionner les mod\u00e8les suivant: Naive Bayes, KNeighbors classifier, SVC et Ensemble classifiers . \u00c0 savoir qu'il existe plusieurs types de mod\u00e8le pour le Naive Bayes, pour le KNeighbors classifier et pour les Ensemble classifiers. Le plus efficace pour pr\u00e9dire correctement est d'avoir un nombre impair de mod\u00e8les pour r\u00e9cup\u00e9rer \u00e0 chaque fois la pr\u00e9diction la plus pr\u00e9sente . Si jamais le nombre est pair est qu'une \u00e9galit\u00e9 survient, il sera difficile d'identifi\u00e9 la pr\u00e9diction a privil\u00e9gier. Liste des packages \u00e0 installer numpy imutils dlib mediapipe pandas scikit-learn opencv-python scikit-image joblib Premi\u00e8re \u00e9tape - formatage et preprocessing des donn\u00e9es Pour pouvoir r\u00e9aliser un machine learning efficace, outre le nombre d'image, il faut des donn\u00e9es coh\u00e9rentes et nettoyer . A ce stade, nous avons d\u00e9j\u00e0 r\u00e9cup\u00e9r\u00e9 les landmarks (zone du visage). Voir la documentation sur la d\u00e9tection de la zone du visage pour comprendre quelles sont les donn\u00e9es r\u00e9cup\u00e9r\u00e9es. Nous avons donc 486 points du visage avec un x correspondant \u00e0 la position en largeur , un y correspondant \u00e0 la position en hauteur et enfin un z correspondant \u00e0 la profondeur. Dans la documentation, j'explique que le z est calcul\u00e9 par rapport \u00e0 une feuille \u00e0 plat . Tout ce qui est avant (plus pr\u00e8s de la cam\u00e9ra) sera positif , tout ce qui est apr\u00e8s (plus loin dans la photo) sera n\u00e9gatif . Cependant, ces valeurs sont toutes diff\u00e9rentes selon la photo, selon le visage et surtout selon l'\u00e9chelle qui n'est jamais la m\u00eame . Cette notion d'\u00e9chelle est tr\u00e8s importante pour le machine learning. Il sera beaucoup moins efficace si ces donn\u00e9es ne sont pas sur la m\u00eame plage de donn\u00e9es. \u00c0 ce stade nous avons donc 486 points * 3 (x, y et z). Si nous donnons les coordonn\u00e9es de tous ces points en donn\u00e9es, il y a de fortes chances qu'ils ne trouvent jamais si la photo est correcte . Les visages sont tous diff\u00e9rents , des personnes ont des mentons moins profonds que le front, d'autres ont un nez plus imposant, ... Il faut donc r\u00e9cup\u00e9rer uniquement les informations qui nous permettront d'avoir des informations performantes et pertinentes . Direction horizontale Il existe plusieurs fa\u00e7ons , avec les points que l'on a, pour d\u00e9terminer si le visage regarde \u00e0 gauche ou \u00e0 droite . Pour que le machine learning soit efficace , nous allons r\u00e9cup\u00e9rer deux donn\u00e9es qui \u00e0 elle seule pourraient d\u00e9terminer si le visage regarde dans une direction. Cependant, si l'on augmente le nombre de donn\u00e9es, le machine learning sera bien plus pr\u00e9cis . Avant de commencer, toutes ses donn\u00e9es partent du principe que chaque visage a une diff\u00e9rence de sym\u00e9trie n\u00e9gligeable . Nous avons tous plus ou moins un visage sym\u00e9trique. Et la cible que nous souhaitons \"cartoonis\u00e9\" sont en g\u00e9n\u00e9ral tr\u00e8s sportif et sans handicap majeur . Rib\u00e9ry est notre ennemi Premi\u00e8re m\u00e9thode La profondeur de deux points peut d\u00e9terminer une direction. Dans notre cas, nous prenons le point du visage le plus \u00e0 gauche et le point du visage le plus \u00e0 droite . Et nous regardons la diff\u00e9rence en pourcentage entre les deux profondeurs. Si la diff\u00e9rence d\u00e9passe un certain seuil, seuil qui sera g\u00e9r\u00e9 par le machine learning , le visage regardera \u00e0 droite ou \u00e0 gauche. Pour mieux comprendre, nous allons, prendre l'axe Z que nous allons utilis\u00e9 comme X et l'axe des Y . Sur l'exemple, si on regarde le sch\u00e9ma, on voit que le point de gauche \u00e0 sa profondeur Z ( x dans le sch\u00e9ma ) plus proche de 0 , le 0 correspond ici \u00e0 l' objectif de la cam\u00e9ra (ou l'\u00e9cran), que le point de droite l\u00e9g\u00e8rement plus \u00e9loign\u00e9 de 0, plus profond . Si vous mettez vos doigts de chaque c\u00f4t\u00e9 de votre visage et que votre \u00e9cran d'ordinateur correspond au 0 , vous verrez que si vous tournez la t\u00eate \u00e0 gauche , votre doigt \u00e0 droite de votre visage se rapproche de l'\u00e9cran tandis que le point gauche s'en \u00e9loigne. Ces donn\u00e9es pourront donc \u00eatre ajout\u00e9es \u00e0 notre datasets (jeu de donn\u00e9es qui seront analys\u00e9es par le machine learning). Deuxi\u00e8me m\u00e9thode Cette m\u00e9thode se base essentiellement sur la sym\u00e9trie du visage . En toute logique, un visage qui regarde la cam\u00e9ra droit devant, aura des proportions identiques entre la partie gauche du visage et la partie droite du visage . Il suffit, pour v\u00e9rifier, de prendre une distance entre deux points sur la partie gauche, et prendre leur deux points sym\u00e9triques du c\u00f4t\u00e9 droit du visage et calculer la diff\u00e9rence entre les deux distances de chaque point. On remarque assez facilement que la distance \u00e0 gauche est plus grande que la distance \u00e0 droite. Ce ph\u00e9nom\u00e8ne s'explique par le fait que la partie d'un visage est compress\u00e9e par rapport \u00e0 l'observateur . Nous avons deux valeurs plut\u00f4t pr\u00e9cises qui assembl\u00e9es, augmenterons la performance du machine learning \u00e0 d\u00e9tecter si le visage regarde trop \u00e0 droite ou trop \u00e0 gauche . Direction verticale Cette partie a \u00e9t\u00e9 la plus compliqu\u00e9e \u00e0 r\u00e9aliser, mais elle s'av\u00e8re la plus efficace. Pour identifier la direction vers le haut ou vers le bas , il y a plusieurs facteurs qui compliquent la lecture des donn\u00e9es. Les visages sont tous diff\u00e9rents en ce qui concerne la profondeur de leur visage. Logiquement, on pourrait se dire : c'est plut\u00f4t simple, on prend la profondeur du point le plus haut et du plus bas du visage et on regarde la diff\u00e9rence. Voil\u00e0 pourquoi il n'est pas possible de faire cela : Ici, comme pour la premi\u00e8re m\u00e9thode horizontale , on met la profondeur sur X en gardant Y. Cela nous permet d'avoir le profil de la photo . On voit clairement que pour la premi\u00e8re, il n'y aurait pas de probl\u00e8me, cependant la seconde, on remarque que son menton est toujours plus avanc\u00e9 que son front . Si on devait ajouter ces donn\u00e9es dans un machine learning, je suis s\u00fbr que la pr\u00e9diction serait souvent fausse car ses donn\u00e9es ressembleraient trop aux donn\u00e9es d'une personne qui regarde l\u00e9g\u00e8rement vers le haut . Cela fonctionne aussi dans le sens inverse (le bas), m\u00eame si c'est plus rare. La premi\u00e8re m\u00e9thode consiste \u00e0 rajouter deux points de r\u00e9f\u00e9rences au niveau du nez et du c\u00f4t\u00e9 de l'oreille . Ces points de r\u00e9f\u00e9rences nous permettront de r\u00e9aliser plusieurs calculs pour en retirer des informations . Bien s\u00fbr la m\u00e9thode sera multipli\u00e9e par deux, c\u00f4t\u00e9 gauche et c\u00f4t\u00e9 droit . Premi\u00e8re m\u00e9thode Ce sont les lignes grises et roses (zoomer pour les voir) qui vont nous int\u00e9resser dans un premier temps. Nous allons reprendre la diff\u00e9rence entre les deux lignes grises en bas et en haut qui correspondent \u00e0 la distance entre le front et le nez et le menton et le nez . Puis reprendre la hauteur entre le nez et le point \u00e0 c\u00f4t\u00e9 de l'oreille (les lignes roses). Avec ces deux donn\u00e9es, le machine learning pourra d\u00e9j\u00e0 pr\u00e9dire correctement plus de la moiti\u00e9 des cas . Il faut savoir que cette m\u00e9thode r\u00e9duit l\u00e9g\u00e8rement le facteur de la morphologie du visage , mais elle ne pourra pas fonctionner seule, il faut obligatoirement la deuxi\u00e8me m\u00e9thode pour que le machine learning puisse identifier un pattern dans ces donn\u00e9es. Deuxi\u00e8me m\u00e9thode Pour que la premi\u00e8re m\u00e9thode fonctionne, il faut une valeur verticale de r\u00e9f\u00e9rence . Il y a beaucoup d'\u00e9tudes plus ou moins scientifique qui ont \u00e9t\u00e9 faites sur la morphologie et sur les diff\u00e9rentes r\u00e8gles de mensuration d'un visage. Celle qui nous int\u00e9resse ici est la mensuration entre le menton et le nez et le nez et le haut front . Pour mieux comprendre, voici une image : La plupart du temps, cette proportion est respect\u00e9e, il y a bien entendu un pourcentage de pr\u00e9cision pour ne pas exclure les cas extr\u00eames , mais encore une fois, c'est au r\u00f4le du machine learning d'effectuer ces v\u00e9rifications. Dans notre datasets, il n'y aura plus cas rajouter les informations de la premi\u00e8re m\u00e9thode et les distances de la deuxi\u00e8me m\u00e9thode . Pour les deux m\u00e9thodes, comme nous le faisons \u00e0 droite et \u00e0 gauche, nous reprenons \u00e0 chaque fois la moyenne entre toutes les valeurs . Rappels / conclusions Il y a 5 donn\u00e9es , 2 horizontales et 3 verticales . Elles correspondent \u00e0 notre datasets. C'est ces donn\u00e9es que l'on donnera au mod\u00e8le du machine learning. En plus, il nous faudra une derni\u00e8re donn\u00e9e, l'une des plus importantes pour entra\u00een\u00e9 notre machine learning : la direction du visage . Comme on sait o\u00f9 se situe nos images de test et \u00e0 quoi elles correspondent, on aura plus qu'\u00e0 rajouter : \"up\", \"down\", \"left\", \"right\", \"front\" . Deuxi\u00e8me \u00e9tape : tester un mod\u00e8le Comme dit pr\u00e9c\u00e9demment, le package scikit-learn fourni les mod\u00e8les, il suffit simplement d'importer les mod\u00e8les souhait\u00e9s. Sur ce site , vous trouvez tous les types de mod\u00e8les. Ceux qui nous int\u00e9ressent finissent la plupart du temps par le mot Classifier . Pour cette documentation, on testera le mod\u00e8le RandomForestClassifier . Avant tout, il est important de comprendre comment il fonctionne. Random forest signifie for\u00eat al\u00e9atoire , il est bas\u00e9 sur les arbres de d\u00e9cisions . L'arbre de d\u00e9cision, \u00e0 chaque valeur d'une premi\u00e8re donn\u00e9e , la divise en plusieurs possibilit\u00e9s . Il va choisir ensuite celle qui correspond la mieux , puis va proposer toutes les possibilit\u00e9s d'une deuxi\u00e8me donn\u00e9e , etc. Le principe de la random forest est de multiplier al\u00e9atoirement les arbres de d\u00e9cisions. Chaque arbre appr\u00e9hendera les donn\u00e9es de fa\u00e7on diff\u00e9rentes . \u00c0 la fin, la donn\u00e9e \u00e0 pr\u00e9dire aura travers\u00e9e chaque arbre , les r\u00e9sultats seront assembl\u00e9s afin de proposer une pr\u00e9diction finale . Code Pour pouvoir commencer \u00e0 cr\u00e9er notre mod\u00e8le et l'entra\u00eener , il faut des donn\u00e9es. Dans notre solution, les donn\u00e9es qui ont servi \u00e0 entra\u00eener le mod\u00e8le se trouvaient dans les dossiers app/generation_nft/librairies/tilt_learning/tests/... . Dans le dossier se trouvait 5 dossiers correspondant aux diff\u00e9rentes directions des visages. Gr\u00e2ce \u00e0 la premi\u00e8re \u00e9tape , nous avons un fichier JSON comportant toutes les informations n\u00e9cessaires. Il n'y a plus qu'a import\u00e9 les donn\u00e9es dans une variable appel\u00e9e datasets . from pandas import read_json data_frame = read_json ( self . tilt_datasets_path ) data_frame est du type DataFrame de la libraire pandas. Cette classe est analysable par tous les mod\u00e8les de machine learning fourni par scikit-learn . Ensuite, il faut diviser ces donn\u00e9es en deux , les donn\u00e9es de d'entra\u00eenement et les donn\u00e9es de test . Les donn\u00e9es d'entra\u00eenement serviront \u00e0, comme son nom l'indique, entra\u00eener le mod\u00e8le . On fournira au mod\u00e8le ces donn\u00e9es avec la direction du visage pour chaque donn\u00e9e. Pour les donn\u00e9es de test, on essayera de les pr\u00e9dire et de faire correspondre la direction pr\u00e9dite avec la r\u00e9elle direction, c'est ce qui nous donnera un pourcentage de pr\u00e9cision du mod\u00e8le . Pour effectuer cette s\u00e9paration, il faut utiliser la fonction train_test_split . Elle accepte un param\u00e8tre test_size qui correspond \u00e0 un pourcentage de division . 0.2 signifie que 20% des donn\u00e9es seront des donn\u00e9es de test et 80% seront celles qui entra\u00eeneront le mod\u00e8le. from sklearn.model_selection import train_test_split trainset , testset = train_test_split ( data_frame , test_size = 0.2 , random_state = 0 ) Pour pouvoir ins\u00e9rer les donn\u00e9es correctement dans le mod\u00e8le, il faut les \"preprocess\" , les traiter. C'est-\u00e0-dire encoder puis isoler la donn\u00e9e cible de pr\u00e9diction. De notre c\u00f4t\u00e9, c'est la direction du visage, la derni\u00e8re colonne dans le DataFrame . { \"horizontal_distance_difference\" : 96 , \"horizontal_z_difference\" : 92 , \"vertical_distance_difference\" : 91 , \"percentage_distance_middlepoint_difference\" : 64 , \"class\" : \"front\" } Pour l'encodage, nous allons convertir les valeurs textes en chiffres : code = { \"up\" : 1 , \"down\" : 1 , \"left\" : 1 , \"right\" : 1 , \"front\" : 2 } for column in data_frame . select_dtypes ( \"object\" ): data_frame [ column ] = data_frame [ column ] . map ( code ) Toutes les directions sauf \"front\" auront la valeur 1. Pour faire la modification, on parcourt les colonnes textes ( object ) du DataFrame et on effectue un mapping des valeurs . Pour l'isolation de la donn\u00e9e cible, il nous suffit de slice le DataFrame, comme s'il s'agissait d'une liste ou un tableau. target_column = \"class\" x = data_frame . drop ( target_column , axis = 1 ) y = data_frame [ target_column ] Passons \u00e0 l'\u00e9tape tant attendue, l'ajout des donn\u00e9es dans le mod\u00e8le. Il faut tout d'abord, importer le mod\u00e8le . Nous allons aussi importer en plus diff\u00e9rentes fonctions qui permettrons de mettre une valeur sur la pr\u00e9cision de la pr\u00e9diction . Ensuite, il ne nous reste plus qu'\u00e0 entra\u00eener notre mod\u00e8le, puis de pr\u00e9dire les donn\u00e9es de test. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score x_train , y_train = self . preprocessing ( trainset ) # FONCTION QUI ENCODE ET ISOLE x_test , y_test = self . preprocessing ( testset ) # FONCTION QUI ENCODE ET ISOLE model = RandomForestClassifier () model . fit ( x_train , y_train ) y_pred = model . predict ( x_test ) print ( accuracy_score ( y_test , y_pred )) Et voil\u00e0, le mod\u00e8le du machine learning a \u00e9t\u00e9 cr\u00e9\u00e9 et entra\u00een\u00e9 . Bien entendu, il y a plusieurs param\u00e8trages pour am\u00e9liorer la pr\u00e9cision pour chaque mod\u00e8le. Pour savoir quels sont les param\u00e8tres les plus optimaux afin d'am\u00e9liorer la pr\u00e9cision, il faut utiliser GridSearchCV . Il n'y aura pas plus d'information concernant cette m\u00e9thode dans cette documentation, mais je vous conseille d'all\u00e9 regarder cette vid\u00e9o . Troisi\u00e8me \u00e9tape : g\u00e9n\u00e9rer son model Dernier point de cette documentation, l'exportation du mod\u00e8le . En production, nous n'allons pas r\u00e9entrainer le mod\u00e8le et retester la pr\u00e9diction. Nous souhaitons uniquement lui fournir une donn\u00e9e et que le mod\u00e8le la pr\u00e9dise . Pour exporter le mod\u00e8le, le package joblib nous permet de le faire tr\u00e8s simplement. from joblib import dump as dump_model dump_model ( model , f \" { self . model_path } _ { name } _model.pkl\" ) # URL OUTPUT Dans le code, il suffira de charger le mod\u00e8le avec la m\u00eame librairie. from joblib import load as load_model from sklearn.feature_extraction import DictVectorizer model = load_model ( f \" { self . model_path } _ { dict_model . get ( 'name' ) } _model.pkl\" ) datasets_array = DictVectorizer () . fit_transform ( datasets ) . toarray () # CONVERTI LES DONNEES EN DATAFRAME prediction = model . predict ( datasets_array )[ 0 ] # CORRESPOND \u00e0 1 (pas front) ou 2 (front) Si vous avez des questions, n'h\u00e9sitez pas \u00e0 les poser sur les supports adapt\u00e9s.","title":"Direction du visage"},{"location":"generation_nft/fonctionnalites/tilt_learning/#direction-du-regard","text":"","title":"Direction du regard"},{"location":"generation_nft/fonctionnalites/tilt_learning/#definition","text":"Le tilt learning est un algorithme de machine learning . Il a \u00e9t\u00e9 cr\u00e9\u00e9 pour savoir si le visage d\u00e9tect\u00e9 est orient\u00e9 de la bonne fa\u00e7on. Un visage correctement orient\u00e9 est un visage qui regarde droit devant .","title":"D\u00e9finition"},{"location":"generation_nft/fonctionnalites/tilt_learning/#contexte","text":"Le but est de pouvoir g\u00e9n\u00e9rer une image originale tir\u00e9e d'un visage d'un joueur de foot . La probl\u00e9matique \u00e9tait de d\u00e9tecter un visage. Gr\u00e2ce \u00e0 plusieurs packages externes : OpenCV et MediaPipe , il a \u00e9t\u00e9 tr\u00e8s facile de r\u00e9cup\u00e9rer plusieurs points importants du visage afin de tracer les contours . Cependant, la question de la direction du regard s'est vite pos\u00e9e. Il n'y a aucun package externe pour r\u00e9aliser cette v\u00e9rification de fa\u00e7on optimale . Il existe des solutions similaires, mais uniquement pour des vid\u00e9os . L'une des solutions les moins co\u00fbteuses en termes de temps que ce soit au niveau du temps de veille , du temps de mont\u00e9 en comp\u00e9tence et du temps de mise en place est le machine learning . Il existe d'autres notions efficaces : le deep learning et les r\u00e9seaux neuronaux . Ces deux notions sont cependant bien plus complexes et ont une pr\u00e9cision similaire compar\u00e9e \u00e0 un bon machine learning bien entrain\u00e9.","title":"Contexte"},{"location":"generation_nft/fonctionnalites/tilt_learning/#ce-quil-faut-savoir-sur-le-machine-learning","text":"Par d\u00e9finition, le machine learning est une technologie d\u2019intelligence artificielle permettant aux ordinateurs d\u2019apprendre sans avoir \u00e9t\u00e9 programm\u00e9s explicitement \u00e0 cet effet. Pour apprendre et se d\u00e9velopper, les ordinateurs ont toutefois besoin de donn\u00e9es \u00e0 analyser et sur lesquelles s\u2019entra\u00eener. La plupart du temps, le machine learning est pour r\u00e9aliser une pr\u00e9diction . En r\u00e9alit\u00e9, ces technologies d'intelligence artificielle sont des algorithmes (mod\u00e8les) qui interpr\u00e8tent plusieurs donn\u00e9es et les regroupent sous forme logique . Il est possible de repr\u00e9senter ce que fait le mod\u00e8le de machine learning sous forme d'un graphique .","title":"Ce qu'il faut savoir sur le machine learning"},{"location":"generation_nft/fonctionnalites/tilt_learning/#prenons-lexemple-avec-le-modele-de-type-clustering","text":"On peut voir plusieurs types d'algorithmes clustering (colonne). Ce sont des algorithmes qui vont regrouper de mani\u00e8re logique diff\u00e9rentes donn\u00e9es afin d'en faire une cat\u00e9gorie . Ces donn\u00e9es sont consid\u00e9r\u00e9es comme des donn\u00e9es d'entrainement . Chacune de ces donn\u00e9es peut \u00eatre identifi\u00e9e afin de fournir une base \u00e0 l'algorithme. Dans notre exemple, il n'est pas obligatoire de lui fournir une cat\u00e9gorisation. Le clustering peut regroup\u00e9 de mani\u00e8re math\u00e9matique les donn\u00e9es. Lorsque l'on testera une nouvelle donn\u00e9e pour faire une pr\u00e9diction, l'algorithme analysera les donn\u00e9es de la m\u00eame mani\u00e8re que les donn\u00e9es qui ont servi pour tester le mod\u00e8le . Il pourra par la suite dire que cette donn\u00e9e correspond aux donn\u00e9es de la cat\u00e9gorie x.","title":"Prenons l'exemple avec le mod\u00e8le de type clustering :"},{"location":"generation_nft/fonctionnalites/tilt_learning/#type-de-machine-learning","text":"Il existe beaucoup de types de machine learning qui eu m\u00eame ont plusieurs sous types . Dans notre solution, nous utilisons le package scikit-learn qui permet de facilit\u00e9 l'utilisation de ces mod\u00e8les. Scikit-learn fourni une tr\u00e8s bonne documentation pour conna\u00eetre tous les param\u00e8tres de chaque mod\u00e8le et leur fonctionnement. Chaque mod\u00e8le est optimis\u00e9 pour des types de donn\u00e9es , quantit\u00e9 de donn\u00e9es et l'objectif de la pr\u00e9diction. Voici un sch\u00e9ma pour savoir quel mod\u00e8le sera le plus adapt\u00e9 \u00e0 ce que l'on souhaite faire :","title":"Type de machine learning"},{"location":"generation_nft/fonctionnalites/tilt_learning/#nos-modeles","text":"Dans notre solution, si on suit le sch\u00e9ma au-dessus, nous avons plus de 50 donn\u00e9es de test (diff\u00e9rentes images de plusieurs visages regardant dans des directions diff\u00e9rentes). Nous souhaitons pr\u00e9dire une cat\u00e9gorie . Nos cat\u00e9gories ici seront les diff\u00e9rentes directions : haut, bas, gauche, droit, en face . Le fait de vouloir pr\u00e9dire une cat\u00e9gorie signifie que les mod\u00e8les de type regression et dimensionality reduction ne seront pas tr\u00e8s utiles. Par la suite, le sch\u00e9ma nous demande s'il y a la pr\u00e9sence de texte : oui (haut, bas ...) . Les mod\u00e8les clustering ne sont donc pas conseill\u00e9s. Ensuite, nous rentrons enfin dans un type de machine learning : classification . La premi\u00e8re information importante est de conna\u00eetre le nombre de donn\u00e9es de test . Malheureusement, r\u00e9unir plus de 100 000 donn\u00e9es quand il s'agit d'image est une mission quasiment impossible, sachant qu'il faut toutes les identifier en termes de direction. ???+ info \"Augmentation d'image\" Cependant, il est possible de faire de **l'augmentation d'image**, c'est-\u00e0-dire **multiplier** le nombre d'images avec **celle que l'on a d\u00e9j\u00e0**. Par exemple, retourner l'image horizontalement. Ce qui multipliera par deux le nombre d'images. <div style=\"display:flex;flex-direction:row;justify-content:center;margin-bottom:10px;\"> <img src=\"../../../pictures/generation_nft/tilt_learning/kilian-left.jpg\" style=\"width:321px;height:191px;margin-right:10px;\"> <img src=\"../../../pictures/generation_nft/tilt_learning/kilian-right.jpg\" style=\"width:321px;height:191px;margin-left:10px;\"> </div> Nous croisons un premier mod\u00e8le : LinearSVC . Nous pourrions nous arr\u00eater ici et tester ce mod\u00e8le avec nos donn\u00e9es. Mais pour que notre pr\u00e9diction soit la plus pr\u00e9cise possible, nous pouvons s\u00e9lectionner plusieurs mod\u00e8les . Nous allons donc s\u00e9lectionner les mod\u00e8les suivant: Naive Bayes, KNeighbors classifier, SVC et Ensemble classifiers . \u00c0 savoir qu'il existe plusieurs types de mod\u00e8le pour le Naive Bayes, pour le KNeighbors classifier et pour les Ensemble classifiers. Le plus efficace pour pr\u00e9dire correctement est d'avoir un nombre impair de mod\u00e8les pour r\u00e9cup\u00e9rer \u00e0 chaque fois la pr\u00e9diction la plus pr\u00e9sente . Si jamais le nombre est pair est qu'une \u00e9galit\u00e9 survient, il sera difficile d'identifi\u00e9 la pr\u00e9diction a privil\u00e9gier.","title":"Nos mod\u00e8les"},{"location":"generation_nft/fonctionnalites/tilt_learning/#liste-des-packages-a-installer","text":"numpy imutils dlib mediapipe pandas scikit-learn opencv-python scikit-image joblib","title":"Liste des packages \u00e0 installer"},{"location":"generation_nft/fonctionnalites/tilt_learning/#premiere-etape-formatage-et-preprocessing-des-donnees","text":"Pour pouvoir r\u00e9aliser un machine learning efficace, outre le nombre d'image, il faut des donn\u00e9es coh\u00e9rentes et nettoyer . A ce stade, nous avons d\u00e9j\u00e0 r\u00e9cup\u00e9r\u00e9 les landmarks (zone du visage). Voir la documentation sur la d\u00e9tection de la zone du visage pour comprendre quelles sont les donn\u00e9es r\u00e9cup\u00e9r\u00e9es. Nous avons donc 486 points du visage avec un x correspondant \u00e0 la position en largeur , un y correspondant \u00e0 la position en hauteur et enfin un z correspondant \u00e0 la profondeur. Dans la documentation, j'explique que le z est calcul\u00e9 par rapport \u00e0 une feuille \u00e0 plat . Tout ce qui est avant (plus pr\u00e8s de la cam\u00e9ra) sera positif , tout ce qui est apr\u00e8s (plus loin dans la photo) sera n\u00e9gatif . Cependant, ces valeurs sont toutes diff\u00e9rentes selon la photo, selon le visage et surtout selon l'\u00e9chelle qui n'est jamais la m\u00eame . Cette notion d'\u00e9chelle est tr\u00e8s importante pour le machine learning. Il sera beaucoup moins efficace si ces donn\u00e9es ne sont pas sur la m\u00eame plage de donn\u00e9es. \u00c0 ce stade nous avons donc 486 points * 3 (x, y et z). Si nous donnons les coordonn\u00e9es de tous ces points en donn\u00e9es, il y a de fortes chances qu'ils ne trouvent jamais si la photo est correcte . Les visages sont tous diff\u00e9rents , des personnes ont des mentons moins profonds que le front, d'autres ont un nez plus imposant, ... Il faut donc r\u00e9cup\u00e9rer uniquement les informations qui nous permettront d'avoir des informations performantes et pertinentes .","title":"Premi\u00e8re \u00e9tape - formatage et preprocessing des donn\u00e9es"},{"location":"generation_nft/fonctionnalites/tilt_learning/#direction-horizontale","text":"Il existe plusieurs fa\u00e7ons , avec les points que l'on a, pour d\u00e9terminer si le visage regarde \u00e0 gauche ou \u00e0 droite . Pour que le machine learning soit efficace , nous allons r\u00e9cup\u00e9rer deux donn\u00e9es qui \u00e0 elle seule pourraient d\u00e9terminer si le visage regarde dans une direction. Cependant, si l'on augmente le nombre de donn\u00e9es, le machine learning sera bien plus pr\u00e9cis . Avant de commencer, toutes ses donn\u00e9es partent du principe que chaque visage a une diff\u00e9rence de sym\u00e9trie n\u00e9gligeable . Nous avons tous plus ou moins un visage sym\u00e9trique. Et la cible que nous souhaitons \"cartoonis\u00e9\" sont en g\u00e9n\u00e9ral tr\u00e8s sportif et sans handicap majeur . Rib\u00e9ry est notre ennemi Premi\u00e8re m\u00e9thode La profondeur de deux points peut d\u00e9terminer une direction. Dans notre cas, nous prenons le point du visage le plus \u00e0 gauche et le point du visage le plus \u00e0 droite . Et nous regardons la diff\u00e9rence en pourcentage entre les deux profondeurs. Si la diff\u00e9rence d\u00e9passe un certain seuil, seuil qui sera g\u00e9r\u00e9 par le machine learning , le visage regardera \u00e0 droite ou \u00e0 gauche. Pour mieux comprendre, nous allons, prendre l'axe Z que nous allons utilis\u00e9 comme X et l'axe des Y . Sur l'exemple, si on regarde le sch\u00e9ma, on voit que le point de gauche \u00e0 sa profondeur Z ( x dans le sch\u00e9ma ) plus proche de 0 , le 0 correspond ici \u00e0 l' objectif de la cam\u00e9ra (ou l'\u00e9cran), que le point de droite l\u00e9g\u00e8rement plus \u00e9loign\u00e9 de 0, plus profond . Si vous mettez vos doigts de chaque c\u00f4t\u00e9 de votre visage et que votre \u00e9cran d'ordinateur correspond au 0 , vous verrez que si vous tournez la t\u00eate \u00e0 gauche , votre doigt \u00e0 droite de votre visage se rapproche de l'\u00e9cran tandis que le point gauche s'en \u00e9loigne. Ces donn\u00e9es pourront donc \u00eatre ajout\u00e9es \u00e0 notre datasets (jeu de donn\u00e9es qui seront analys\u00e9es par le machine learning). Deuxi\u00e8me m\u00e9thode Cette m\u00e9thode se base essentiellement sur la sym\u00e9trie du visage . En toute logique, un visage qui regarde la cam\u00e9ra droit devant, aura des proportions identiques entre la partie gauche du visage et la partie droite du visage . Il suffit, pour v\u00e9rifier, de prendre une distance entre deux points sur la partie gauche, et prendre leur deux points sym\u00e9triques du c\u00f4t\u00e9 droit du visage et calculer la diff\u00e9rence entre les deux distances de chaque point. On remarque assez facilement que la distance \u00e0 gauche est plus grande que la distance \u00e0 droite. Ce ph\u00e9nom\u00e8ne s'explique par le fait que la partie d'un visage est compress\u00e9e par rapport \u00e0 l'observateur . Nous avons deux valeurs plut\u00f4t pr\u00e9cises qui assembl\u00e9es, augmenterons la performance du machine learning \u00e0 d\u00e9tecter si le visage regarde trop \u00e0 droite ou trop \u00e0 gauche .","title":"Direction horizontale"},{"location":"generation_nft/fonctionnalites/tilt_learning/#direction-verticale","text":"Cette partie a \u00e9t\u00e9 la plus compliqu\u00e9e \u00e0 r\u00e9aliser, mais elle s'av\u00e8re la plus efficace. Pour identifier la direction vers le haut ou vers le bas , il y a plusieurs facteurs qui compliquent la lecture des donn\u00e9es. Les visages sont tous diff\u00e9rents en ce qui concerne la profondeur de leur visage. Logiquement, on pourrait se dire : c'est plut\u00f4t simple, on prend la profondeur du point le plus haut et du plus bas du visage et on regarde la diff\u00e9rence. Voil\u00e0 pourquoi il n'est pas possible de faire cela : Ici, comme pour la premi\u00e8re m\u00e9thode horizontale , on met la profondeur sur X en gardant Y. Cela nous permet d'avoir le profil de la photo . On voit clairement que pour la premi\u00e8re, il n'y aurait pas de probl\u00e8me, cependant la seconde, on remarque que son menton est toujours plus avanc\u00e9 que son front . Si on devait ajouter ces donn\u00e9es dans un machine learning, je suis s\u00fbr que la pr\u00e9diction serait souvent fausse car ses donn\u00e9es ressembleraient trop aux donn\u00e9es d'une personne qui regarde l\u00e9g\u00e8rement vers le haut . Cela fonctionne aussi dans le sens inverse (le bas), m\u00eame si c'est plus rare. La premi\u00e8re m\u00e9thode consiste \u00e0 rajouter deux points de r\u00e9f\u00e9rences au niveau du nez et du c\u00f4t\u00e9 de l'oreille . Ces points de r\u00e9f\u00e9rences nous permettront de r\u00e9aliser plusieurs calculs pour en retirer des informations . Bien s\u00fbr la m\u00e9thode sera multipli\u00e9e par deux, c\u00f4t\u00e9 gauche et c\u00f4t\u00e9 droit . Premi\u00e8re m\u00e9thode Ce sont les lignes grises et roses (zoomer pour les voir) qui vont nous int\u00e9resser dans un premier temps. Nous allons reprendre la diff\u00e9rence entre les deux lignes grises en bas et en haut qui correspondent \u00e0 la distance entre le front et le nez et le menton et le nez . Puis reprendre la hauteur entre le nez et le point \u00e0 c\u00f4t\u00e9 de l'oreille (les lignes roses). Avec ces deux donn\u00e9es, le machine learning pourra d\u00e9j\u00e0 pr\u00e9dire correctement plus de la moiti\u00e9 des cas . Il faut savoir que cette m\u00e9thode r\u00e9duit l\u00e9g\u00e8rement le facteur de la morphologie du visage , mais elle ne pourra pas fonctionner seule, il faut obligatoirement la deuxi\u00e8me m\u00e9thode pour que le machine learning puisse identifier un pattern dans ces donn\u00e9es. Deuxi\u00e8me m\u00e9thode Pour que la premi\u00e8re m\u00e9thode fonctionne, il faut une valeur verticale de r\u00e9f\u00e9rence . Il y a beaucoup d'\u00e9tudes plus ou moins scientifique qui ont \u00e9t\u00e9 faites sur la morphologie et sur les diff\u00e9rentes r\u00e8gles de mensuration d'un visage. Celle qui nous int\u00e9resse ici est la mensuration entre le menton et le nez et le nez et le haut front . Pour mieux comprendre, voici une image : La plupart du temps, cette proportion est respect\u00e9e, il y a bien entendu un pourcentage de pr\u00e9cision pour ne pas exclure les cas extr\u00eames , mais encore une fois, c'est au r\u00f4le du machine learning d'effectuer ces v\u00e9rifications. Dans notre datasets, il n'y aura plus cas rajouter les informations de la premi\u00e8re m\u00e9thode et les distances de la deuxi\u00e8me m\u00e9thode . Pour les deux m\u00e9thodes, comme nous le faisons \u00e0 droite et \u00e0 gauche, nous reprenons \u00e0 chaque fois la moyenne entre toutes les valeurs .","title":"Direction verticale"},{"location":"generation_nft/fonctionnalites/tilt_learning/#rappels-conclusions","text":"Il y a 5 donn\u00e9es , 2 horizontales et 3 verticales . Elles correspondent \u00e0 notre datasets. C'est ces donn\u00e9es que l'on donnera au mod\u00e8le du machine learning. En plus, il nous faudra une derni\u00e8re donn\u00e9e, l'une des plus importantes pour entra\u00een\u00e9 notre machine learning : la direction du visage . Comme on sait o\u00f9 se situe nos images de test et \u00e0 quoi elles correspondent, on aura plus qu'\u00e0 rajouter : \"up\", \"down\", \"left\", \"right\", \"front\" .","title":"Rappels / conclusions"},{"location":"generation_nft/fonctionnalites/tilt_learning/#deuxieme-etape-tester-un-modele","text":"Comme dit pr\u00e9c\u00e9demment, le package scikit-learn fourni les mod\u00e8les, il suffit simplement d'importer les mod\u00e8les souhait\u00e9s. Sur ce site , vous trouvez tous les types de mod\u00e8les. Ceux qui nous int\u00e9ressent finissent la plupart du temps par le mot Classifier . Pour cette documentation, on testera le mod\u00e8le RandomForestClassifier . Avant tout, il est important de comprendre comment il fonctionne. Random forest signifie for\u00eat al\u00e9atoire , il est bas\u00e9 sur les arbres de d\u00e9cisions . L'arbre de d\u00e9cision, \u00e0 chaque valeur d'une premi\u00e8re donn\u00e9e , la divise en plusieurs possibilit\u00e9s . Il va choisir ensuite celle qui correspond la mieux , puis va proposer toutes les possibilit\u00e9s d'une deuxi\u00e8me donn\u00e9e , etc. Le principe de la random forest est de multiplier al\u00e9atoirement les arbres de d\u00e9cisions. Chaque arbre appr\u00e9hendera les donn\u00e9es de fa\u00e7on diff\u00e9rentes . \u00c0 la fin, la donn\u00e9e \u00e0 pr\u00e9dire aura travers\u00e9e chaque arbre , les r\u00e9sultats seront assembl\u00e9s afin de proposer une pr\u00e9diction finale .","title":"Deuxi\u00e8me \u00e9tape : tester un mod\u00e8le"},{"location":"generation_nft/fonctionnalites/tilt_learning/#code","text":"Pour pouvoir commencer \u00e0 cr\u00e9er notre mod\u00e8le et l'entra\u00eener , il faut des donn\u00e9es. Dans notre solution, les donn\u00e9es qui ont servi \u00e0 entra\u00eener le mod\u00e8le se trouvaient dans les dossiers app/generation_nft/librairies/tilt_learning/tests/... . Dans le dossier se trouvait 5 dossiers correspondant aux diff\u00e9rentes directions des visages. Gr\u00e2ce \u00e0 la premi\u00e8re \u00e9tape , nous avons un fichier JSON comportant toutes les informations n\u00e9cessaires. Il n'y a plus qu'a import\u00e9 les donn\u00e9es dans une variable appel\u00e9e datasets . from pandas import read_json data_frame = read_json ( self . tilt_datasets_path ) data_frame est du type DataFrame de la libraire pandas. Cette classe est analysable par tous les mod\u00e8les de machine learning fourni par scikit-learn . Ensuite, il faut diviser ces donn\u00e9es en deux , les donn\u00e9es de d'entra\u00eenement et les donn\u00e9es de test . Les donn\u00e9es d'entra\u00eenement serviront \u00e0, comme son nom l'indique, entra\u00eener le mod\u00e8le . On fournira au mod\u00e8le ces donn\u00e9es avec la direction du visage pour chaque donn\u00e9e. Pour les donn\u00e9es de test, on essayera de les pr\u00e9dire et de faire correspondre la direction pr\u00e9dite avec la r\u00e9elle direction, c'est ce qui nous donnera un pourcentage de pr\u00e9cision du mod\u00e8le . Pour effectuer cette s\u00e9paration, il faut utiliser la fonction train_test_split . Elle accepte un param\u00e8tre test_size qui correspond \u00e0 un pourcentage de division . 0.2 signifie que 20% des donn\u00e9es seront des donn\u00e9es de test et 80% seront celles qui entra\u00eeneront le mod\u00e8le. from sklearn.model_selection import train_test_split trainset , testset = train_test_split ( data_frame , test_size = 0.2 , random_state = 0 ) Pour pouvoir ins\u00e9rer les donn\u00e9es correctement dans le mod\u00e8le, il faut les \"preprocess\" , les traiter. C'est-\u00e0-dire encoder puis isoler la donn\u00e9e cible de pr\u00e9diction. De notre c\u00f4t\u00e9, c'est la direction du visage, la derni\u00e8re colonne dans le DataFrame . { \"horizontal_distance_difference\" : 96 , \"horizontal_z_difference\" : 92 , \"vertical_distance_difference\" : 91 , \"percentage_distance_middlepoint_difference\" : 64 , \"class\" : \"front\" } Pour l'encodage, nous allons convertir les valeurs textes en chiffres : code = { \"up\" : 1 , \"down\" : 1 , \"left\" : 1 , \"right\" : 1 , \"front\" : 2 } for column in data_frame . select_dtypes ( \"object\" ): data_frame [ column ] = data_frame [ column ] . map ( code ) Toutes les directions sauf \"front\" auront la valeur 1. Pour faire la modification, on parcourt les colonnes textes ( object ) du DataFrame et on effectue un mapping des valeurs . Pour l'isolation de la donn\u00e9e cible, il nous suffit de slice le DataFrame, comme s'il s'agissait d'une liste ou un tableau. target_column = \"class\" x = data_frame . drop ( target_column , axis = 1 ) y = data_frame [ target_column ] Passons \u00e0 l'\u00e9tape tant attendue, l'ajout des donn\u00e9es dans le mod\u00e8le. Il faut tout d'abord, importer le mod\u00e8le . Nous allons aussi importer en plus diff\u00e9rentes fonctions qui permettrons de mettre une valeur sur la pr\u00e9cision de la pr\u00e9diction . Ensuite, il ne nous reste plus qu'\u00e0 entra\u00eener notre mod\u00e8le, puis de pr\u00e9dire les donn\u00e9es de test. from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import accuracy_score x_train , y_train = self . preprocessing ( trainset ) # FONCTION QUI ENCODE ET ISOLE x_test , y_test = self . preprocessing ( testset ) # FONCTION QUI ENCODE ET ISOLE model = RandomForestClassifier () model . fit ( x_train , y_train ) y_pred = model . predict ( x_test ) print ( accuracy_score ( y_test , y_pred )) Et voil\u00e0, le mod\u00e8le du machine learning a \u00e9t\u00e9 cr\u00e9\u00e9 et entra\u00een\u00e9 . Bien entendu, il y a plusieurs param\u00e8trages pour am\u00e9liorer la pr\u00e9cision pour chaque mod\u00e8le. Pour savoir quels sont les param\u00e8tres les plus optimaux afin d'am\u00e9liorer la pr\u00e9cision, il faut utiliser GridSearchCV . Il n'y aura pas plus d'information concernant cette m\u00e9thode dans cette documentation, mais je vous conseille d'all\u00e9 regarder cette vid\u00e9o .","title":"Code"},{"location":"generation_nft/fonctionnalites/tilt_learning/#troisieme-etape-generer-son-model","text":"Dernier point de cette documentation, l'exportation du mod\u00e8le . En production, nous n'allons pas r\u00e9entrainer le mod\u00e8le et retester la pr\u00e9diction. Nous souhaitons uniquement lui fournir une donn\u00e9e et que le mod\u00e8le la pr\u00e9dise . Pour exporter le mod\u00e8le, le package joblib nous permet de le faire tr\u00e8s simplement. from joblib import dump as dump_model dump_model ( model , f \" { self . model_path } _ { name } _model.pkl\" ) # URL OUTPUT Dans le code, il suffira de charger le mod\u00e8le avec la m\u00eame librairie. from joblib import load as load_model from sklearn.feature_extraction import DictVectorizer model = load_model ( f \" { self . model_path } _ { dict_model . get ( 'name' ) } _model.pkl\" ) datasets_array = DictVectorizer () . fit_transform ( datasets ) . toarray () # CONVERTI LES DONNEES EN DATAFRAME prediction = model . predict ( datasets_array )[ 0 ] # CORRESPOND \u00e0 1 (pas front) ou 2 (front) Si vous avez des questions, n'h\u00e9sitez pas \u00e0 les poser sur les supports adapt\u00e9s.","title":"Troisi\u00e8me \u00e9tape : g\u00e9n\u00e9rer son model"},{"location":"generation_nft/python/architecture/","text":"Architecture Explication Python n'est pas un langage qui pr\u00e9conise des architectures compliqu\u00e9es . Il est possible d'appliquer tous les design pattern commun \u00e0 la plupart des autres langages, cependant, dans le monde pythonic, ce n'est absolument pas la norme . L'architecture d'une application Python est la plus basique qui existe et se base sur le syst\u00e8me de module et de package. Notre application est structur\u00e9e en plusieurs dossiers , si ce dossier comporte le fichier __init__.py : c'est un module, sinon c'est juste un dossier lambda qui ne sera pas atteignable par des fichiers externes au dossier. Le fichier __init__.py peut \u00eatre laiss\u00e9 vide ou comporter du code. R\u00e8gles de nommage Le monde du langage python est tr\u00e8s r\u00e9glement\u00e9 et notamment gr\u00e2ce au PEP. Une r\u00e8gle sur les noms des fichiers, des dossiers, des variables, des fonctions et des classes doit \u00eatre sp\u00e9cifi\u00e9e. nom_du_dossier nom_du_fichier.py test_nom_du_fichier.py : pour un fichier de test, le nom doit commencer par \"test\" . nom_variable nom_fonction NomClasse Structure Tous nos d\u00e9veloppements, except\u00e9 pour la partie documentation, se situera dans le module parent app . Ce module est divis\u00e9 en deux modules parents. - generation_nft : comporte toute la logique autour du NFT. - generation_nft_api : API et tous ce qui est li\u00e9. Cette structure ne changera pas. Pour vous repr\u00e9senter plus facilement l'architecture, le squelette ci-dessous montre les diff\u00e9rentes parties. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 . env \u2502 \u251c\u2500\u2500 . env . example \u2502 \u251c\u2500\u2500 logging_configuration . py \u2502 \u251c\u2500\u2500 launch . py \u2502 \u251c\u2500\u2500 settings . py \u2502 \u251c\u2500\u2500 logs \u2502 \u251c\u2500\u2500 generation_nft \u2502 \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u2502 \u251c\u2500\u2500 ... ( fichiers test_ *. py ) \u2502 \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2502 \u251c\u2500\u2500 ... ( listes des modules : machine learning , deep learning ... ) \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 generation_nft_api \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 ... ( fichiers test_ *. py ) \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 ... ( listes des fichiers de routes ) \u2502 \u2514\u2500\u2500 __init__ . py \u251c\u2500\u2500 dockerfiles \u251c\u2500\u2500 documentation ( uniquement en version d\u00e9veloppement ) \u251c\u2500\u2500 pronochain ( environnement local ) \u251c\u2500\u2500 docker - compose . dev . yml \u251c\u2500\u2500 docker - compose . prod . yml \u251c\u2500\u2500 docker - compose . yml \u251c\u2500\u2500 setup . cfg \u251c\u2500\u2500 setup . py \u251c\u2500\u2500 pytest . ini \u251c\u2500\u2500 . pyproject . toml \u251c\u2500\u2500 . pre - commit - config . yaml \u251c\u2500\u2500 requirements - dev . txt \u2514\u2500\u2500 requirements . txt Setuptools Plusieurs m\u00e9thodes existent pour importer les modules internes dans d'autres fichiers d'autres modules. Si ce sont des fichiers qui se trouvent dans le m\u00eame module ou que le fichier import\u00e9 se trouve plus bas dans le module, un simple import fonctionne. from sous_module import le_fichier from sous_module.le_fichier import la_fonction Cependant, si vous vous situez dans le fichier fichier_de_l_import.py et que vous souhaitez importer le fichier fichier_a_importer.py , il vous faudra faire attention \u00e0 comment l'importer. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 generation_nft \u2502 \u2502 \u251c\u2500\u2500 fichier_de_l_import . py \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 generation_nft_api \u2502 \u251c\u2500\u2500 fichier_a_importer . py \u2502 \u2514\u2500\u2500 __init__ . py \u2514\u2500\u2500 __init__ . py Il y a plusieurs m\u00e9thodes pour importer le fichier. Pour tous les exemples, nous nous situons dans le fichier fichier_de_l_import.py . Import relatif , consiste \u00e0 simplement importer le fichier en respectant l'architecture comme pour les commandes linux. from ..generation_nft_api import fichier_a_importer Import depuis un package ins\u00e9r\u00e9 dans les chemins de python . De base, python a une liste de chemin de dossiers qui seront accessibles directement par tous les fichiers peu importe o\u00f9 vous vous situez . Pour ajouter un dossier en particulier, il faut ex\u00e9cuter du code dans un fichier. Cette m\u00e9thode est la plus utilis\u00e9e dans le monde de python et permet d'avoir le contr\u00f4le total sur ce qui est accessible ou non. Nous n'utiliserons pas cette m\u00e9thode, car nous souhaitons que tous les modules soient accessibles. Import gr\u00e2ce \u00e0 Setuptools . Setuptools est un package permettant de cr\u00e9er nous-m\u00eames nos propres packages. Lorsque vous voulez utiliser une librairie, il suffit de simplement d'importer le nom de ce package et ce que vous souhaitez. Setuptools fait en sorte de cr\u00e9er un package pour chaque dossier qui comporte un fichier __init__.py . Il suffira par la suite d'importer depuis le module le plus haut. from app.generation_nft_api import fichier_a_importer Comme vous l'aurez compris, nous utilisons Setuptools . Vous n'avez absolument aucune manipulation \u00e0 faire pour importer un nouveau module mise \u00e0 part ajouter le fichier __init__.py dans le dossier.","title":"Architecture"},{"location":"generation_nft/python/architecture/#architecture","text":"","title":"Architecture"},{"location":"generation_nft/python/architecture/#explication","text":"Python n'est pas un langage qui pr\u00e9conise des architectures compliqu\u00e9es . Il est possible d'appliquer tous les design pattern commun \u00e0 la plupart des autres langages, cependant, dans le monde pythonic, ce n'est absolument pas la norme . L'architecture d'une application Python est la plus basique qui existe et se base sur le syst\u00e8me de module et de package. Notre application est structur\u00e9e en plusieurs dossiers , si ce dossier comporte le fichier __init__.py : c'est un module, sinon c'est juste un dossier lambda qui ne sera pas atteignable par des fichiers externes au dossier. Le fichier __init__.py peut \u00eatre laiss\u00e9 vide ou comporter du code.","title":"Explication"},{"location":"generation_nft/python/architecture/#regles-de-nommage","text":"Le monde du langage python est tr\u00e8s r\u00e9glement\u00e9 et notamment gr\u00e2ce au PEP. Une r\u00e8gle sur les noms des fichiers, des dossiers, des variables, des fonctions et des classes doit \u00eatre sp\u00e9cifi\u00e9e. nom_du_dossier nom_du_fichier.py test_nom_du_fichier.py : pour un fichier de test, le nom doit commencer par \"test\" . nom_variable nom_fonction NomClasse","title":"R\u00e8gles de nommage"},{"location":"generation_nft/python/architecture/#structure","text":"Tous nos d\u00e9veloppements, except\u00e9 pour la partie documentation, se situera dans le module parent app . Ce module est divis\u00e9 en deux modules parents. - generation_nft : comporte toute la logique autour du NFT. - generation_nft_api : API et tous ce qui est li\u00e9. Cette structure ne changera pas. Pour vous repr\u00e9senter plus facilement l'architecture, le squelette ci-dessous montre les diff\u00e9rentes parties. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 . env \u2502 \u251c\u2500\u2500 . env . example \u2502 \u251c\u2500\u2500 logging_configuration . py \u2502 \u251c\u2500\u2500 launch . py \u2502 \u251c\u2500\u2500 settings . py \u2502 \u251c\u2500\u2500 logs \u2502 \u251c\u2500\u2500 generation_nft \u2502 \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u2502 \u251c\u2500\u2500 ... ( fichiers test_ *. py ) \u2502 \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2502 \u251c\u2500\u2500 ... ( listes des modules : machine learning , deep learning ... ) \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 generation_nft_api \u2502 \u251c\u2500\u2500 tests \u2502 \u2502 \u251c\u2500\u2500 ... ( fichiers test_ *. py ) \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 ... ( listes des fichiers de routes ) \u2502 \u2514\u2500\u2500 __init__ . py \u251c\u2500\u2500 dockerfiles \u251c\u2500\u2500 documentation ( uniquement en version d\u00e9veloppement ) \u251c\u2500\u2500 pronochain ( environnement local ) \u251c\u2500\u2500 docker - compose . dev . yml \u251c\u2500\u2500 docker - compose . prod . yml \u251c\u2500\u2500 docker - compose . yml \u251c\u2500\u2500 setup . cfg \u251c\u2500\u2500 setup . py \u251c\u2500\u2500 pytest . ini \u251c\u2500\u2500 . pyproject . toml \u251c\u2500\u2500 . pre - commit - config . yaml \u251c\u2500\u2500 requirements - dev . txt \u2514\u2500\u2500 requirements . txt","title":"Structure"},{"location":"generation_nft/python/architecture/#setuptools","text":"Plusieurs m\u00e9thodes existent pour importer les modules internes dans d'autres fichiers d'autres modules. Si ce sont des fichiers qui se trouvent dans le m\u00eame module ou que le fichier import\u00e9 se trouve plus bas dans le module, un simple import fonctionne. from sous_module import le_fichier from sous_module.le_fichier import la_fonction Cependant, si vous vous situez dans le fichier fichier_de_l_import.py et que vous souhaitez importer le fichier fichier_a_importer.py , il vous faudra faire attention \u00e0 comment l'importer. . \u251c\u2500\u2500 app \u2502 \u251c\u2500\u2500 __init__ . py \u2502 \u251c\u2500\u2500 generation_nft \u2502 \u2502 \u251c\u2500\u2500 fichier_de_l_import . py \u2502 \u2502 \u2514\u2500\u2500 __init__ . py \u2502 \u2514\u2500\u2500 generation_nft_api \u2502 \u251c\u2500\u2500 fichier_a_importer . py \u2502 \u2514\u2500\u2500 __init__ . py \u2514\u2500\u2500 __init__ . py Il y a plusieurs m\u00e9thodes pour importer le fichier. Pour tous les exemples, nous nous situons dans le fichier fichier_de_l_import.py . Import relatif , consiste \u00e0 simplement importer le fichier en respectant l'architecture comme pour les commandes linux. from ..generation_nft_api import fichier_a_importer Import depuis un package ins\u00e9r\u00e9 dans les chemins de python . De base, python a une liste de chemin de dossiers qui seront accessibles directement par tous les fichiers peu importe o\u00f9 vous vous situez . Pour ajouter un dossier en particulier, il faut ex\u00e9cuter du code dans un fichier. Cette m\u00e9thode est la plus utilis\u00e9e dans le monde de python et permet d'avoir le contr\u00f4le total sur ce qui est accessible ou non. Nous n'utiliserons pas cette m\u00e9thode, car nous souhaitons que tous les modules soient accessibles. Import gr\u00e2ce \u00e0 Setuptools . Setuptools est un package permettant de cr\u00e9er nous-m\u00eames nos propres packages. Lorsque vous voulez utiliser une librairie, il suffit de simplement d'importer le nom de ce package et ce que vous souhaitez. Setuptools fait en sorte de cr\u00e9er un package pour chaque dossier qui comporte un fichier __init__.py . Il suffira par la suite d'importer depuis le module le plus haut. from app.generation_nft_api import fichier_a_importer Comme vous l'aurez compris, nous utilisons Setuptools . Vous n'avez absolument aucune manipulation \u00e0 faire pour importer un nouveau module mise \u00e0 part ajouter le fichier __init__.py dans le dossier.","title":"Setuptools"},{"location":"generation_nft/python/logging/","text":"Logging Explication Le syst\u00e8me de logs en python est int\u00e9gr\u00e9 . Il n'y a pas besoin de t\u00e9l\u00e9charger un package externe. Il permet de g\u00e9rer plusieurs m\u00e9thodes de logs avec diff\u00e9rents niveaux et des d\u00e9tails d'erreurs tr\u00e8s \u00e9volu\u00e9s. De plus, ce syst\u00e8me est compl\u00e8tement configurable. Pour notre solution, elle se trouve dans le fichier logging_configuration dans le dossier app . Formatter Le formatter est un \u00e9l\u00e9ment permettant d'indiquer les \u00e9l\u00e9ments pr\u00e9sents dans le message du log . Il existe plusieurs variables qui r\u00e9cup\u00e8rent les informations souhait\u00e9es. Notre solution en d\u00e9finit deux . Une pour les logs dans les fichiers et une pour les logs dans la console en mode d\u00e9veloppement. Format console : 'simpleForma tter ' : { ' f orma t ' : ' [ %(level na me)s ] - %(asc t ime)s - %( na me)s : %(message)s' } corresponds \u00e0 ce message dans la console : [ DEBUG ] - 2021 -12-19 11 :12:09,765 - generation_nft_api - : { 'Hello' : 'World' } Format fichier : 'verboseForma tter ' : { ' f orma t ' : ' [ %(level na me)s ] - %(asc t ime)s - %( na me)s - (%(module)s.py | de f %( fun cName)s() | l. %(li nen o)d) : %(message)s' } corresponds \u00e0 ce message dans la console : [ DEBUG ] - 2021 -12-19 11 :12:09,765 - generation_nft_api - ( main.py | def read_root () | l. 44 ) : { 'Hello' : 'World' } Handler Les handlers correspondent \u00e0 tous les types de logs. Il en existe plusieurs : Dans la console . Dans un fichier . Dans un mail . ... Dans notre solution, nous en utilisons deux : fichier et console . Cependant nous en avons trois , deux pour deux fichiers de logs concernant l'API et la g\u00e9n\u00e9ration du NFT et un pour la console en mode d\u00e9veloppement. La configuration d'un handler s'effectue sous cette forme : 'ge nerat io n N ft Ha n dler' : { 'level' : 'NOTSET' , ' f orma tter ' : 'verboseForma tter ' , 'class' : 'loggi n g.FileHa n dler' , ' f ile na me' : 'app/logs/ge nerat io n _ nft .log' , 'mode' : 'a' , }, Le level correspond aux informations que l'on souhaite r\u00e9cup\u00e9rer. Pas besoin de r\u00e9expliquer le formatter. La class est le type du handler, pour voir toutes les possibilit\u00e9s, la documentation python est tr\u00e8s compl\u00e8tes. Le filename et le mode sont li\u00e9s aux faits que le handler est un FileHandler , il indique le chemin du fichier et le mode d'\u00e9dition. Le \"a\" correspond \u00e0 l' \u00e9criture d'un fichier d\u00e9j\u00e0 existant . Voir la documentation officielle file system de python pour voir tous les modes. Les diff\u00e9rents level Les levels sont tri\u00e9s et ordonn\u00e9s en fonction du niveau de d\u00e9tails . Plus le level est bas (dans la liste), plus il est d\u00e9taill\u00e9 et comporte le niveau de d\u00e9tail du level du dessus. DEBUG : informations d\u00e9taill\u00e9es , g\u00e9n\u00e9ralement utiles pour le diagnostic des probl\u00e8mes. INFO : confirmation que les choses fonctionnent comme pr\u00e9vu . WARNING : indication que quelque chose d'inattendu s'est produit , ou indication d'un probl\u00e8me dans un avenir proche (par exemple, \"espace disque faible\"). Le logiciel fonctionne toujours comme pr\u00e9vu. C'est le level par d\u00e9faut si aucun n'est sp\u00e9cifi\u00e9. ERROR : en raison d'un probl\u00e8me plus grave , le logiciel n'a pas \u00e9t\u00e9 en mesure d'ex\u00e9cuter certaines fonctions . CRITICAL : une erreur s\u00e9rieuse , indiquant que le programme lui-m\u00eame n'est pas capable de continuer \u00e0 fonctionner . NOTSET : tous les levels du dessus r\u00e9unis. Logger Le logger est l'\u00e9l\u00e9ment qui sera appel\u00e9 dans les fichiers pour \u00e9crire les logs ou pour catch des erreurs. Sa configuration est assez simple : 'ge nerat io n _ nft ' : { 'ha n dlers' : [ 'co ns oleHa n dler' , 'ge nerat io n N ft Ha n dler' ] i f is_dev else [ 'ge nerat io n N ft Ha n dler' ], 'level' : 'DEBUG' , 'propaga te ' : False , 'qual na me' : 'ge nerat io n _ nft ' }, Dans un premier temps, on sp\u00e9cifie les handlers . Si c'est la version de d\u00e9veloppement qui tourne, les logs seront visibles dans la console en plus des fichiers. Si c'est en version production , la console est d\u00e9sactiv\u00e9e et tous les logs iront dans les fichiers. Le level ici n'est pas important , il sera \u00e9cras\u00e9 par le level du handler. Si aucun level n'\u00e9tait sp\u00e9cifi\u00e9 dans le handler, c'est ce level qui ferait loi. La propri\u00e9t\u00e9 propagate doit \u00eatre \u00e0 False , on ne veux pas qu'une erreur remonte les logs des fichiers parents . Enfin qualname , la propri\u00e9t\u00e9 qui indique quel nom indiquer lorsque l'on souhaite appeler un logger en particulier dans le code . Initialisation du logger Les logs doivent \u00eatre initialis\u00e9s au d\u00e9marrage de l'application . Dans notre cas, elle se fait dans le fichier app/ init .py . import logging from app.settings import GlobalSettings from logging_configuration import config from logging.config import dictConfig dictConfig ( config ( GlobalSettings () . DEBUG )) logger = logging . getLogger ( \"generation_nft\" ) logger_api = logging . getLogger ( \"generation_nft_api\" ) dictConfig est la fonction qui permet de reprendre la configuration des logs et l' ajouter dans la solution enti\u00e8re . Ensuite, nous d\u00e9finissons deux variables pour les deux loggers . Il faudra importer l'une des deux en fonction de la localisation du fichier . Si le fichier est dans l'API alors il faudra importer logger_api , sinon il faudra importer logger . Utilisation du logger L'une des impl\u00e9mentations des logs les plus simples que j'ai eu l'occasion de voir. Lorsque vous souhaitez logger une erreur, information ou simplement une variable, il suffit d' importer l'une des deux variables de la partie Initialisation . Une fois importer, utilis\u00e9 la variable pour tout type d'\u00e9v\u00e9nement. from app import logger_api logger_api . debug ( variable ) Toutes les fonctions \u00e9v\u00e9nements Les diff\u00e9rentes fonctions \u00e9v\u00e9nements sont les diff\u00e9rents levels possibles : debug warning info error","title":"Logging"},{"location":"generation_nft/python/logging/#logging","text":"","title":"Logging"},{"location":"generation_nft/python/logging/#explication","text":"Le syst\u00e8me de logs en python est int\u00e9gr\u00e9 . Il n'y a pas besoin de t\u00e9l\u00e9charger un package externe. Il permet de g\u00e9rer plusieurs m\u00e9thodes de logs avec diff\u00e9rents niveaux et des d\u00e9tails d'erreurs tr\u00e8s \u00e9volu\u00e9s. De plus, ce syst\u00e8me est compl\u00e8tement configurable. Pour notre solution, elle se trouve dans le fichier logging_configuration dans le dossier app .","title":"Explication"},{"location":"generation_nft/python/logging/#formatter","text":"Le formatter est un \u00e9l\u00e9ment permettant d'indiquer les \u00e9l\u00e9ments pr\u00e9sents dans le message du log . Il existe plusieurs variables qui r\u00e9cup\u00e8rent les informations souhait\u00e9es. Notre solution en d\u00e9finit deux . Une pour les logs dans les fichiers et une pour les logs dans la console en mode d\u00e9veloppement. Format console : 'simpleForma tter ' : { ' f orma t ' : ' [ %(level na me)s ] - %(asc t ime)s - %( na me)s : %(message)s' } corresponds \u00e0 ce message dans la console : [ DEBUG ] - 2021 -12-19 11 :12:09,765 - generation_nft_api - : { 'Hello' : 'World' } Format fichier : 'verboseForma tter ' : { ' f orma t ' : ' [ %(level na me)s ] - %(asc t ime)s - %( na me)s - (%(module)s.py | de f %( fun cName)s() | l. %(li nen o)d) : %(message)s' } corresponds \u00e0 ce message dans la console : [ DEBUG ] - 2021 -12-19 11 :12:09,765 - generation_nft_api - ( main.py | def read_root () | l. 44 ) : { 'Hello' : 'World' }","title":"Formatter"},{"location":"generation_nft/python/logging/#handler","text":"Les handlers correspondent \u00e0 tous les types de logs. Il en existe plusieurs : Dans la console . Dans un fichier . Dans un mail . ... Dans notre solution, nous en utilisons deux : fichier et console . Cependant nous en avons trois , deux pour deux fichiers de logs concernant l'API et la g\u00e9n\u00e9ration du NFT et un pour la console en mode d\u00e9veloppement. La configuration d'un handler s'effectue sous cette forme : 'ge nerat io n N ft Ha n dler' : { 'level' : 'NOTSET' , ' f orma tter ' : 'verboseForma tter ' , 'class' : 'loggi n g.FileHa n dler' , ' f ile na me' : 'app/logs/ge nerat io n _ nft .log' , 'mode' : 'a' , }, Le level correspond aux informations que l'on souhaite r\u00e9cup\u00e9rer. Pas besoin de r\u00e9expliquer le formatter. La class est le type du handler, pour voir toutes les possibilit\u00e9s, la documentation python est tr\u00e8s compl\u00e8tes. Le filename et le mode sont li\u00e9s aux faits que le handler est un FileHandler , il indique le chemin du fichier et le mode d'\u00e9dition. Le \"a\" correspond \u00e0 l' \u00e9criture d'un fichier d\u00e9j\u00e0 existant . Voir la documentation officielle file system de python pour voir tous les modes. Les diff\u00e9rents level Les levels sont tri\u00e9s et ordonn\u00e9s en fonction du niveau de d\u00e9tails . Plus le level est bas (dans la liste), plus il est d\u00e9taill\u00e9 et comporte le niveau de d\u00e9tail du level du dessus. DEBUG : informations d\u00e9taill\u00e9es , g\u00e9n\u00e9ralement utiles pour le diagnostic des probl\u00e8mes. INFO : confirmation que les choses fonctionnent comme pr\u00e9vu . WARNING : indication que quelque chose d'inattendu s'est produit , ou indication d'un probl\u00e8me dans un avenir proche (par exemple, \"espace disque faible\"). Le logiciel fonctionne toujours comme pr\u00e9vu. C'est le level par d\u00e9faut si aucun n'est sp\u00e9cifi\u00e9. ERROR : en raison d'un probl\u00e8me plus grave , le logiciel n'a pas \u00e9t\u00e9 en mesure d'ex\u00e9cuter certaines fonctions . CRITICAL : une erreur s\u00e9rieuse , indiquant que le programme lui-m\u00eame n'est pas capable de continuer \u00e0 fonctionner . NOTSET : tous les levels du dessus r\u00e9unis.","title":"Handler"},{"location":"generation_nft/python/logging/#logger","text":"Le logger est l'\u00e9l\u00e9ment qui sera appel\u00e9 dans les fichiers pour \u00e9crire les logs ou pour catch des erreurs. Sa configuration est assez simple : 'ge nerat io n _ nft ' : { 'ha n dlers' : [ 'co ns oleHa n dler' , 'ge nerat io n N ft Ha n dler' ] i f is_dev else [ 'ge nerat io n N ft Ha n dler' ], 'level' : 'DEBUG' , 'propaga te ' : False , 'qual na me' : 'ge nerat io n _ nft ' }, Dans un premier temps, on sp\u00e9cifie les handlers . Si c'est la version de d\u00e9veloppement qui tourne, les logs seront visibles dans la console en plus des fichiers. Si c'est en version production , la console est d\u00e9sactiv\u00e9e et tous les logs iront dans les fichiers. Le level ici n'est pas important , il sera \u00e9cras\u00e9 par le level du handler. Si aucun level n'\u00e9tait sp\u00e9cifi\u00e9 dans le handler, c'est ce level qui ferait loi. La propri\u00e9t\u00e9 propagate doit \u00eatre \u00e0 False , on ne veux pas qu'une erreur remonte les logs des fichiers parents . Enfin qualname , la propri\u00e9t\u00e9 qui indique quel nom indiquer lorsque l'on souhaite appeler un logger en particulier dans le code .","title":"Logger"},{"location":"generation_nft/python/logging/#initialisation-du-logger","text":"Les logs doivent \u00eatre initialis\u00e9s au d\u00e9marrage de l'application . Dans notre cas, elle se fait dans le fichier app/ init .py . import logging from app.settings import GlobalSettings from logging_configuration import config from logging.config import dictConfig dictConfig ( config ( GlobalSettings () . DEBUG )) logger = logging . getLogger ( \"generation_nft\" ) logger_api = logging . getLogger ( \"generation_nft_api\" ) dictConfig est la fonction qui permet de reprendre la configuration des logs et l' ajouter dans la solution enti\u00e8re . Ensuite, nous d\u00e9finissons deux variables pour les deux loggers . Il faudra importer l'une des deux en fonction de la localisation du fichier . Si le fichier est dans l'API alors il faudra importer logger_api , sinon il faudra importer logger .","title":"Initialisation du logger"},{"location":"generation_nft/python/logging/#utilisation-du-logger","text":"L'une des impl\u00e9mentations des logs les plus simples que j'ai eu l'occasion de voir. Lorsque vous souhaitez logger une erreur, information ou simplement une variable, il suffit d' importer l'une des deux variables de la partie Initialisation . Une fois importer, utilis\u00e9 la variable pour tout type d'\u00e9v\u00e9nement. from app import logger_api logger_api . debug ( variable ) Toutes les fonctions \u00e9v\u00e9nements Les diff\u00e9rentes fonctions \u00e9v\u00e9nements sont les diff\u00e9rents levels possibles : debug warning info error","title":"Utilisation du logger"},{"location":"generation_nft/python/packages/","text":"Packages Liste des packages Cette partie va lister tous les packages et expliquer leurs utilit\u00e9s. Il faut absolument maintenir \u00e0 jour cette liste pour \u00e9viter les packages obsol\u00e8tes , faire un suivi des versions install\u00e9es et informer les autres d\u00e9veloppeurs . Packages globaux fastapi 0.70.1 : librairie de l' API . gdown 4.2.0 : librairie google pour t\u00e9l\u00e9charger des fichiers h\u00e9berg\u00e9s sur le drive . mediapipe 0.8.9.1 : librairie pour r\u00e9cup\u00e9rer 468 points d'un visage . numpy 1.21.4 : librairie pour les calculs scientifiques complexes notamment la manipulation de matrice . opencv-python 4.5.4.60 : librairie de computer vision et de manipulation d'image . pandas 1.3.5 : librairie de data analyse . pytest 6.2.5 : librairie officielle et conseill\u00e9e par python pour effectuer les tests. python-dotenv 0.19.2 : librairie pour ajouter les variables d'environnements du fichier *.env dans des variables constantes python. scikit-image 0.19.1 : librairie pour la manipulation d'image plus avanc\u00e9e que opencv. scikit-learn 1.0.1 : librairie pour faciliter la mise en place de mod\u00e8le de machine learning . Autres packages Les autres packages dans le fichier requirements.txt sont des packages natifs \u00e0 python ou des packages import\u00e9s gr\u00e2ce aux packages cit\u00e9s au-dessus. Packages d\u00e9veloppements black 21.12b0 : voir documentation sur pre-commit . flake8 4.0.1 : voir documentation sur pre-commit . isort 5.10.1 : voir documentation sur pre-commit . pre-commit 2.16.0 : voir documentation sur pre-commit . pydocstyle 6.1.1 : voir documentation sur pre-commit . vulture 2.3 : voir documentation sur pre-commit . semantic-release 7.23.0 : permet de g\u00e9rer les versions . Lorsqu'un nouveau commit est d\u00e9tect\u00e9 par rapport \u00e0 la version pr\u00e9c\u00e9dente . Il suffira de faire la commande python release.py pour en cr\u00e9er une nouvelle. Ce fichier va cr\u00e9er un commit avec le tag de la nouvelle version et remplir le fichier CHANGELOG.md . Il est possible de rajouter des tags comme \"-p\" pour un patch (dernier num\u00e9ro), \"-mi\" (num\u00e9ro du centre) ou \"-ma\" (premier num\u00e9ro) pour indiquer le niveau de la version. Comment ajouter un nouveau package En mode d\u00e9veloppement, vous devez installer un nouveau package dans l'environnement virtuel. Il faut suivre plusieurs \u00e9tapes pour ajouter correctement un package et pour pouvoir l' utiliser . Activer l'environnement virtuel. pronochain/Scripts/activate Installer le package sur l'environnement virtuel. TOUJOURS installer avec une version pr\u00e9d\u00e9finie (garder de c\u00f4t\u00e9 cette version). python -m pip install nomdupackage == versiondupackage Ajouter le package dans le fichier requirements.txt ou requirements-dev.txt . D\u00e9pends de l'utilisation du module. nomdupackage==versiondupackage Rebuild le conteneur docker. docker-compose pull && docker-compose -f docker-compose.yml -f docker-compose.dev.yml up --build -d Une fois ces \u00e9tapes r\u00e9alis\u00e9es, vous pourrez d\u00e9velopper correctement et la version production pourra profiter de ce nouveau package sans probl\u00e8me.","title":"Packages"},{"location":"generation_nft/python/packages/#packages","text":"","title":"Packages"},{"location":"generation_nft/python/packages/#liste-des-packages","text":"Cette partie va lister tous les packages et expliquer leurs utilit\u00e9s. Il faut absolument maintenir \u00e0 jour cette liste pour \u00e9viter les packages obsol\u00e8tes , faire un suivi des versions install\u00e9es et informer les autres d\u00e9veloppeurs .","title":"Liste des packages"},{"location":"generation_nft/python/packages/#packages-globaux","text":"fastapi 0.70.1 : librairie de l' API . gdown 4.2.0 : librairie google pour t\u00e9l\u00e9charger des fichiers h\u00e9berg\u00e9s sur le drive . mediapipe 0.8.9.1 : librairie pour r\u00e9cup\u00e9rer 468 points d'un visage . numpy 1.21.4 : librairie pour les calculs scientifiques complexes notamment la manipulation de matrice . opencv-python 4.5.4.60 : librairie de computer vision et de manipulation d'image . pandas 1.3.5 : librairie de data analyse . pytest 6.2.5 : librairie officielle et conseill\u00e9e par python pour effectuer les tests. python-dotenv 0.19.2 : librairie pour ajouter les variables d'environnements du fichier *.env dans des variables constantes python. scikit-image 0.19.1 : librairie pour la manipulation d'image plus avanc\u00e9e que opencv. scikit-learn 1.0.1 : librairie pour faciliter la mise en place de mod\u00e8le de machine learning . Autres packages Les autres packages dans le fichier requirements.txt sont des packages natifs \u00e0 python ou des packages import\u00e9s gr\u00e2ce aux packages cit\u00e9s au-dessus.","title":"Packages globaux"},{"location":"generation_nft/python/packages/#packages-developpements","text":"black 21.12b0 : voir documentation sur pre-commit . flake8 4.0.1 : voir documentation sur pre-commit . isort 5.10.1 : voir documentation sur pre-commit . pre-commit 2.16.0 : voir documentation sur pre-commit . pydocstyle 6.1.1 : voir documentation sur pre-commit . vulture 2.3 : voir documentation sur pre-commit . semantic-release 7.23.0 : permet de g\u00e9rer les versions . Lorsqu'un nouveau commit est d\u00e9tect\u00e9 par rapport \u00e0 la version pr\u00e9c\u00e9dente . Il suffira de faire la commande python release.py pour en cr\u00e9er une nouvelle. Ce fichier va cr\u00e9er un commit avec le tag de la nouvelle version et remplir le fichier CHANGELOG.md . Il est possible de rajouter des tags comme \"-p\" pour un patch (dernier num\u00e9ro), \"-mi\" (num\u00e9ro du centre) ou \"-ma\" (premier num\u00e9ro) pour indiquer le niveau de la version.","title":"Packages d\u00e9veloppements"},{"location":"generation_nft/python/packages/#comment-ajouter-un-nouveau-package","text":"En mode d\u00e9veloppement, vous devez installer un nouveau package dans l'environnement virtuel. Il faut suivre plusieurs \u00e9tapes pour ajouter correctement un package et pour pouvoir l' utiliser . Activer l'environnement virtuel. pronochain/Scripts/activate Installer le package sur l'environnement virtuel. TOUJOURS installer avec une version pr\u00e9d\u00e9finie (garder de c\u00f4t\u00e9 cette version). python -m pip install nomdupackage == versiondupackage Ajouter le package dans le fichier requirements.txt ou requirements-dev.txt . D\u00e9pends de l'utilisation du module. nomdupackage==versiondupackage Rebuild le conteneur docker. docker-compose pull && docker-compose -f docker-compose.yml -f docker-compose.dev.yml up --build -d Une fois ces \u00e9tapes r\u00e9alis\u00e9es, vous pourrez d\u00e9velopper correctement et la version production pourra profiter de ce nouveau package sans probl\u00e8me.","title":"Comment ajouter un nouveau package"},{"location":"generation_nft/python/regles/","text":"R\u00e8gles et nomenclature Cette page \u00e9num\u00e8re les diff\u00e9rentes r\u00e8gles autour du langage python et fournie plusieurs informations suppl\u00e9mentaires afin de vous faciliter le d\u00e9veloppement. Tous les commentaires sont \u00e0 r\u00e9diger en Fran\u00e7ais . Commentaire Vous devez ajouter des commentaires \u00e0 diff\u00e9rents endroits : Au d\u00e9but du fichier : ajout\u00e9 le template Pronochain accessible depuis les fichiers d\u00e9j\u00e0 cr\u00e9\u00e9s et remplacer le chemin du fichier par le nouveau. Apr\u00e8s une classe : petite phrase pour expliquer \u00e0 quoi correspond la classe. \"\"\" [summary]. \"\"\" Apr\u00e8s une fonction : dois expliquer \u00e0 quoi elle sert, les param\u00e8tres, les erreurs et ce qu'elle retourne. \"\"\"[summary] Args: parameter (int): [description] Raises: Exception: [description] Returns: str: [description] \"\"\" Pour du code : positionner le commentaire sur la ligne du code ou au-dessus en minuscule sans \".\" \u00e0 la fin. var test = \"hello\" # [commentaire] # [commentaire] if test == \"hello\" : test = \"world\" Pour cr\u00e9er le template plus rapidement, positionnez-vous \u00e0 l'endroit o\u00f9 vous souhaitez mettre un commentaire et faites Ctrl + Shift + 2 . Ne pas oublier le . (period en anglais) pour les premi\u00e8res lignes de commentaires Type hinting Par d\u00e9faut, python n'oblige en aucun cas de typer les variables ni les fonctions . Dans les derni\u00e8res versions de python, la fonctionnalit\u00e9 type hinting a \u00e9t\u00e9 ajout\u00e9e et permet de sp\u00e9cifier le type. N\u00e9anmoins, il n'impose aucune obligation . Si vous stipulez que le param\u00e8tre est un string mais que vous lui passez un int, python ne d\u00e9tectera pas l'incoh\u00e9rence . Le type hinting est surtout utile pour la documentation et concerne g\u00e9n\u00e9ralement les fonctions . Exemple def test ( parametre : int , parametre_deux : int = 0 ) -> str : Deux param\u00e8tres de type int dont un param\u00e8tre a une valeur par d\u00e9faut 0 . La fonction quant \u00e0 elle retourne un str . Types Les types python sont tr\u00e8s g\u00e9n\u00e9riques : str, int, bool . Vous pouvez aussi sp\u00e9cifier une classe . Plusieurs types pour une m\u00eame notion Si le param\u00e8tre ou la valeur de retour peut retourner plusieurs possibilit\u00e9s. Il faudra importer Union du module typing . from typing import Union def test ( parametre : Union [ int , str ], parametre_deux : Union [ int , str ] = 0 ) -> Union [ str , bool ]: Particularit\u00e9s nomenclature Fichiers Certains fichiers, correspondant \u00e0 des fonctionnalit\u00e9s bien pr\u00e9cises, sont r\u00e9glement\u00e9s par des nomenclatures . Tous les noms des fichiers sont en minuscule et en snake case : snake_case . Fichier de test : commence par test_ . Fichier pour l'API et les routes : commence par le nom de l'\u00e9l\u00e9ment et fini par _controller . Nom des routes Le nom des routes doit commencer par la m\u00e9thode : get, post, put, delete et indiquer l' action de la route avec le nom de l'\u00e9l\u00e9ment . @app . get ( \"/team/ {team_id} \" ) def get_team_by_id ( team_id : int ) -> dict : Nom des variables, fonctions et classes Le nom des variables et des fonctions doivent \u00eatre en snake_case . Le nom des classes en PascalCase .","title":"R\u00e8gles et nomenclatures"},{"location":"generation_nft/python/regles/#regles-et-nomenclature","text":"Cette page \u00e9num\u00e8re les diff\u00e9rentes r\u00e8gles autour du langage python et fournie plusieurs informations suppl\u00e9mentaires afin de vous faciliter le d\u00e9veloppement. Tous les commentaires sont \u00e0 r\u00e9diger en Fran\u00e7ais .","title":"R\u00e8gles et nomenclature"},{"location":"generation_nft/python/regles/#commentaire","text":"Vous devez ajouter des commentaires \u00e0 diff\u00e9rents endroits : Au d\u00e9but du fichier : ajout\u00e9 le template Pronochain accessible depuis les fichiers d\u00e9j\u00e0 cr\u00e9\u00e9s et remplacer le chemin du fichier par le nouveau. Apr\u00e8s une classe : petite phrase pour expliquer \u00e0 quoi correspond la classe. \"\"\" [summary]. \"\"\" Apr\u00e8s une fonction : dois expliquer \u00e0 quoi elle sert, les param\u00e8tres, les erreurs et ce qu'elle retourne. \"\"\"[summary] Args: parameter (int): [description] Raises: Exception: [description] Returns: str: [description] \"\"\" Pour du code : positionner le commentaire sur la ligne du code ou au-dessus en minuscule sans \".\" \u00e0 la fin. var test = \"hello\" # [commentaire] # [commentaire] if test == \"hello\" : test = \"world\" Pour cr\u00e9er le template plus rapidement, positionnez-vous \u00e0 l'endroit o\u00f9 vous souhaitez mettre un commentaire et faites Ctrl + Shift + 2 . Ne pas oublier le . (period en anglais) pour les premi\u00e8res lignes de commentaires","title":"Commentaire"},{"location":"generation_nft/python/regles/#type-hinting","text":"Par d\u00e9faut, python n'oblige en aucun cas de typer les variables ni les fonctions . Dans les derni\u00e8res versions de python, la fonctionnalit\u00e9 type hinting a \u00e9t\u00e9 ajout\u00e9e et permet de sp\u00e9cifier le type. N\u00e9anmoins, il n'impose aucune obligation . Si vous stipulez que le param\u00e8tre est un string mais que vous lui passez un int, python ne d\u00e9tectera pas l'incoh\u00e9rence . Le type hinting est surtout utile pour la documentation et concerne g\u00e9n\u00e9ralement les fonctions .","title":"Type hinting"},{"location":"generation_nft/python/regles/#exemple","text":"def test ( parametre : int , parametre_deux : int = 0 ) -> str : Deux param\u00e8tres de type int dont un param\u00e8tre a une valeur par d\u00e9faut 0 . La fonction quant \u00e0 elle retourne un str . Types Les types python sont tr\u00e8s g\u00e9n\u00e9riques : str, int, bool . Vous pouvez aussi sp\u00e9cifier une classe . Plusieurs types pour une m\u00eame notion Si le param\u00e8tre ou la valeur de retour peut retourner plusieurs possibilit\u00e9s. Il faudra importer Union du module typing . from typing import Union def test ( parametre : Union [ int , str ], parametre_deux : Union [ int , str ] = 0 ) -> Union [ str , bool ]:","title":"Exemple"},{"location":"generation_nft/python/regles/#particularites-nomenclature","text":"","title":"Particularit\u00e9s nomenclature"},{"location":"generation_nft/python/regles/#fichiers","text":"Certains fichiers, correspondant \u00e0 des fonctionnalit\u00e9s bien pr\u00e9cises, sont r\u00e9glement\u00e9s par des nomenclatures . Tous les noms des fichiers sont en minuscule et en snake case : snake_case . Fichier de test : commence par test_ . Fichier pour l'API et les routes : commence par le nom de l'\u00e9l\u00e9ment et fini par _controller .","title":"Fichiers"},{"location":"generation_nft/python/regles/#nom-des-routes","text":"Le nom des routes doit commencer par la m\u00e9thode : get, post, put, delete et indiquer l' action de la route avec le nom de l'\u00e9l\u00e9ment . @app . get ( \"/team/ {team_id} \" ) def get_team_by_id ( team_id : int ) -> dict :","title":"Nom des routes"},{"location":"generation_nft/python/regles/#nom-des-variables-fonctions-et-classes","text":"Le nom des variables et des fonctions doivent \u00eatre en snake_case . Le nom des classes en PascalCase .","title":"Nom des variables, fonctions et classes"},{"location":"generation_nft/python/tests/","text":"Tests Cr\u00e9ation d'un test Sous python, il existe plusieurs packages pour cr\u00e9er des tests. Le plus connue et surtout le plus simple d'utilisation est pytest . La convention pour cr\u00e9er un test est d'ajouter un fichier commen\u00e7ant par le mot test_ . Cette convention va permettre \u00e0 pytest d'ex\u00e9cuter le test cr\u00e9\u00e9 sans avoir \u00e0 l'importer dans un fichier. Configuration de pytest L'automatisation de l'ex\u00e9cution des tests et notamment la convention de nommage s'effectue gr\u00e2ce au fichier pytest.ini . Consultez-le si cela vous int\u00e9resse. En ce qui concerne le nom de la fonction de test, elle doit aussi commencer par test (r\u00e8gle interne). Ensuite, vous pouvez appeler les diff\u00e9rentes fonctions a tester depuis le fichier de test. Ne pas utiliser assert assert est d\u00e9conseill\u00e9 pour des raisons de s\u00e9curit\u00e9. Ce probl\u00e8me de s\u00e9curit\u00e9 sera d\u00e9tect\u00e9 par bandit et vous emp\u00eachera de faire un commit. if not ( response . status_code == 200 ): raise AssertionError ( \"Le statut code n'est pas correct.\" ) En plus d'\u00eatre plus s\u00e9curis\u00e9e, cette syntaxe permet d'\u00eatre beaucoup plus pr\u00e9cise au moment o\u00f9 le test \u00e9choue. Exemple def test_read_root (): \"\"\"Test read root route.\"\"\" response = client . get ( \"/\" ) if not ( response . status_code == 200 ): raise AssertionError ( \"Le status code n'est pas correct.\" ) if not ( response . json () == { \"Hello\" : \"World\" }): raise AssertionError ( \"La r\u00e9ponse ne contient pas les bonnes donn\u00e9es.\" ) Tester votre test Pour d\u00e9marrer votre test, rien de plus simple. Vous pouvez simplement faire la commande \u00e0 la racine du projet : python -m pytest Cette commande est tr\u00e8s utile, mais si le nombre de tests devient trop volumineux , il peut \u00eatre long d'attendre que votre test passe. Si vous souhaitez tester uniquement le v\u00f4tre : python -m pytest app\\generation_nft_api\\tests\\test_main.py Le d\u00e9savantage de cette commande est de conna\u00eetre obligatoirement le chemin du fichier .","title":"Tests"},{"location":"generation_nft/python/tests/#tests","text":"","title":"Tests"},{"location":"generation_nft/python/tests/#creation-dun-test","text":"Sous python, il existe plusieurs packages pour cr\u00e9er des tests. Le plus connue et surtout le plus simple d'utilisation est pytest . La convention pour cr\u00e9er un test est d'ajouter un fichier commen\u00e7ant par le mot test_ . Cette convention va permettre \u00e0 pytest d'ex\u00e9cuter le test cr\u00e9\u00e9 sans avoir \u00e0 l'importer dans un fichier. Configuration de pytest L'automatisation de l'ex\u00e9cution des tests et notamment la convention de nommage s'effectue gr\u00e2ce au fichier pytest.ini . Consultez-le si cela vous int\u00e9resse. En ce qui concerne le nom de la fonction de test, elle doit aussi commencer par test (r\u00e8gle interne). Ensuite, vous pouvez appeler les diff\u00e9rentes fonctions a tester depuis le fichier de test. Ne pas utiliser assert assert est d\u00e9conseill\u00e9 pour des raisons de s\u00e9curit\u00e9. Ce probl\u00e8me de s\u00e9curit\u00e9 sera d\u00e9tect\u00e9 par bandit et vous emp\u00eachera de faire un commit. if not ( response . status_code == 200 ): raise AssertionError ( \"Le statut code n'est pas correct.\" ) En plus d'\u00eatre plus s\u00e9curis\u00e9e, cette syntaxe permet d'\u00eatre beaucoup plus pr\u00e9cise au moment o\u00f9 le test \u00e9choue.","title":"Cr\u00e9ation d'un test"},{"location":"generation_nft/python/tests/#exemple","text":"def test_read_root (): \"\"\"Test read root route.\"\"\" response = client . get ( \"/\" ) if not ( response . status_code == 200 ): raise AssertionError ( \"Le status code n'est pas correct.\" ) if not ( response . json () == { \"Hello\" : \"World\" }): raise AssertionError ( \"La r\u00e9ponse ne contient pas les bonnes donn\u00e9es.\" )","title":"Exemple"},{"location":"generation_nft/python/tests/#tester-votre-test","text":"Pour d\u00e9marrer votre test, rien de plus simple. Vous pouvez simplement faire la commande \u00e0 la racine du projet : python -m pytest Cette commande est tr\u00e8s utile, mais si le nombre de tests devient trop volumineux , il peut \u00eatre long d'attendre que votre test passe. Si vous souhaitez tester uniquement le v\u00f4tre : python -m pytest app\\generation_nft_api\\tests\\test_main.py Le d\u00e9savantage de cette commande est de conna\u00eetre obligatoirement le chemin du fichier .","title":"Tester votre test"},{"location":"global/configuration/documentation/","text":"Documentation Il a \u00e9t\u00e9 d\u00e9cid\u00e9 de centraliser toutes nos documentations des diff\u00e9rentes solutions afin d'\u00e9viter de se rendre dans les quatre solutions pour trouver une information. Nous avons choisi squidfunk/mkdocs pour plusieurs raisons : Initialisation et installation tr\u00e8s simple : fonctionne sur une image docker sans d\u00e9pendances. Ergonomie et fonctionnalit\u00e9s : de base, le design de la documentation est plut\u00f4t beau et la multitude des fonctionnalit\u00e9s impl\u00e9ment\u00e9es de base nous permet de structurer notre documentation de la meilleure des mani\u00e8res. H\u00e9bergement en ligne gratuit sur GitHub. Consultation Si ce que vous souhaitez correspond uniquement \u00e0 de la consultation rendez vous sur la documentation en ligne : Documentation Pronochain . Sinon, si vous souhaitez ajouter ou modifier une page , suivez les \u00e9tapes dans Installation Installation Pour installer la documentation, il vous suffit de cloner le r\u00e9pertoire GIT . Dans ce r\u00e9pertoire, le fichier docker-compose.yml va vous permettre de r\u00e9cup\u00e9rer l'image et de la build sur votre docker. Une fois d\u00e9marr\u00e9, il suffira de se rendre sur localhost:9090 . Rendez-vous \u00e0 la racine de la solution de la documentation et tapez la commande dans une console ( powershell de pr\u00e9f\u00e9rence si vous \u00eates sous Windows) : docker-compose up --build -d Cette commande va build l'image et d\u00e9marrer la documentation . Si vous souhaitez seulement la d\u00e9marrer, car vous l'avez d\u00e9j\u00e0 build : docker-compose up -d Installer la police \"Poppins-Regular.ttf\" Ajouter une page Cr\u00e9er votre branche depuis master . Tout ce qui concerne la navigation se trouve dans le fichier mkdocs.yml . Ce fichier comporte plusieurs informations, dont l' ajout d'extentions . La seule partie dans ce fichier qui doit \u00eatre modifi\u00e9e est le code \u00e0 la fin du fichier . La structure et l'indentation de la navigation doivent \u00eatre respect\u00e9es, comme pour les dossiers et les fichiers dans le dossier docs . nav : - Accueil : index.md - Global : - Configuration : - Documentation : global/configuration/documentation.md - G\u00e9n\u00e9ration NFT : - Initialisation : generation_nft/index.md - Configuration : - Variables d'environnements : generation_nft/configuration/variables_environnements.md - Pre-commit : generation_nft/configuration/pre_commit.md - Visual Studio Code : generation_nft/configuration/vs_code.md - Python : - Architecture : generation_nft/python/architecture.md - Packages : generation_nft/python/packages.md - Tests : generation_nft/python/tests.md - R\u00e8gles et nomenclatures : generation_nft/python/regles.md - Logging : generation_nft/python/logging.md Pour ce qui est du contenu des pages et des diff\u00e9rentes syntaxes, il faut conna\u00eetre le markdown . De plus, vous pouvez vous rendre sur cette page pour voir la liste des extensions que vous pouvez utiliser. D\u00e9ployer en ligne ATTENTION, NE JAMAIS PULL REQUEST, REBASE OU MERGE LA BRANCHE origin/gh-pages OU gh-pages SUR master OU UNE AUTRE BRANCHE ! Lorsque vous avez modifi\u00e9 la documentation, il est imp\u00e9ratif de red\u00e9ployer les nouveaut\u00e9es. Faites une pull request que vous allez directement accepter dans le r\u00e9pertoire GitHub. LIEN RAPIDE . Une fois que la pull request de votre branche \u00e0 master a \u00e9t\u00e9 valid\u00e9e et merg\u00e9e , faites les \u00e9tapes qui suivent. Installer sur votre ordinateur le package mkdocs-material (voir documentation de la solution G\u00e9n\u00e9ration NFT -> Initialisation pour installer correctement Python sur votre ordinateur). pip install mkdocs-material Une fois que votre documentation est \u00e9crite , commit\u00e9 vos changements et push\u00e9 votre branche . git push nom-de-ta-branche La section qui va suivre n\u00e9cessite de se rendre sur le Google DRIVE -> Pronochain -> Informations -> Identifiants -> Identifiants compte . Il se peut qu'une popup GitHub s'ouvre pour vous demander les informations de connexion pour la premi\u00e8re action sur le r\u00e9pertoire. Pour la premi\u00e8re popup avec le logo de GitHub visible, remplissez avec l'email et le mot de passe . La seconde popup : l'email , et la derni\u00e8re : le PAT ( personal access token ). Une fois votre branch push en distant, faites en sorte de mettre votre branche sur master gr\u00e2ce \u00e0 la pull request du github (vous pouvez directement sans reviewers). Une fois master \u00e0 jour en distant et en local , faites: mkdocs gh-deploy --force Et voil\u00e0 !","title":"Documentation"},{"location":"global/configuration/documentation/#documentation","text":"Il a \u00e9t\u00e9 d\u00e9cid\u00e9 de centraliser toutes nos documentations des diff\u00e9rentes solutions afin d'\u00e9viter de se rendre dans les quatre solutions pour trouver une information. Nous avons choisi squidfunk/mkdocs pour plusieurs raisons : Initialisation et installation tr\u00e8s simple : fonctionne sur une image docker sans d\u00e9pendances. Ergonomie et fonctionnalit\u00e9s : de base, le design de la documentation est plut\u00f4t beau et la multitude des fonctionnalit\u00e9s impl\u00e9ment\u00e9es de base nous permet de structurer notre documentation de la meilleure des mani\u00e8res. H\u00e9bergement en ligne gratuit sur GitHub.","title":"Documentation"},{"location":"global/configuration/documentation/#consultation","text":"Si ce que vous souhaitez correspond uniquement \u00e0 de la consultation rendez vous sur la documentation en ligne : Documentation Pronochain . Sinon, si vous souhaitez ajouter ou modifier une page , suivez les \u00e9tapes dans Installation","title":"Consultation"},{"location":"global/configuration/documentation/#installation","text":"Pour installer la documentation, il vous suffit de cloner le r\u00e9pertoire GIT . Dans ce r\u00e9pertoire, le fichier docker-compose.yml va vous permettre de r\u00e9cup\u00e9rer l'image et de la build sur votre docker. Une fois d\u00e9marr\u00e9, il suffira de se rendre sur localhost:9090 . Rendez-vous \u00e0 la racine de la solution de la documentation et tapez la commande dans une console ( powershell de pr\u00e9f\u00e9rence si vous \u00eates sous Windows) : docker-compose up --build -d Cette commande va build l'image et d\u00e9marrer la documentation . Si vous souhaitez seulement la d\u00e9marrer, car vous l'avez d\u00e9j\u00e0 build : docker-compose up -d Installer la police \"Poppins-Regular.ttf\"","title":"Installation"},{"location":"global/configuration/documentation/#ajouter-une-page","text":"Cr\u00e9er votre branche depuis master . Tout ce qui concerne la navigation se trouve dans le fichier mkdocs.yml . Ce fichier comporte plusieurs informations, dont l' ajout d'extentions . La seule partie dans ce fichier qui doit \u00eatre modifi\u00e9e est le code \u00e0 la fin du fichier . La structure et l'indentation de la navigation doivent \u00eatre respect\u00e9es, comme pour les dossiers et les fichiers dans le dossier docs . nav : - Accueil : index.md - Global : - Configuration : - Documentation : global/configuration/documentation.md - G\u00e9n\u00e9ration NFT : - Initialisation : generation_nft/index.md - Configuration : - Variables d'environnements : generation_nft/configuration/variables_environnements.md - Pre-commit : generation_nft/configuration/pre_commit.md - Visual Studio Code : generation_nft/configuration/vs_code.md - Python : - Architecture : generation_nft/python/architecture.md - Packages : generation_nft/python/packages.md - Tests : generation_nft/python/tests.md - R\u00e8gles et nomenclatures : generation_nft/python/regles.md - Logging : generation_nft/python/logging.md Pour ce qui est du contenu des pages et des diff\u00e9rentes syntaxes, il faut conna\u00eetre le markdown . De plus, vous pouvez vous rendre sur cette page pour voir la liste des extensions que vous pouvez utiliser.","title":"Ajouter une page"},{"location":"global/configuration/documentation/#deployer-en-ligne","text":"ATTENTION, NE JAMAIS PULL REQUEST, REBASE OU MERGE LA BRANCHE origin/gh-pages OU gh-pages SUR master OU UNE AUTRE BRANCHE ! Lorsque vous avez modifi\u00e9 la documentation, il est imp\u00e9ratif de red\u00e9ployer les nouveaut\u00e9es. Faites une pull request que vous allez directement accepter dans le r\u00e9pertoire GitHub. LIEN RAPIDE . Une fois que la pull request de votre branche \u00e0 master a \u00e9t\u00e9 valid\u00e9e et merg\u00e9e , faites les \u00e9tapes qui suivent. Installer sur votre ordinateur le package mkdocs-material (voir documentation de la solution G\u00e9n\u00e9ration NFT -> Initialisation pour installer correctement Python sur votre ordinateur). pip install mkdocs-material Une fois que votre documentation est \u00e9crite , commit\u00e9 vos changements et push\u00e9 votre branche . git push nom-de-ta-branche La section qui va suivre n\u00e9cessite de se rendre sur le Google DRIVE -> Pronochain -> Informations -> Identifiants -> Identifiants compte . Il se peut qu'une popup GitHub s'ouvre pour vous demander les informations de connexion pour la premi\u00e8re action sur le r\u00e9pertoire. Pour la premi\u00e8re popup avec le logo de GitHub visible, remplissez avec l'email et le mot de passe . La seconde popup : l'email , et la derni\u00e8re : le PAT ( personal access token ). Une fois votre branch push en distant, faites en sorte de mettre votre branche sur master gr\u00e2ce \u00e0 la pull request du github (vous pouvez directement sans reviewers). Une fois master \u00e0 jour en distant et en local , faites: mkdocs gh-deploy --force Et voil\u00e0 !","title":"D\u00e9ployer en ligne"}]}